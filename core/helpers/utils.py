# core/helpers/utils.py
"""
General utility functions for the Crypto Prediction project.
Includes configuration loading, CLI interaction helpers, time conversions,
and other miscellaneous helper functions.
"""
import yaml
import os
import sys
import shutil
import subprocess
import logging
import pandas as pd  # For type hinting and potentially some data ops
import numpy as np  # For classify_delta_value

logger = logging.getLogger(__name__)

# --- Configuration Loading ---
DEFAULT_CONFIG_PATH = 'core/config.yaml'  # Default path relative to project root


def load_config(config_path=None):
    """
    Loads the YAML configuration file.

    Args:
        config_path (str, optional): Path to the YAML configuration file.
            If None, tries to load from DEFAULT_CONFIG_PATH relative to project root,
            then from DEFAULT_CONFIG_PATH relative to current working directory.

    Returns:
        dict: A dictionary containing the configuration, or None if loading fails.
    """
    paths_to_try = []
    if config_path:
        paths_to_try.append(config_path)
    else:
        # Try path relative to project root (assuming utils.py is in core/helpers)
        project_root_config_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),  # core/helpers
            "..",  # core
            "..",  # project root
            DEFAULT_CONFIG_PATH
        )
        paths_to_try.append(os.path.normpath(project_root_config_path))
        # Try path relative to current working directory
        paths_to_try.append(DEFAULT_CONFIG_PATH)

    for path in paths_to_try:
        try:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    config_data = yaml.safe_load(f)
                logger.info(f"Configuration loaded successfully from: {path}")
                return config_data
            else:
                logger.debug(f"Config file not found at: {path}")
        except yaml.YAMLError as e:
            logger.error(f"Error parsing YAML configuration file {path}: {e}", exc_info=True)
            return None  # Critical error, stop trying
        except IOError as e:
            logger.error(f"Error reading configuration file {path}: {e}", exc_info=True)
            # Continue to next path if IOError on one, unless it was explicitly provided
            if config_path and path == config_path: return None
        except Exception as e:
            logger.error(f"Unexpected error loading configuration from {path}: {e}", exc_info=True)
            if config_path and path == config_path: return None

    logger.error(f"Configuration file could not be loaded from any of the tried paths: {paths_to_try}")
    return None


# --- CLI Interaction Helpers (from main_cli.py) ---

def print_header(title, width=50):
    """Prints a formatted header to the console."""
    print("\n" + "=" * width)
    print(f" {title.center(width - 2)} ")
    print("=" * width)


def run_script(command_list, description, python_executable=None):
    """
    Runs an external Python script using subprocess.

    Args:
        command_list (list): The command and its arguments as a list of strings.
                             The first element should be the Python executable,
                             followed by the script name, then script arguments.
        description (str): A description of the script/task being run.
        python_executable (str, optional): Path to the python interpreter.
                                           If None, sys.executable is used.

    Returns:
        int: The return code of the script. -1 if an exception occurred.
    """
    if not command_list:
        logger.error("Command list for run_script is empty.")
        return -1

    # Ensure the first element is the python executable if not already set
    # This is a bit redundant if the calling code already does this, but good for safety.
    actual_command = list(command_list)  # Make a copy
    if not actual_command[0].lower().endswith("python") and \
            not actual_command[0].lower().endswith("python.exe") and \
            not os.path.basename(actual_command[0]).lower().startswith("python"):
        py_exec = python_executable or sys.executable
        actual_command.insert(0, py_exec)

    logger.info(f"â³ Ğ—Ğ°Ğ¿ÑƒÑĞº: {description}...")
    logger.info(f"    ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {' '.join(actual_command)}")
    print(f"\nâ³  Ğ—Ğ°Ğ¿ÑƒÑĞº: {description}...")
    print(f"    ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {' '.join(actual_command)}")  # Also print to console for CLI user

    try:
        # For simple pass-through of output, Popen or run without capture_output is better.
        # check=False allows manual handling of return codes.
        result = subprocess.run(actual_command, check=False)  # Runs and waits

        if result.returncode == 0:
            logger.info(f"âœ…  Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {description}")
            print(f"âœ…  Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {description}")
        elif result.returncode == 130:  # Ctrl+C in child process
            logger.warning(f"ğŸ”¶  ĞŸÑ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼: {description}")
            print(f"ğŸ”¶  ĞŸÑ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼: {description}")
        else:
            logger.error(f"âŒ  ĞÑˆĞ¸Ğ±ĞºĞ° (ĞºĞ¾Ğ´ {result.returncode}): {description}")
            print(f"âŒ  ĞÑˆĞ¸Ğ±ĞºĞ° (ĞºĞ¾Ğ´ {result.returncode}): {description}")
        return result.returncode
    except FileNotFoundError:
        logger.error(f"âŒ  ĞÑˆĞ¸Ğ±ĞºĞ°: ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°/ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Python Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ '{actual_command[1] if len(actual_command) > 1 else 'N/A'}' ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚.")
        print(f"âŒ  ĞÑˆĞ¸Ğ±ĞºĞ°: ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Python Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ '{actual_command[1] if len(actual_command) > 1 else 'N/A'}' ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚.")
        return -1
    except Exception as e:
        logger.error(f"âŒ  ĞĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ '{description}': {e}", exc_info=True)
        print(f"âŒ  ĞĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ '{description}': {e}")
        return -1


def select_timeframes_interactive(prompt_message="Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹", available_timeframes=None):
    """
    Prompts the user to select timeframes interactively.

    Args:
        prompt_message (str): The message to display to the user.
        available_timeframes (list, optional): A list of valid timeframe strings.
            If None, a default list is used for validation.

    Returns:
        list or None: A list of selected timeframe strings, sorted,
                      or None if the user cancels or provides no valid input.
    """
    if available_timeframes is None:
        # Fallback if no list is provided (e.g., from config)
        available_timeframes = ['1m', '5m', '15m', '30m', '1h', '4h', '1d']
        logger.debug("select_timeframes_interactive using default available_timeframes.")

    print_header(prompt_message)
    print(f"Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹: {', '.join(available_timeframes)}")

    while True:
        selected_tfs_str = input(
            f"Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 15m 1h),\n"
            f"'all' Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° ({len(available_timeframes)}), Ğ¸Ğ»Ğ¸ 'q' Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‹: "
        ).strip().lower()  # Convert to lower for 'all' and 'q'

        if selected_tfs_str == 'q':
            logger.info("Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ² Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼.")
            return None
        if not selected_tfs_str or selected_tfs_str == 'all':
            logger.info(f"Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ñ‹ Ğ²ÑĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹: {available_timeframes}")
            return available_timeframes  # Return a copy

        selected_tfs = []
        invalid_tfs_input = []
        for tf_input in selected_tfs_str.split():
            if tf_input in available_timeframes:
                if tf_input not in selected_tfs:  # Avoid duplicates if user enters "15m 15m"
                    selected_tfs.append(tf_input)
            else:
                invalid_tfs_input.append(tf_input)

        if invalid_tfs_input:
            logger.warning(f"ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ğ² Ğ²Ğ²Ğ¾Ğ´Ğµ: {', '.join(invalid_tfs_input)}. Ğ‘ÑƒĞ´ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹.")
            print(f"ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ ({', '.join(invalid_tfs_input)}) Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹.")

        if not selected_tfs:
            logger.warning("ĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°.")
            print("ĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ· Ğ¸Ğ»Ğ¸ Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ 'q'.")
            continue

        # Sort selected timeframes based on their order in available_timeframes for consistency
        try:
            selected_tfs.sort(key=lambda x: available_timeframes.index(x))
        except ValueError:  # Should not happen if logic above is correct
            logger.error("ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ğ»Ñ„Ğ°Ğ²Ğ¸Ñ‚Ğ½Ğ°Ñ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°.")
            selected_tfs.sort()

        logger.info(f"Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ñ‹ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹: {', '.join(selected_tfs)}")
        print(f"Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ñ‹ Ñ‚Ğ°Ğ¹Ğ¼Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹: {', '.join(selected_tfs)}")
        return selected_tfs


def clear_training_artifacts_interactive(
        models_dir_path, logs_dir_path, data_features_dir_path,
        database_dir_path, update_log_file_path  # Pass full paths
):
    """
    Interactively prompts the user to clear training artifacts like models, logs,
    and feature files. Specific files/dirs like database and update_log are preserved.

    Args:
        models_dir_path (str): Path to the models directory.
        logs_dir_path (str): Path to the logs directory.
        data_features_dir_path (str): Path to the data directory containing features/samples.
        database_dir_path (str): Path to the database directory (will NOT be cleared).
        update_log_file_path (str): Path to the update_log.txt file (will NOT be cleared).
    """
    print_header("ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ")
    confirm = input(
        "Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•! Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚:\n"
        f"  - Ğ’ÑĞµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ Ğ¿Ğ°Ğ¿ĞºĞ¸ Ğ¸Ğ· Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ '{models_dir_path}/'\n"
        f"  - Ğ’ÑĞµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ Ğ¿Ğ°Ğ¿ĞºĞ¸ Ğ¸Ğ· Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ '{logs_dir_path}/'\n"
        f"  - Ğ¤Ğ°Ğ¹Ğ»Ñ‹ 'features_*.pkl' Ğ¸ 'sample_*.csv' Ğ¸Ğ· Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ '{data_features_dir_path}/'\n"
        f"ĞŸĞ°Ğ¿ĞºĞ° Ñ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… '{database_dir_path}/' Ğ¸ Ñ„Ğ°Ğ¹Ğ» '{update_log_file_path}' ĞĞ• Ğ±ÑƒĞ´ÑƒÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ¾Ğ½ÑƒÑ‚Ñ‹.\n"
        "ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ? (y/n): "
    ).lower()

    if confirm == 'y':
        logger.info("ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.")
        print("\nğŸ§¹  ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ...")

        # Clear contents of models and logs directories
        for dir_to_clear, description in [(models_dir_path, "Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"), (logs_dir_path, "Ğ»Ğ¾Ğ³Ğ¾Ğ²")]:
            if os.path.exists(dir_to_clear):
                try:
                    for item in os.listdir(dir_to_clear):
                        item_path = os.path.join(dir_to_clear, item)
                        if os.path.isfile(item_path) or os.path.islink(item_path):  # also remove symlinks
                            os.unlink(item_path)  # Use unlink for files and symlinks
                        elif os.path.isdir(item_path):
                            shutil.rmtree(item_path)
                    logger.info(f"Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {description} '{dir_to_clear}' Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ¾.")
                    print(f"    Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {description} '{dir_to_clear}' Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ¾.")
                except Exception as e:
                    logger.error(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {dir_to_clear}: {e}")
                    print(f"    ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {dir_to_clear}: {e}")
            else:
                logger.info(f"Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ {dir_to_clear} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° (Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾).")
                print(f"    Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ {dir_to_clear} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° (Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾).")
            # Ensure directory exists after clearing (it might have been removed if rmtree was used on the dir itself)
            os.makedirs(dir_to_clear, exist_ok=True)

        # Clear specific files from data_features_dir
        cleaned_feature_files_count = 0
        if os.path.exists(data_features_dir_path):
            try:
                for item in os.listdir(data_features_dir_path):
                    item_path = os.path.join(data_features_dir_path, item)
                    # Check if it's a file and matches patterns
                    if os.path.isfile(item_path) and \
                            ((item.startswith("features_") and item.endswith(".pkl")) or \
                             (item.startswith("sample_") and item.endswith(".csv"))):
                        # Crucially, do not delete the update_log.txt if it's in this directory
                        if item_path.lower() == update_log_file_path.lower():
                            logger.debug(f"ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ»Ğ¾Ğ³Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹: {item_path}")
                            continue
                        try:
                            os.remove(item_path)
                            logger.debug(f"Ğ£Ğ´Ğ°Ğ»ĞµĞ½ Ñ„Ğ°Ğ¹Ğ»: {item_path}")
                            cleaned_feature_files_count += 1
                        except Exception as e:
                            logger.error(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» {item_path}: {e}")
                            print(f"    ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» {item_path}: {e}")
            except Exception as e:
                logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {data_features_dir_path}: {e}")
                print(f"    ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {data_features_dir_path}: {e}")

            if cleaned_feature_files_count > 0:
                logger.info(f"Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ {cleaned_feature_files_count} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²/ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸Ğ· '{data_features_dir_path}'.")
                print(f"    Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ {cleaned_feature_files_count} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²/ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¸Ğ· '{data_features_dir_path}'.")
            else:
                logger.info(f"Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²/ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ² '{data_features_dir_path}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ¸Ğ»Ğ¸ ÑƒĞ¶Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ñ‹.")
                print(f"    Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²/ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ² '{data_features_dir_path}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ¸Ğ»Ğ¸ ÑƒĞ¶Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ñ‹.")
        else:
            logger.info(f"Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ {data_features_dir_path} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²/ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ².")
            print(f"    Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ {data_features_dir_path} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²/ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ².")

        logger.info("ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")
        print("âœ…  ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")
    else:
        logger.info("ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼.")
        print("ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°.")


def ensure_base_directories(dir_paths_list):
    """
    Creates a list of base directories if they don't exist.

    Args:
        dir_paths_list (list): A list of directory paths to create.
    """
    logger.info("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹...")
    print("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹...")
    for dir_path in dir_paths_list:
        if not dir_path:  # Skip if path is empty or None
            logger.warning("ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº.")
            continue
        try:
            if not os.path.exists(dir_path):
                os.makedirs(dir_path, exist_ok=True)  # exist_ok=True is important
                logger.info(f"  Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ: {dir_path}")
                print(f"  Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ: {dir_path}")
            # else:
            #     logger.debug(f"  Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚: {dir_path}")
        except OSError as e:  # Catch potential OS errors during makedirs
            logger.error(f"  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {dir_path}: {e}")
            print(f"  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {dir_path}: {e}")
        except Exception as e:
            logger.error(f"  ĞĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ/ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {dir_path}: {e}")
            print(f"  ĞĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ/ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ {dir_path}: {e}")

    logger.info("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")
    print("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")


# --- Timeframe and Data Conversion Helpers ---

def get_tf_ms(timeframe_str):
    """
    Converts a timeframe string (e.g., '1m', '1h', '1d') to milliseconds.

    Args:
        timeframe_str (str): The timeframe string.

    Returns:
        int: The timeframe in milliseconds, or 0 if conversion fails.
    """
    if not isinstance(timeframe_str, str) or not timeframe_str:
        logger.error(f"Invalid timeframe_str: '{timeframe_str}'. Must be a non-empty string.")
        return 0

    multipliers = {'m': 60, 'h': 3600, 'd': 86400, 'w': 604800, 'M': 2592000}  # Added W, M for flexibility

    numeric_part_str = ""
    unit_part_str = ""

    for char in timeframe_str:
        if char.isdigit():
            numeric_part_str += char
        elif char.isalpha():
            unit_part_str += char
            # Allow for multi-char units if needed in future, but typically single like 'm', 'h'
            # For now, let's assume the first alphabetic char is the unit.
            break

    if not numeric_part_str or not unit_part_str:
        logger.error(f"Could not parse numeric value or unit from timeframe: '{timeframe_str}'")
        return 0

    try:
        numeric_value = int(numeric_part_str)
    except ValueError:
        logger.error(f"Could not convert numeric part '{numeric_part_str}' to int from timeframe: '{timeframe_str}'")
        return 0

    unit_key = unit_part_str[0].lower()  # Take first char of unit and lower case it
    if unit_key == 'm' and unit_part_str.lower() == 'mo':  # Distinguish 'Minutes' from 'Months'
        unit_key = 'M'  # Use 'M' for Month

    if unit_key not in multipliers:
        logger.error(f"Unknown time unit '{unit_part_str}' (parsed as '{unit_key}') in timeframe: '{timeframe_str}'")
        return 0

    return numeric_value * multipliers[unit_key] * 1000


# --- Data Classification Helpers ---

def classify_delta_value(delta_val, up_thresh=0.002, down_thresh=-0.002, neutral_is_nan=True):
    """
    Classifies a delta value into 'UP', 'DOWN', or NEUTRAL/NaN.
    NEUTRAL can be np.nan or a specific string like 'NEUTRAL'.

    Args:
        delta_val (float or pd.Series): The delta value(s) to classify.
        up_thresh (float): Threshold above which delta is considered 'UP'.
        down_thresh (float): Threshold below which delta is considered 'DOWN'.
        neutral_is_nan (bool): If True, NEUTRAL is represented as np.nan.
                               If False, NEUTRAL is represented as the string "NEUTRAL".

    Returns:
        str or np.nan or pd.Series: Classified label(s).
    """
    if isinstance(delta_val, pd.Series):
        # Apply classification element-wise for a Series
        def _classify_single(val):
            if pd.isna(val): return np.nan
            if val > up_thresh: return 'UP'
            if val < down_thresh: return 'DOWN'
            return np.nan if neutral_is_nan else 'NEUTRAL'

        return delta_val.apply(_classify_single)
    else:  # Single float value
        if pd.isna(delta_val): return np.nan
        if delta_val > up_thresh: return 'UP'
        if delta_val < down_thresh: return 'DOWN'
        return np.nan if neutral_is_nan else 'NEUTRAL'


# Example usage (if run directly for testing)
if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - [%(module)s] - %(message)s')

    print("\n--- Testing load_config ---")
    # Create a dummy config for testing if it doesn't exist in expected default location
    dummy_cfg_path = "temp_test_config.yaml"
    if not os.path.exists(DEFAULT_CONFIG_PATH) and not os.path.exists(os.path.join("..", "..", DEFAULT_CONFIG_PATH)):
        with open(dummy_cfg_path, "w") as f_cfg:
            yaml.dump({"test_key": "test_value", "nested": {"num": 123}}, f_cfg)
        cfg = load_config(dummy_cfg_path)
        os.remove(dummy_cfg_path)
    else:
        cfg = load_config()  # Try default paths

    if cfg:
        print(f"Config loaded: {cfg}")
        assert cfg.get("test_key") == "test_value" if "test_key" in cfg else True
    else:
        print("Config not loaded (this might be normal if no default config exists for test).")

    print("\n--- Testing print_header ---")
    print_header("Test Header Utility")

    print("\n--- Testing select_timeframes_interactive ---")
    # This is interactive, so manual test or skip in automated tests
    # selected = select_timeframes_interactive(available_timeframes=['1h', '4h', '1d', '5m'])
    # print(f"Selected TFs: {selected}")

    print("\n--- Testing ensure_base_directories ---")
    test_dirs = ["temp_dir1/subdir", "temp_dir2"]
    ensure_base_directories(test_dirs)
    for d in test_dirs: assert os.path.exists(d)
    if os.path.exists("temp_dir1"): shutil.rmtree("temp_dir1")
    if os.path.exists("temp_dir2"): shutil.rmtree("temp_dir2")
    print("Base directories test passed and cleaned up.")

    print("\n--- Testing get_tf_ms ---")
    assert get_tf_ms("1m") == 60000
    assert get_tf_ms("15m") == 15 * 60000
    assert get_tf_ms("1h") == 3600000
    assert get_tf_ms("4h") == 4 * 3600000
    assert get_tf_ms("1d") == 86400000
    assert get_tf_ms("1w") == 7 * 86400000  # Week
    assert get_tf_ms("1mo") == 30 * 86400000  # Month (approx 30 days)
    assert get_tf_ms("5min") == 5 * 60000  # allow 'min'
    assert get_tf_ms("1hour") == 1 * 3600000  # allow 'hour'
    assert get_tf_ms("1day") == 1 * 86400000  # allow 'day'
    assert get_tf_ms("invalid") == 0
    assert get_tf_ms("m1") == 0  # number must come first
    print("get_tf_ms tests passed.")

    print("\n--- Testing classify_delta_value ---")
    assert classify_delta_value(0.003) == 'UP'
    assert classify_delta_value(-0.003) == 'DOWN'
    assert classify_delta_value(0.001) is np.nan
    assert classify_delta_value(0.001, neutral_is_nan=False) == 'NEUTRAL'
    assert pd.isna(classify_delta_value(np.nan))

    series_deltas = pd.Series([0.003, -0.005, 0.000, np.nan])
    classified_series = classify_delta_value(series_deltas)
    print(f"Classified series: {classified_series.tolist()}")
    expected_series = ['UP', 'DOWN', np.nan, np.nan]
    # Need to handle NaN comparison carefully for series assert
    pd.testing.assert_series_equal(classified_series, pd.Series(expected_series, dtype=object), check_dtype=False)
    print("classify_delta_value tests passed.")

    print("\n--- Testing run_script (example: python --version) ---")
    # This will print Python version to console
    ret_code = run_script([sys.executable, "--version"], "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ĞµÑ€ÑĞ¸Ğ¸ Python")
    assert ret_code == 0, f"run_script for python --version failed with code {ret_code}"
    print("run_script test (python --version) seemed to work.")
