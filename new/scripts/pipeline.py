# scripts/pipeline.py

import subprocess
import sys
import time
import argparse
import logging
import os

# Ensure the src directory is in the Python path if necessary (usually handled by running from project root)
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

# Import config and logging setup first
from src.utils.config import get_config
from src.utils.logging_setup import setup_logging

# --- Initial Setup ---
# Load configuration
config = get_config()
TIMEFRAMES_CONFIG = config['timeframes']
SYMBOL_GROUPS = config['symbol_groups'] # Needed for argument validation
PATHS_CONFIG = config['paths'] # Needed for logs path

# Configure logging for this script
# Note: In a pipeline via cli.py, logging might already be set up.
# Calling setup_logging again is generally safe if it uses dictConfig.
setup_logging()
logger = logging.getLogger(__name__) # Use logger specific to this module

# --- Constants from Config ---
DEFAULT_TIMEFRAMES = TIMEFRAMES_CONFIG['default']


# --- Utility Functions ---

def run_step(description, command_list):
    """
    Runs a command as a subprocess.

    Args:
        description (str): A human-readable description of the step.
        command_list (list): The command and its arguments as a list.

    Returns:
        int: The return code of the subprocess.
    """
    print(f"\n[Pipeline] üîß  {description}...")
    logger.info(f"–°—Ç–∞—Ä—Ç —à–∞–≥–∞: {description}")
    logger.info(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(command_list)}")
    start_time = time.time()

    try:
        # shell=False, command_list is passed directly
        # stdout=subprocess.PIPE, stderr=subprocess.PIPE will capture output
        # but we usually want to see the output of the subprocesses directly
        # in the console for tools like tqdm.
        # So, let's keep the default (None), which means stdout/stderr go to parent.
        result = subprocess.run(command_list, check=False) # check=False means we handle non-zero exit codes

        duration = time.time() - start_time

        if result.returncode == 0:
            logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {description} (‚è±  {duration:.1f}s)")
            print(f"[Pipeline] ‚úÖ  {description} ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω–æ (‚è±  {duration:.1f}s)")
        elif result.returncode == 130:  # Standard Unix exit code for Ctrl+C
            logger.warning(f"–®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º: {description} (–∫–æ–¥ {result.returncode}) (‚è±  {duration:.1f}s)")
            print(f"[Pipeline] üî∂  –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º: {description} (–∫–æ–¥ {result.returncode}) (‚è±  {duration:.1f}s)")
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —à–∞–≥–µ: {description} (–∫–æ–¥ {result.returncode}) (‚è±  {duration:.1f}s)")
            print(f"[Pipeline] ‚ùå  –û—à–∏–±–∫–∞ –≤ —à–∞–≥–µ: {description} (–∫–æ–¥ {result.returncode}) (‚è±  {duration:.1f}s)")

        return result.returncode

    except FileNotFoundError:
        logger.error(f"–û—à–∏–±–∫–∞: –°–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Python –¥–æ—Å—Ç—É–ø–µ–Ω –∏ —Å–∫—Ä–∏–ø—Ç '{command_list[1]}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ PATH –∏–ª–∏ —É–∫–∞–∑–∞–Ω –ø–æ–ª–Ω—ã–π –ø—É—Ç—å.")
        print(f"[Pipeline] ‚ùå  –û—à–∏–±–∫–∞: –°–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω ({command_list[1]})")
        return -1 # Custom error code for script not found
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ '{description}': {e}", exc_info=True)
        print(f"[Pipeline] ‚ùå  –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ '{description}': {e}")
        return -2 # Custom error code for other exceptions


# --- Main Pipeline Logic ---

def main():
    parser = argparse.ArgumentParser(description="–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∏–ø—Ç–æ-–¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è.")
    parser.add_argument('--tf', nargs='*', default=DEFAULT_TIMEFRAMES,
                        help=f"–¢–∞–π–º—Ñ—Ä–µ–π–º—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: --tf 5m 15m). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: {' '.join(DEFAULT_TIMEFRAMES)}")
    parser.add_argument('--train', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (CatBoost) –ø–æ—Å–ª–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.')
    parser.add_argument('--skip-predict', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è.')

    # Control data update step
    update_group = parser.add_mutually_exclusive_group()
    update_group.add_argument('--full-update', action='store_true', help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö (scripts/update_data.py --full).')
    update_group.add_argument('--mini-update', action='store_true', help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö (scripts/update_data.py --mini). (–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --full-update)')

    # Filter by symbol or group (passed down to preprocess, train, predict)
    filter_group_pipeline = parser.add_mutually_exclusive_group()
    filter_group_pipeline.add_argument('--symbol', type=str, help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞.")
    filter_group_pipeline.add_argument('--symbol-group', type=str, help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –≥—Ä—É–ø–ø—ã —Å–∏–º–≤–æ–ª–æ–≤.")


    args = parser.parse_args()

    # --- Determine parameters based on args ---
    timeframes_to_process = args.tf # Use argparse default
    # Validate specified timeframes
    allowed_timeframes = TIMEFRAMES_CONFIG['default']
    valid_tfs = [t for t in timeframes_to_process if t in allowed_timeframes]
    invalid_tfs = [t for t in timeframes_to_process if t not in allowed_timeframes]
    if invalid_tfs:
        logger.warning(f"–ü–∞–π–ø–ª–∞–π–Ω –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã: {', '.join(invalid_tfs)}. –î–æ–ø—É—Å—Ç–∏–º—ã–µ: {', '.join(allowed_timeframes)}")
    if not valid_tfs:
        logger.error("–ù–µ —É–∫–∞–∑–∞–Ω –Ω–∏ –æ–¥–∏–Ω –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞. –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω.")
        sys.exit(1)
    timeframes_to_process = valid_tfs # Use only valid timeframes

    # Determine symbol/group filter arguments to pass to subsequent scripts
    filter_args = []
    filter_description_suffix = ""
    if args.symbol:
        filter_args = ["--symbol", args.symbol.upper()] # Pass uppercase symbol
        filter_description_suffix = f" –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {args.symbol.upper()}"
        # Note: update_data.py --symbol mode is for single symbol/tf.
        # The pipeline will still run update_data.py --full/--mini for CORE_SYMBOLS,
        # and the filter will be applied in preprocess/train/predict.
        logger.warning("–í —Ä–µ–∂–∏–º–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ñ–∏–ª—å—Ç—Ä --symbol –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ —à–∞–≥–∞–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞.")
    elif args.symbol_group:
        group_name = args.symbol_group.lower()
        if group_name not in SYMBOL_GROUPS and group_name != 'all': # Allow 'all' special key
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤ '{args.symbol_group}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≥—Ä—É–ø–ø—ã: {list(SYMBOL_GROUPS.keys())} –∏–ª–∏ 'all'. –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω.")
            sys.exit(1)
        filter_args = ["--symbol-group", group_name] # Pass lowercase group name
        filter_description_suffix = f" –¥–ª—è –≥—Ä—É–ø–ø—ã {group_name}"
        logger.warning("–í —Ä–µ–∂–∏–º–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ñ–∏–ª—å—Ç—Ä --symbol-group –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ —à–∞–≥–∞–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞.")
    # If no filter args, filter_args remains empty, and subprocesses run without --symbol/--symbol-group


    print(f"[Pipeline] üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤: {', '.join(timeframes_to_process)}{filter_description_suffix}")
    if args.full_update:
        print("[Pipeline] –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º –ø–æ–ª–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.")
    elif args.mini_update or (not args.full_update and not args.mini_update): # mini is default
         print("[Pipeline] –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é).")
    if args.train:
        print("[Pipeline] –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–µ–Ω–æ.")
    if args.skip_predict:
        print("[Pipeline] –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω.")


    # --- Step 1: Data Update ---
    update_script = os.path.join(os.path.dirname(__file__), "update_data.py")
    update_command = [sys.executable, update_script]

    if args.full_update:
        update_command.append("--full")
        # Note: --full updates CORE_SYMBOLS, --tf can filter which TFs to update
        update_command.extend(["--tf"] + timeframes_to_process) # Pass specified TFs to update script
        step_desc = f"üì¶  –ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {', '.join(timeframes_to_process)}"
    else: # Default is mini update
        update_command.append("--mini")
        # Note: --mini updates CORE_SYMBOLS, --tf can filter which TFs to update
        update_command.extend(["--tf"] + timeframes_to_process) # Pass specified TFs to update script
        step_desc = f"üì•  –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤–µ—á–µ–π –¥–ª—è {', '.join(timeframes_to_process)}"

    return_code = run_step(step_desc, update_command)
    if return_code != 0:
        # run_step already logs and prints error
        sys.exit(return_code)


    # --- Step 2: Preprocess Features ---
    preprocess_script = os.path.join(os.path.dirname(__file__), "preprocess.py")
    for tf_item in timeframes_to_process:
        preprocess_command = [sys.executable, preprocess_script, "--tf", tf_item]
        preprocess_command.extend(filter_args) # Pass symbol/group filter args

        step_desc = f"‚öôÔ∏è  [{tf_item}] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤{filter_description_suffix}"
        return_code = run_step(step_desc, preprocess_command)
        if return_code != 0:
            # run_step already logs and prints error. Exit pipeline.
            # If you want to continue processing other TFs despite an error,
            # you would move this sys.exit outside the TF loop and just log.
            # But typically, a failure to preprocess one TF means the pipeline should stop.
            sys.exit(return_code)


    # --- Step 3: Train Models (Conditional) ---
    if args.train:
        train_script = os.path.join(os.path.dirname(__file__), "train.py")
        # The train script expects a single key (--symbol or --symbol-group)
        # If filter_args is empty (no filter), we cannot call train.py directly.
        # Training models for 'all' requires a specific key, e.g., '--symbol-group all'.
        # If the pipeline is run *without* filter args, should it train 'all' models?
        # Or should it require a filter for training?
        # Let's make it require a filter for training for clarity.
        # If filter_args is empty, we skip training unless --symbol-group all is explicitly allowed.
        # The CLI parser already handles mutually exclusive symbols/groups.
        # We just need the *key* to pass to train.py.
        train_key = None
        if args.symbol:
            train_key = args.symbol.upper()
        elif args.symbol_group:
            train_key = args.symbol_group.lower()
            if train_key != 'all' and train_key not in SYMBOL_GROUPS:
                 # This validation should ideally happen earlier in argparse or when determining filter_args
                 # but double check here.
                 logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {args.symbol_group}. –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω.")
                 sys.exit(1)
        else:
             # This state should not be reached if --train implies a filter is needed
             # Let's add a check that if --train is used, either --symbol or --symbol-group must be provided.
             # Or, assume training 'all' is the default if --train but no filter.
             # Let's assume training 'all' is the default if --train is present but no filter is specified.
             train_key = 'all'
             filter_description_suffix = " (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã)" # Update desc if training 'all' by default


        logger.info(f"–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª—é—á–∞ '{train_key}' –Ω–∞ –¢–§: {', '.join(timeframes_to_process)}.")

        for tf_item in timeframes_to_process:
            train_command = [sys.executable, train_script, "--tf", tf_item]
            if args.symbol:
                 train_command.extend(["--symbol", train_key])
            elif args.symbol_group:
                 train_command.extend(["--symbol-group", train_key])
            else: # Default to training 'all' if --train is present but no filter
                 train_command.extend(["--symbol-group", 'all'])


            step_desc = f"üß†  [{tf_item}] –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª—é—á–∞ '{train_key}'"
            return_code = run_step(step_desc, train_command)
            if return_code != 0:
                sys.exit(return_code)
    else:
        logger.info("–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–ø—É—â–µ–Ω (--train –Ω–µ —É–∫–∞–∑–∞–Ω).")


    # --- Step 4: Generate Predictions (Conditional) ---
    if not args.skip_predict:
        predict_script = os.path.join(os.path.dirname(__file__), "predict.py")
        predict_command = [sys.executable, predict_script]
        predict_command.extend(["--tf"] + timeframes_to_process) # Pass specified TFs to predict script
        predict_command.extend(filter_args) # Pass symbol/group filter args
        predict_command.append("--save") # Always save predictions from pipeline? Or make it optional?
        # Let's make --save the default behavior in the pipeline for the predict step.

        step_desc = f"üîÆ  –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ{filter_description_suffix}"
        return_code = run_step(step_desc, predict_command)
        if return_code != 0:
            sys.exit(return_code)
    else:
        logger.info("–®–∞–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω (--skip-predict —É–∫–∞–∑–∞–Ω).")


    print("[Pipeline] üéâ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω.")


if __name__ == "__main__":
    # Ensure basic directories exist before running the pipeline
    os.makedirs(PATHS_CONFIG['data_dir'], exist_ok=True)
    os.makedirs(PATHS_CONFIG['logs_dir'], exist_ok=True)
    os.makedirs(PATHS_CONFIG['models_dir'], exist_ok=True)
    # database dir is handled by src.db.py init_db

    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\n[Pipeline] üõë –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C).")
        sys.exit(130)
    except SystemExit as e: # Catch sys.exit calls from run_step or validation errors
        if e.code == 130:
            logger.warning("\n[Pipeline] üõë –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω –∏–∑-–∑–∞ Ctrl+C –≤ –¥–æ—á–µ—Ä–Ω–µ–º –ø—Ä–æ—Ü–µ—Å—Å–µ.")
        elif e.code != 0: # Any non-zero exit code indicates an error
            logger.error("\n[Pipeline] üõë –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≤ –¥–æ—á–µ—Ä–Ω–µ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.")
        sys.exit(e.code)
    except Exception as e:
        logger.error(f"[Pipeline] üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}", exc_info=True)
        print(f"\n[Pipeline] üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
        sys.exit(1) # Exit with a general error code