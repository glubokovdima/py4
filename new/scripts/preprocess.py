# scripts/preprocess.py

import pandas as pd
import os
import argparse
from tqdm import tqdm
import sys
import logging

# Ensure the src directory is in the Python path if necessary (usually handled by running from project root)
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

# Import config, logging setup, and db module
from src.utils.config import get_config
from src.utils.logging_setup import setup_logging
from src import db # Import the db module

# Import feature computation functions from the new module
from src.features.build import compute_all_features_for_symbol, compute_btc_features, apply_flat_filter

# --- Initial Setup ---
# Load configuration
config = get_config()
FEATURE_BUILDER_CONFIG = config['feature_builder']
PATHS_CONFIG = config['paths']
SYMBOL_GROUPS = config['symbol_groups'] # Get symbol groups from config

# Configure logging for this script
# Note: In a pipeline or via cli.py, logging might already be set up.
# Calling setup_logging again is generally safe if it uses dictConfig,
# but logging.basicConfig should be avoided if calling setup_logging.
# Let's assume setup_logging handles idempotency or is called once by the entry point.
setup_logging()
logger = logging.getLogger(__name__) # Use logger specific to this module

# --- Constants from Config ---
# We now get most feature-related constants from FEATURE_BUILDER_CONFIG
MIN_DATA_FOR_FEATURES = FEATURE_BUILDER_CONFIG.get('min_data_for_features', 100) # Default if not in config
DROPNNA_COLS = FEATURE_BUILDER_CONFIG['dropna_columns'] # List of columns to dropna on


# --- Main Script Logic ---

def main_preprocess(tf_arg, symbols_filter=None):
    """
    Main function to load data, compute features, filter, and save.

    Args:
        tf_arg (str): Timeframe key to process.
        symbols_filter (list, optional): List of symbols to filter by.
                                         If None, process all symbols in the loaded data.
    """
    logger.info(f"‚öôÔ∏è  –ù–∞—á–∞–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf_arg}")

    # --- Determine target symbols and file suffix ---
    # symbols_filter is passed directly now based on CLI args in __main__
    # Let's derive the suffix from the symbols_filter list if provided, or default to 'all'
    model_name_suffix_for_files = "all"
    log_processing_details = " (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –ë–î –¥–ª—è —ç—Ç–æ–≥–æ –¢–§)"

    if symbols_filter is not None:
         # Try to find if the symbols_filter list matches a known group for naming
         matched_group_name = None
         # Convert symbols_filter to a set for efficient comparison
         symbols_filter_set = set(symbols_filter)
         for group_name, group_symbols in SYMBOL_GROUPS.items():
             if symbols_filter_set == set(group_symbols):
                 matched_group_name = group_name
                 break

         if matched_group_name:
             model_name_suffix_for_files = matched_group_name
             log_processing_details = f" –≥—Ä—É–ø–ø—ã '{matched_group_name}' (—Å–∏–º–≤–æ–ª—ã: {', '.join(symbols_filter)})"
         elif len(symbols_filter) == 1:
             model_name_suffix_for_files = symbols_filter[0] # Use symbol name as suffix
             log_processing_details = f" —Å–∏–º–≤–æ–ª–∞ '{symbols_filter[0]}'"
         else:
             # For a custom list, maybe create a hash or just use a generic 'custom' + count/prefix?
             # Let's use 'custom_N_symbols' for clarity
             model_name_suffix_for_files = f"custom_{len(symbols_filter)}"
             log_processing_details = f" —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ ({', '.join(symbols_filter)})"

         logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details}. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É —Å–ø–∏—Å–∫—É.")
    else:
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details}. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã.")

    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤: {model_name_suffix_for_files}")


    # --- Load data ---
    # Load ALL symbols for the given TF initially. Filtering happens next.
    df_all_candles = db.load_candles(tf_arg)

    if df_all_candles.empty:
        logger.error(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î –¥–ª—è {tf_arg}. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
        return

    # --- Filter data if symbols_filter is provided ---
    if symbols_filter is not None:
        original_symbols_count = df_all_candles['symbol'].nunique()
        original_rows_count = len(df_all_candles)

        if 'symbol' not in df_all_candles.columns:
            logger.error("–ö–æ–ª–æ–Ω–∫–∞ 'symbol' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
            return

        df_all_candles = df_all_candles[df_all_candles['symbol'].isin(symbols_filter)].copy() # Use .copy()

        if df_all_candles.empty:
            logger.warning(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º {symbols_filter} –¥–ª—è –¢–§ {tf_arg} –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å. "
                            f"(–ò—Å—Ö–æ–¥–Ω–æ –±—ã–ª–æ {original_rows_count} —Å—Ç—Ä–æ–∫ –¥–ª—è {original_symbols_count} —Å–∏–º–≤–æ–ª–æ–≤). "
                            f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ –±—É–¥–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∞.")
            return
        else:
            filtered_symbols_count = df_all_candles['symbol'].nunique()
            logger.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ —Å–∏–º–≤–æ–ª–∞–º. –û—Å—Ç–∞–ª–æ—Å—å {len(df_all_candles)} —Å—Ç—Ä–æ–∫ –¥–ª—è {filtered_symbols_count} —Å–∏–º–≤–æ–ª–æ–≤.")
            symbols_present_after_filter = df_all_candles['symbol'].unique().tolist()
            missing_requested_symbols = [s for s in symbols_filter if s not in symbols_present_after_filter]
            if missing_requested_symbols:
                 logger.warning(f"–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {tf_arg}: {missing_requested_symbols}")


    # --- Prepare BTC features (if BTCUSDT is in the data after filtering) ---
    btc_features_prepared = None
    if 'BTCUSDT' in df_all_candles['symbol'].unique():
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ BTCUSDT...")
        df_btc_raw = df_all_candles[df_all_candles['symbol'] == 'BTCUSDT'].copy()
        # Call the function from src.features.build
        btc_features_prepared = compute_btc_features(df_btc_raw)

        if btc_features_prepared is None or btc_features_prepared.empty:
             logger.warning("BTCUSDT features could not be computed or resulted in an empty DataFrame.")
        else:
             logger.info(f"BTCUSDT features computed and ready for merge: {len(btc_features_prepared)} rows.")
    else:
        logger.info("BTCUSDT not present in the data for this TF/filter. Skipping BTC features.")


    # --- Compute features for each symbol ---
    df_to_process = df_all_candles # This is already filtered if symbols_filter was provided
    if df_to_process.empty:
        logger.error(
            f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –¢–§ {tf_arg} –¥–ª—è{log_processing_details} (df_to_process –ø—É—Å—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏).")
        return

    all_symbols_features = []
    unique_symbols_in_data = df_to_process['symbol'].unique()

    if len(unique_symbols_in_data) == 0:
         logger.warning(f"–ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ df_to_process –¥–ª—è –¢–§ {tf_arg}.")
         return

    logger.info(f"–í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è {len(unique_symbols_in_data)} —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¢–§ {tf_arg}...")

    for symbol_val in tqdm(unique_symbols_in_data, desc=f"–ü—Ä–∏–∑–Ω–∞–∫–∏ {tf_arg}", unit="symbol"):
        df_sym = df_to_process[df_to_process['symbol'] == symbol_val].copy()
        # No need to sort or set index here, compute_all_features_for_symbol handles it

        # Call the main feature computation function from src.features.build
        # This function handles checking MIN_DATA_FOR_FEATURES internally
        df_sym_features = compute_all_features_for_symbol(df_sym, tf_arg, btc_features_prepared)

        if not df_sym_features.empty:
            # compute_all_features_for_symbol returns with timestamp as index, reset it
            df_sym_features = df_sym_features.reset_index()
            df_sym_features['symbol'] = symbol_val # Ensure symbol column is present
            all_symbols_features.append(df_sym_features)
        else:
             logger.debug(f"Feature computation for {symbol_val} on {tf_arg} resulted in an empty DataFrame or insufficient data after checks.")


    if not all_symbols_features:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –Ω–∞ –¢–§ {tf_arg} –¥–ª—è{log_processing_details}. –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω.")
        return

    full_features_df = pd.concat(all_symbols_features).reset_index(drop=True)
    logger.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ {tf_arg}. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ dropna: {len(full_features_df)}.")


    # --- Apply Flat Filter ---
    # Apply the flat filter to the combined DataFrame.
    # apply_flat_filter expects timestamp as a column, so ensure it's reset if needed.
    # compute_all_features_for_symbol returns with index reset, so this is fine.
    len_before_flat_filter = len(full_features_df)
    # Call the function from src.features.build
    full_features_df = apply_flat_filter(full_features_df)
    len_after_flat_filter = len(full_features_df)
    if len_before_flat_filter > len_after_flat_filter:
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len_before_flat_filter - len_after_flat_filter} —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–ª–æ—Å–∫–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞.")
    else:
        logger.debug("–ü–ª–æ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –Ω–µ —É–¥–∞–ª–∏–ª –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.")


    # --- Drop NaNs ---
    # Drop rows with NaN in essential columns required for training
    len_before_dropna = len(full_features_df)
    if DROPNNA_COLS and not full_features_df.empty:
         # Check if DROPNNA_COLS actually exist in the DataFrame
         cols_to_drop_subset = [col for col in DROPNNA_COLS if col in full_features_df.columns]
         if cols_to_drop_subset:
              full_features_df.dropna(subset=cols_to_drop_subset, inplace=True)
              len_after_dropna = len(full_features_df)
              if len_before_dropna > len_after_dropna:
                   logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len_before_dropna - len_after_dropna} —Å—Ç—Ä–æ–∫ –∏–∑-–∑–∞ NaN –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö ({', '.join(cols_to_drop_subset)}).")
              else:
                   logger.debug("dropna –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö –Ω–µ —É–¥–∞–ª–∏–ª –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.")
         else:
              logger.warning(f"–ù–∏ –æ–¥–Ω–∞ –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è dropna ({', '.join(DROPNNA_COLS)}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ DataFrame. dropna –ø—Ä–æ–ø—É—â–µ–Ω.")
              len_after_dropna = len_before_dropna # No change in length
    else:
        logger.warning("–°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è dropna –ø—É—Å—Ç –∏–ª–∏ DataFrame –ø—É—Å—Ç. dropna –ø—Ä–æ–ø—É—â–µ–Ω.")
        len_after_dropna = len_before_dropna # No change in length


    if full_features_df.empty:
        logger.error(f"–ò—Ç–æ–≥–æ–≤—ã–π DataFrame –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details} –ø—É—Å—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ dropna. –§–∞–π–ª—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã.")
        return

    logger.info(f"–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf_arg} –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Ç–æ–≥–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ dropna: {len(full_features_df)}.")


    # --- Save results ---
    os.makedirs(PATHS_CONFIG['data_dir'], exist_ok=True)

    output_pickle_path = os.path.join(PATHS_CONFIG['data_dir'], f"features_{model_name_suffix_for_files}_{tf_arg}.pkl")
    output_sample_csv_path = os.path.join(PATHS_CONFIG['data_dir'], f"sample_{model_name_suffix_for_files}_{tf_arg}.csv")

    try:
        full_features_df.to_pickle(output_pickle_path)
        logger.info(f"üíæ  –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_pickle_path}, —Ñ–æ—Ä–º–∞: {full_features_df.shape}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Pickle —Ñ–∞–π–ª–∞ {output_pickle_path}: {e}")

    try:
        sample_size = min(1000, len(full_features_df))
        if sample_size > 0:
            # Ensure timestamp is in a readable format for CSV sample
            df_sample = full_features_df.head(sample_size).copy()
            if 'timestamp' in df_sample.columns and pd.api.types.is_datetime64_any_dtype(df_sample['timestamp']):
                df_sample['timestamp'] = df_sample['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')
            # Save with timestamp column for sample visibility
            df_sample.to_csv(output_sample_csv_path, index=False)
            logger.info(f"üìÑ  –°—ç–º–ø–ª –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_sample_csv_path} ({sample_size} —Å—Ç—Ä–æ–∫)")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV —Å—ç–º–ø–ª–∞ {output_sample_csv_path}: {e}")

    logger.info(f"‚úÖ  –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf_arg} –¥–ª—è{log_processing_details}")


# --- Command Line Interface ---

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π –∏–∑ SQLite.")
    parser.add_argument('--tf', type=str, required=True, help='–¢–∞–π–º—Ñ—Ä–µ–π–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 5m, 15m)')
    # Allow specifying symbol, symbol-group, or a list of symbols
    symbol_group = parser.add_mutually_exclusive_group()
    symbol_group.add_argument('--symbol', type=str, default=None, help="–°–∏–º–≤–æ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: BTCUSDT)")
    symbol_group.add_argument('--symbol-group', type=str, help="–ü—Å–µ–≤–¥–æ–Ω–∏–º –≥—Ä—É–ø–ø—ã –º–æ–Ω–µ—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: top8, meme)")
    symbol_group.add_argument('--symbol-list', nargs='+', help="–°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: BTCUSDT ETHUSDT ...)")


    args = parser.parse_args()

    # Determine the list of symbols to process based on arguments
    symbols_to_process = None # Default to None to process all
    log_filter_source = None

    if args.symbol_group:
        group_name = args.symbol_group.lower()
        if group_name in SYMBOL_GROUPS:
            symbols_to_process = SYMBOL_GROUPS[group_name]
            log_filter_source = f"–≥—Ä—É–ø–ø–∞ '{group_name}'"
        else:
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤: {args.symbol_group}. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≥—Ä—É–ø–ø—ã: {list(SYMBOL_GROUPS.keys())}")
            sys.exit(1)
    elif args.symbol_list:
        symbols_to_process = [s.upper() for s in args.symbol_list] # Ensure uppercase
        log_filter_source = f"—Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ ({len(symbols_to_process)})"
    elif args.symbol:
        symbols_to_process = [args.symbol.upper()] # Ensure uppercase
        log_filter_source = f"—Å–∏–º–≤–æ–ª '{args.symbol.upper()}'"

    if log_filter_source:
         logger.info(f"–§–∏–ª—å—Ç—Ä —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞–¥–∞–Ω —á–µ—Ä–µ–∑ {log_filter_source}.")
    else:
         logger.info("–§–∏–ª—å—Ç—Ä —Å–∏–º–≤–æ–ª–æ–≤ –Ω–µ –∑–∞–¥–∞–Ω. –ë—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –ë–î –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ TF.")


    try:
        main_preprocess(args.tf, symbols_to_process)
    except KeyboardInterrupt:
        logger.warning(f"\n–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {args.tf} –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C).")
        sys.exit(130)
    except Exception as e:
        logger.error(f"üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {args.tf}: {e}", exc_info=True)
        sys.exit(1)