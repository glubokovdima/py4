# src/models/evaluation.py

import pandas as pd
import numpy as np
import os
import logging

# Import sklearn metrics
from sklearn.metrics import (
    classification_report, mean_absolute_error, accuracy_score,
    f1_score, precision_score, confusion_matrix, roc_auc_score,
    average_precision_score
)

# Import config
from src.utils.config import get_config

# Get logger for this module
logger = logging.getLogger(__name__)

# --- Load Configuration ---
config = get_config()
PATHS_CONFIG = config['paths']
# No specific evaluation metrics thresholds needed from config for this module,
# as the evaluation functions just report standard metrics.

# Target name mapping (must be consistent with feature building/training)
# In src/models/train.py, we mapped {'DOWN': 0, 'UP': 1}
TARGET_CLASS_NAMES_MAPPED = {0: 'DOWN', 1: 'UP'}


# --- Evaluation Function ---

def evaluate_models(models, X_test, y_test_class, y_test_delta, y_test_vol, y_test_tp_hit, file_prefix, log_context_str):
    """
    Evaluates trained models on the test set and logs the metrics.

    Args:
        models (dict): Dictionary containing trained model instances
                       (keys: 'clf_class', 'reg_delta', 'reg_vol', 'clf_tp_hit').
                       Model instance can be None if not trained.
        X_test (pd.DataFrame): Test features.
        y_test_class (pd.Series): Test target for classification (0/1 integers).
        y_test_delta (pd.Series): Test target for delta regression (float).
        y_test_vol (pd.Series): Test target for volatility regression (float).
        y_test_tp_hit (pd.Series): Test target for TP-hit classification (0/1 integers).
        file_prefix (str): Prefix for log file names (e.g., "symbol_tf").
        log_context_str (str): String describing the current training context for logging.
    """
    logger.info(f"\nüìä  –ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ ({log_context_str}):")

    if X_test.empty:
        logger.warning(f"X_test –ø—É—Å—Ç, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {log_context_str}. –ü—Ä–æ–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏.")
        return

    # Use a dedicated log file for metrics for this specific training run
    metrics_log_path = os.path.join(PATHS_CONFIG['logs_dir'], f"train_metrics_{file_prefix}.txt")
    os.makedirs(PATHS_CONFIG['logs_dir'], exist_ok=True) # Ensure log directory exists

    try:
        # Open the log file in append mode, as training logs might have already started it
        # Or, better, open in write mode to ensure a fresh metrics log per run
        # Let's open in write mode to overwrite if re-running training
        with open(metrics_log_path, "w", encoding="utf-8") as f_log:
            f_log.write(f"=== –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {log_context_str} ({pd.Timestamp.now()}) ===\n\n")

            # --- Metrics for clf_class ---
            f_log.write(f"--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (clf_class) ---\n")
            clf_class_model = models.get('clf_class')
            if clf_class_model and not y_test_class.empty:
                try:
                    # Ensure y_test_class is integer type for evaluation
                    y_test_class_int = y_test_class.astype(int).squeeze() # Ensure flat series
                    y_pred_class_test = clf_class_model.predict(X_test)
                    proba_class_test = clf_class_model.predict_proba(X_test)

                    # Ensure predictions are flat
                    y_pred_class_flat = pd.Series(y_pred_class_test).squeeze()

                    # Handle potential single class in test set for reporting
                    # Use unique labels from both true and predicted for report labels
                    unique_labels = sorted(list(set(y_test_class_flat.unique()) | set(y_pred_class_flat.unique())))
                    # Map integer labels back to names for report
                    target_names_for_report = [TARGET_CLASS_NAMES_MAPPED.get(i, f'Class_{i}') for i in unique_labels]

                    try:
                        report_class_str = classification_report(y_test_class_flat, y_pred_class_flat,
                                                                 labels=unique_labels, # Use actual labels present
                                                                 target_names=target_names_for_report,
                                                                 digits=4, zero_division=0)
                        f_log.write(report_class_str + "\n")
                        logger.info(f"\nClassification Report (clf_class) for {log_context_str}:\n{report_class_str}")
                    except Exception as e_report:
                        f_log.write("Classification report failed.\n")
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ classification_report –¥–ª—è clf_class ({log_context_str}): {e_report}. –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.")


                    # Calculate individual metrics
                    try:
                        acc_class_val = accuracy_score(y_test_class_flat, y_pred_class_flat)
                        f_log.write(f"Accuracy: {acc_class_val:.4f}\n")
                        logger.info(f"Accuracy (clf_class): {acc_class_val:.4f}")
                    except Exception: pass # Ignore if calculation fails

                    # Need to find the index of class 1 for F1, Precision, AUC
                    model_classes_clf = getattr(clf_class_model, 'classes_', [0, 1]) # Default to [0, 1] if .classes_ not found
                    try:
                        # Find the index corresponding to the integer label 1
                        positive_class_idx_clf = list(model_classes_clf).index(1)
                    except ValueError:
                        logger.warning(f"Positive class integer label '1' not found in clf_class.classes_ ({model_classes_clf}). Cannot compute F1/Precision/AUC for class 1.")
                        positive_class_idx_clf = None # Cannot proceed with class-specific metrics


                    if positive_class_idx_clf is not None:
                         # Check if class 1 is actually present in the true test labels before calculating pos_label metrics
                         if 1 in y_test_class_flat.unique():
                              try:
                                 f1_class_val = f1_score(y_test_class_flat, y_pred_class_flat, pos_label=1, zero_division=0)
                                 f_log.write(f"F1-score (UP/1): {f1_class_val:.4f}\n")
                                 logger.info(f"F1-score (UP/1, clf_class): {f1_class_val:.4f}")
                              except Exception: pass # Ignore if calculation fails

                              try:
                                 precision_class_val = precision_score(y_test_class_flat, y_pred_class_flat, pos_label=1, zero_division=0)
                                 f_log.write(f"Precision (UP/1): {precision_class_val:.4f}\n")
                                 logger.info(f"Precision (UP/1, clf_class): {precision_class_val:.4f}")
                              except Exception: pass # Ignore if calculation fails
                         else:
                              f_log.write("F1/Precision for UP/1 not calculated (class 1 not in y_test).\n")
                              logger.warning(f"Class 1 ('UP') not present in y_test_class_flat for {log_context_str}. Skipping F1/Precision for UP.")


                         # ROC AUC and PR AUC require probability scores for the positive class
                         if len(np.unique(y_test_class_flat)) > 1: # Need at least two classes in true labels
                              try:
                                  # Select probability column corresponding to integer label 1
                                  y_score_probs_clf = proba_class_test[:, positive_class_idx_clf]
                                  auc_class_val = roc_auc_score(y_test_class_flat, y_score_probs_clf)
                                  f_log.write(f"ROC AUC (UP/1): {auc_class_val:.4f}\n")
                                  logger.info(f"ROC AUC (UP/1, clf_class): {auc_class_val:.4f}")
                              except Exception as e_auc:
                                  f_log.write("ROC AUC calculation failed.\n")
                                  logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å ROC AUC –¥–ª—è clf_class ({log_context_str}): {e_auc}")

                              try:
                                  pr_auc_class_val = average_precision_score(y_test_class_flat, y_score_probs_clf)
                                  f_log.write(f"PR AUC (UP/1): {pr_auc_class_val:.4f}\n")
                                  logger.info(f"PR AUC (UP/1, clf_class): {pr_auc_class_val:.4f}")
                              except Exception as e_pr_auc:
                                   f_log.write("PR AUC calculation failed.\n")
                                   logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å PR AUC –¥–ª—è clf_class ({log_context_str}): {e_pr_auc}")

                         else:
                              f_log.write("ROC AUC/PR AUC not calculated (only one class in y_test).\n")
                              logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å ROC AUC/PR AUC –¥–ª—è clf_class ({log_context_str}): –≤ y_test_class_flat —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å.")


                    # Confusion Matrix
                    try:
                         # Use labels [0, 1] if both are present in test/pred for consistent CM structure
                         cm_labels_for_matrix = [0, 1]
                         # Ensure these labels are actually in the unique labels set before trying to create CM
                         if set(cm_labels_for_matrix).issubset(set(y_test_class_flat.unique()) | set(y_pred_class_flat.unique())):
                              cm = confusion_matrix(y_test_class_flat, y_pred_class_flat, labels=cm_labels_for_matrix)
                              cm_df = pd.DataFrame(cm,
                                                   index=[f"True_{TARGET_CLASS_NAMES_MAPPED.get(l, l)}" for l in cm_labels_for_matrix],
                                                   columns=[f"Pred_{TARGET_CLASS_NAMES_MAPPED.get(l, l)}" for l in cm_labels_for_matrix])
                              f_log.write("\nConfusion Matrix (clf_class):\n")
                              f_log.write(cm_df.to_string() + "\n")
                              logger.info(f"\nConfusion Matrix (clf_class) for {log_context_str}:\n{cm_df}")
                         else:
                              f_log.write("Confusion Matrix not calculated (not enough unique labels in test/pred).\n")
                              logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤ y_test_class_flat/y_pred_class_flat –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã —Å–º–µ—â–µ–Ω–∏—è ({log_context_str}).")

                    except Exception as e_cm:
                          f_log.write("Confusion Matrix calculation failed.\n")
                          logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Confusion Matrix –¥–ª—è clf_class ({log_context_str}): {e_cm}")


                except Exception as e:
                    f_log.write(f"Error calculating clf_class metrics: {e}\n")
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫ clf_class –¥–ª—è {log_context_str}: {e}", exc_info=True)
            else:
                f_log.write("Metrics not calculated (model not trained or test data empty).\n")
                logger.warning(
                    f"–ú–µ—Ç—Ä–∏–∫–∏ clf_class –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è {log_context_str} (–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ y_test_class –ø—É—Å—Ç).")


            # --- Metrics for reg_delta ---
            f_log.write("\n--- –†–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–µ–ª—å—Ç—ã (reg_delta) ---\n")
            reg_delta_model = models.get('reg_delta')
            if reg_delta_model and not X_test.empty and not y_test_delta.empty:
                try:
                    y_pred_delta_test = reg_delta_model.predict(X_test)
                    mae_delta_val = mean_absolute_error(y_test_delta, y_pred_delta_test)
                    f_log.write(f"MAE Delta: {mae_delta_val:.6f}\n")
                    logger.info(f"MAE Delta: {mae_delta_val:.6f}")
                except Exception as e:
                    f_log.write(f"Error calculating MAE Delta: {e}\n")
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ MAE Delta –¥–ª—è {log_context_str}: {e}")
            else:
                f_log.write("Metrics not calculated (model not trained or test data empty).\n")
                logger.warning(f"MAE Delta –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –¥–ª—è {log_context_str} (–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ y_test_delta –ø—É—Å—Ç).")


            # --- Metrics for reg_vol ---
            f_log.write("\n--- –†–µ–≥—Ä–µ—Å—Å–æ—Ä –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (reg_vol) ---\n")
            reg_vol_model = models.get('reg_vol')
            if reg_vol_model and not X_test.empty and not y_test_vol.empty:
                try:
                    y_pred_vol_test = reg_vol_model.predict(X_test)
                    mae_vol_val = mean_absolute_error(y_test_vol, y_pred_vol_test)
                    f_log.write(f"MAE Volatility: {mae_vol_val:.6f}\n")
                    logger.info(f"MAE Volatility: {mae_vol_val:.6f}")
                except Exception as e:
                    f_log.write(f"Error calculating MAE Volatility: {e}\n")
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ MAE Volatility –¥–ª—è {log_context_str}: {e}")
            else:
                f_log.write("Metrics not calculated (model not trained or test data empty).\n")
                logger.warning(f"MAE Volatility –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –¥–ª—è {log_context_str} (–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ y_test_vol –ø—É—Å—Ç).")


            # --- Metrics for clf_tp_hit ---
            clf_tp_hit_model = models.get('clf_tp_hit')
            if clf_tp_hit_model: # Only include section if TP-hit model was trained (and exists)
                 f_log.write("\n--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä TP-Hit (clf_tp_hit) ---\n")
                 if not X_test.empty and not y_test_tp_hit.empty:
                     try:
                         y_test_tp_hit_int = y_test_tp_hit.astype(int).squeeze() # Ensure flat series
                         y_pred_tp_hit_test = clf_tp_hit_model.predict(X_test)

                         # Ensure predictions are flat and valid (not NaN)
                         y_pred_tp_hit_flat = pd.Series(y_pred_tp_hit_test).squeeze().astype(int) # Ensure int type

                         if not y_test_tp_hit_flat.empty and not y_pred_tp_hit_flat.empty:
                              # Handle potential single class in test set for reporting
                              unique_labels_tp = sorted(list(set(y_test_tp_hit_flat.unique()) | set(y_pred_tp_hit_flat.unique())))
                              target_names_tp = {0: 'No TP Hit (0)', 1: 'TP Hit (1)'}
                              target_names_for_report_tp = [target_names_tp.get(i, f'Class_{i}') for i in unique_labels_tp]


                              try:
                                 report_tp_hit_str = classification_report(y_test_tp_hit_flat, y_pred_tp_hit_flat,
                                                                           labels=unique_labels_tp, # Use actual labels present
                                                                           target_names=target_names_for_report_tp,
                                                                           digits=4, zero_division=0)
                                 f_log.write(report_tp_hit_str + "\n")
                                 logger.info(f"\nClassification Report (clf_tp_hit) for {log_context_str}:\n{report_tp_hit_str}")
                              except Exception as e_report_tp:
                                 f_log.write("Classification report failed.\n")
                                 logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ classification_report –¥–ª—è clf_tp_hit ({log_context_str}): {e_report_tp}. –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.")

                              # Calculate individual metrics
                              try:
                                  acc_tp_hit_val = accuracy_score(y_test_tp_hit_flat, y_pred_tp_hit_flat)
                                  f_log.write(f"Accuracy: {acc_tp_hit_val:.4f}\n")
                                  logger.info(f"Accuracy (clf_tp_hit): {acc_tp_hit_val:.4f}")
                              except Exception: pass # Ignore if calculation fails

                              # Check if class 1 is actually present in the true test labels before calculating pos_label metrics
                              if 1 in y_test_tp_hit_flat.unique():
                                   try:
                                      # Calculate F1 for the positive class (1)
                                      f1_tp_hit_val = f1_score(y_test_tp_hit_flat, y_pred_tp_hit_flat, zero_division=0, pos_label=1)
                                      f_log.write(f"F1-score (TP-hit/1): {f1_tp_hit_val:.4f}\n")
                                      logger.info(f"F1-score (TP-hit/1, clf_tp_hit): {f1_tp_hit_val:.4f}")
                                   except Exception: pass # Ignore if calculation fails
                              else:
                                   f_log.write("F1 for TP-hit/1 not calculated (class 1 not in y_test).\n")
                                   logger.warning(f"Class 1 ('TP Hit') not present in y_test_tp_hit_flat for {log_context_str}. Skipping F1 for TP Hit.")


                              # Confusion Matrix
                              try:
                                   # Use labels [0, 1] if both are present in test/pred for consistent CM structure
                                   cm_labels_for_matrix_tp = [0, 1]
                                   if set(cm_labels_for_matrix_tp).issubset(set(y_test_tp_hit_flat.unique()) | set(y_pred_tp_hit_flat.unique())):
                                      cm_tp_hit = confusion_matrix(y_test_tp_hit_flat, y_pred_tp_hit_flat, labels=cm_labels_for_matrix_tp)
                                      cm_tp_hit_df = pd.DataFrame(cm_tp_hit,
                                                                   index=[f"True_{target_names_tp.get(l, l)}" for l in cm_labels_for_matrix_tp],
                                                                   columns=[f"Pred_{target_names_tp.get(l, l)}" for l in cm_labels_for_matrix_tp])
                                      f_log.write("\nConfusion Matrix (clf_tp_hit):\n")
                                      f_log.write(cm_tp_hit_df.to_string() + "\n")
                                      logger.info(f"\nConfusion Matrix (clf_tp_hit) for {log_context_str}:\n{cm_tp_hit_df}")
                                   else:
                                      f_log.write("Confusion Matrix not calculated (not enough unique labels in test/pred).\n")
                                      logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤ y_test_tp_hit_flat/y_pred_tp_hit_flat –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã —Å–º–µ—â–µ–Ω–∏—è ({log_context_str}).")

                              except Exception as e_cm_tp:
                                   f_log.write("Confusion Matrix calculation failed.\n")
                                   logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Confusion Matrix –¥–ª—è clf_tp_hit ({log_context_str}): {e_cm_tp}")


                         else:
                             f_log.write("Metrics not calculated (no valid test/pred data after alignment).\n")
                             logger.warning(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ y_test_tp_hit_flat –∏–ª–∏ y_pred_tp_hit_flat –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ TP-hit ({log_context_str}).")

                     except Exception as e:
                         f_log.write(f"Error calculating clf_tp_hit metrics: {e}\n")
                         logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫ clf_tp_hit –¥–ª—è {log_context_str}: {e}", exc_info=True)
                 else:
                     f_log.write("Metrics not calculated (model not trained or test data empty).\n")
                     logger.warning(f"–ú–µ—Ç—Ä–∏–∫–∏ clf_tp_hit –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è {log_context_str} (–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ y_test_tp_hit –ø—É—Å—Ç).")
            else:
                 f_log.write("\n--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä TP-Hit (clf_tp_hit) ---\n")
                 f_log.write("Model was not trained (column missing or data insufficient).\n")
                 logger.info(f"TP-hit model was not trained for {log_context_str}. Skipping evaluation.")


            # Optionally write selected features list to metrics log for completeness
            # This requires passing the selected features list to this function
            # For now, it's saved separately by the train orchestrator function.
            # We could load it here, but passing it is cleaner. Let's add it as an argument.
            # Add feature_cols_final argument to evaluate_models function signature.
            # feature_cols_final = models.get('feature_cols_final') # Or pass it explicitly
            # if feature_cols_final:
            #     f_log.write(f"\nUsed Features ({len(feature_cols_final)}):\n" + ", ".join(feature_cols_final) + "\n")


        logger.info(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ª–æ–≥: {metrics_log_path}")

    except Exception as e_log_save:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏/–∑–∞–ø–∏—Å–∏ –ª–æ–≥–∞ –º–µ—Ç—Ä–∏–∫ ({metrics_log_path}): {e_log_save}")


# Example usage (optional, for testing the module directly)
if __name__ == "__main__":
    # In a real run, logging and config would be set up by the entry script
    # For direct testing, set it up here
    from src.utils.logging_setup import setup_logging
    setup_logging()
    logger = logging.getLogger(__name__) # Re-get logger after setup
    config = get_config() # Re-load config after setup

    print("Testing src/models/evaluation.py")

    # Create dummy test data and dummy models for testing evaluation functions
    n_samples_test = 500
    n_features_test = 20

    # Create dummy test data
    dummy_X_test = pd.DataFrame(np.random.rand(n_samples_test, n_features_test), columns=[f'feature_{i}' for i in range(n_features_test)])
    # Create dummy test targets
    dummy_y_test_class = pd.Series(np.random.randint(0, 2, n_samples_test)).astype(int) # Binary (0/1)
    dummy_y_test_delta = pd.Series(np.random.randn(n_samples_test) * 0.01) # Float
    dummy_y_test_vol = pd.Series(np.random.rand(n_samples_test) * 0.02) # Float
    dummy_y_test_tp_hit = pd.Series(np.random.randint(0, 2, n_samples_test)).astype(int) # Binary (0/1)

    # Add some NaNs to simulate real data issues
    dummy_y_test_class.loc[dummy_y_test_class.sample(frac=0.05).index] = np.nan
    dummy_y_test_delta.loc[dummy_y_test_delta.sample(frac=0.05).index] = np.nan
    dummy_y_test_vol.loc[dummy_y_test_vol.sample(frac=0.05).index] = np.nan
    dummy_y_test_tp_hit.loc[dummy_y_test_tp_hit.sample(frac=0.05).index] = np.nan

    # Drop rows with NaN in test targets to simulate the data passed for evaluation
    # Evaluation functions expect clean y_test data relevant to the model
    valid_indices_class = dummy_y_test_class.dropna().index
    X_test_class_aligned = dummy_X_test.loc[valid_indices_class]
    y_test_class_clean = dummy_y_test_class.loc[valid_indices_class].astype(int)

    valid_indices_delta = dummy_y_test_delta.dropna().index
    X_test_delta_aligned = dummy_X_test.loc[valid_indices_delta]
    y_test_delta_clean = dummy_y_test_delta.loc[valid_indices_delta]

    valid_indices_vol = dummy_y_test_vol.dropna().index
    X_test_vol_aligned = dummy_X_test.loc[valid_indices_vol]
    y_test_vol_clean = dummy_y_test_vol.loc[valid_indices_vol]

    valid_indices_tp_hit = dummy_y_test_tp_hit.dropna().index
    X_test_tp_hit_aligned = dummy_X_test.loc[valid_indices_tp_hit]
    y_test_tp_hit_clean = dummy_y_test_tp_hit.loc[valid_indices_tp_hit].astype(int)


    # Create dummy model objects (minimalistic, only need predict and predict_proba)
    class DummyClassifier:
        def predict(self, X):
            # Simulate prediction (e.g., predict 0 or 1)
            return np.random.randint(0, 2, len(X))
        def predict_proba(self, X):
            # Simulate probabilities (e.g., random probabilities)
            probs = np.random.rand(len(X), 2)
            probs = probs / probs.sum(axis=1, keepdims=True) # Normalize
            return probs
        # Add classes_ attribute needed for evaluation
        classes_ = np.array([0, 1])

    class DummyRegressor:
        def predict(self, X):
            # Simulate regression prediction
            return np.random.randn(len(X)) * 0.01

    # Instantiate dummy models
    dummy_models = {
        'clf_class': DummyClassifier(),
        'reg_delta': DummyRegressor(),
        'reg_vol': DummyRegressor(),
        'clf_tp_hit': DummyClassifier(), # Assume TP-hit is also binary classification
    }

    # Test evaluation with all models present
    print("\n--- Evaluating with all dummy models ---")
    evaluate_models(dummy_models, X_test_class_aligned, y_test_class_clean, y_test_delta_clean, y_test_vol_clean, y_test_tp_hit_clean, "dummy_all_models", "dummy_context")

    # Test evaluation with some models missing
    print("\n--- Evaluating with some dummy models missing ---")
    dummy_models_missing = {
        'clf_class': dummy_models['clf_class'],
        'reg_delta': None, # Missing
        'reg_vol': dummy_models['reg_vol'],
        'clf_tp_hit': None, # Missing
    }
    evaluate_models(dummy_models_missing, X_test_class_aligned, y_test_class_clean, y_test_delta_clean, y_test_vol_clean, y_test_tp_hit_clean, "dummy_missing_models", "dummy_context_missing")

    print("\nEvaluation module test finished.")