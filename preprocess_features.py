import sqlite3
import pandas as pd
import numpy as np
import ta
import os
import argparse
from tqdm import tqdm
import sys  # –î–ª—è Ctrl+C
import logging
from ta.volatility import BollingerBands  # üìä [2] –ò–º–ø–æ—Ä—Ç –¥–ª—è BollingerBands

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s [Preprocess] - %(message)s',
                    stream=sys.stdout)

# üîß 1.2 –î–æ–±–∞–≤—å —Å–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø –≤–≤–µ—Ä—Ö—É —Ñ–∞–π–ª–∞:
SYMBOL_GROUPS = {
    "top8": [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT",
        "XRPUSDT", "ADAUSDT", "LINKUSDT", "AVAXUSDT"
    ],
    "meme": [
        "DOGEUSDT", "PEPEUSDT", "FLOKIUSDT", "WIFUSDT", "SHIBUSDT"
    ],
    "defi": [
        "UNIUSDT", "AAVEUSDT", "SUSHIUSDT", "COMPUSDT"
    ]
}

DB_PATH = 'database/market_data.db'
TARGET_SHIFT = 5
TP_THRESHOLD = 0.005


def load_candles_from_db(tf_key):
    logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–µ–π –∏–∑ candles_{tf_key}...")
    if not os.path.exists(DB_PATH):
        logging.error(f"–§–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö {DB_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return pd.DataFrame()

    conn = sqlite3.connect(DB_PATH)
    try:
        cursor = conn.cursor()
        cursor.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='candles_{tf_key}';")
        if cursor.fetchone() is None:
            logging.error(f"–¢–∞–±–ª–∏—Ü–∞ 'candles_{tf_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö {DB_PATH}.")
            return pd.DataFrame()

        df = pd.read_sql_query(f"SELECT * FROM candles_{tf_key}", conn)
        if df.empty:
            logging.warning(f"–¢–∞–±–ª–∏—Ü–∞ candles_{tf_key} –ø—É—Å—Ç–∞.")
            return pd.DataFrame()

        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df = df.sort_values(['symbol', 'timestamp']).reset_index(drop=True)
        logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π –∏–∑ candles_{tf_key} –¥–ª—è {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤.")
        return df
    except pd.io.sql.DatabaseError as e:
        logging.error(f"–û—à–∏–±–∫–∞ SQL –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ candles_{tf_key}: {e}")
        return pd.DataFrame()
    except Exception as e:
        logging.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–≤–µ—á–µ–π –¥–ª—è {tf_key}: {e}")
        return pd.DataFrame()
    finally:
        conn.close()


def classify_delta_value(delta_val):
    if pd.isna(delta_val): return np.nan
    if delta_val > 0.002:
        return 'UP'
    elif delta_val < -0.002:
        return 'DOWN'
    else:
        return np.nan


def compute_ta_features(df_group):
    # Lag-—Ñ–∏—á–∏

    # Lag –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    df_group['target_class_shift1'] = df_group['target_class'].shift(1)
    # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –ê–Ω–∞–ª–∏–∑–∞
    # RSI
    df_group['rsi'] = ta.momentum.RSIIndicator(close=df_group['close'], window=14).rsi()
    # EMA
    df_group['ema_20'] = ta.trend.EMAIndicator(close=df_group['close'], window=20).ema_indicator()
    df_group['ema_50'] = ta.trend.EMAIndicator(close=df_group['close'], window=50).ema_indicator()
    # MACD
    macd_indicator = ta.trend.MACD(close=df_group['close'], window_slow=26, window_fast=12, window_sign=9)
    df_group['macd'] = macd_indicator.macd()
    df_group['macd_signal'] = macd_indicator.macd_signal()
    df_group['macd_diff'] = macd_indicator.macd_diff()
    # OBV (On-Balance Volume)
    df_group['obv'] = ta.volume.OnBalanceVolumeIndicator(close=df_group['close'],
                                                         volume=df_group['volume']).on_balance_volume()
    # ATR (Average True Range)
    df_group['atr'] = ta.volatility.AverageTrueRange(high=df_group['high'], low=df_group['low'],
                                                     close=df_group['close'], window=14).average_true_range()

    # === Lag-—Ñ–∏—á–∏ –ø–æ RSI –∏ EMA ===
    for shift_val in [1, 2, 3]:
        df_group[f'rsi_shift{shift_val}'] = df_group['rsi'].shift(shift_val)
        df_group[f'ema_20_shift{shift_val}'] = df_group['ema_20'].shift(shift_val)
        df_group[f'ema_50_shift{shift_val}'] = df_group['ema_50'].shift(shift_val)

    # Lag –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    df_group['target_class_shift1'] = df_group['target_class'].shift(1) if 'target_class' in df_group.columns else 0
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)
    df_group['volume_z'] = (df_group['volume'] - df_group['volume'].rolling(window=20, min_periods=1).mean()) / \
                           (df_group['volume'].rolling(window=20, min_periods=1).std().replace(0, np.nan))
    # üìâ [1] –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ volume_z
    df_group['volume_z'] = df_group['volume_z'].clip(-5, 5)

    df_group['candle_body_size'] = abs(df_group['close'] - df_group['open'])
    df_group['candle_hl_range'] = df_group['high'] - df_group['low']
    df_group['is_doji'] = ((df_group['candle_body_size'] / (df_group['candle_hl_range'] + 1e-9)) < 0.1).astype(int)

    if isinstance(df_group.index, pd.DatetimeIndex):
        df_group['hour'] = df_group.index.hour
        df_group['minute'] = df_group.index.minute
        df_group['dayofweek'] = df_group.index.dayofweek
        df_group['dayofmonth'] = df_group.index.day
        df_group['weekofyear'] = df_group.index.isocalendar().week.astype(int)
    else:
        for col in ['hour', 'dayofweek', 'minute', 'dayofmonth', 'weekofyear']:  # Added missing cols to default to NaN
            if col not in df_group.columns:
                df_group[col] = np.nan

    df_group['ema_20_slope'] = df_group['ema_20'].diff()
    df_group['ema_50_slope'] = df_group['ema_50'].diff()

    if 'rsi' in df_group.columns and len(df_group['rsi']) > 1:
        df_group['rsi_cross_50'] = ((df_group['rsi'] > 50) & (df_group['rsi'].shift(1) <= 50)).astype(int)
    else:
        df_group['rsi_cross_50'] = 0

    if all(col in df_group.columns for col in ['high', 'open', 'close', 'low']):
        wick_up = df_group['high'] - df_group[['open', 'close']].max(axis=1)
        wick_down = df_group[['open', 'close']].min(axis=1) - df_group['low']
        body = abs(df_group['close'] - df_group['open'])
        df_group['pin_bar'] = ((wick_up > (body * 2 + 1e-9)) | (wick_down > (body * 2 + 1e-9))).astype(int)
    else:
        df_group['pin_bar'] = 0

    # –ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    if 'ema_20' in df_group.columns and 'ema_50' in df_group.columns:
        df_group['ema_diff'] = df_group['ema_20'] - df_group['ema_50']
    else:
        df_group['ema_diff'] = np.nan

    if 'rsi' in df_group.columns:
        df_group['rsi_change_3'] = df_group['rsi'].diff(3)
    else:
        df_group['rsi_change_3'] = np.nan

    if 'volume' in df_group.columns:
        df_group['volume_mean'] = df_group['volume'].rolling(window=20).mean()
        df_group['volume_std'] = df_group['volume'].rolling(window=20).std()
        df_group['volume_spike'] = ((df_group['volume'] - df_group['volume_mean']) > 2 * df_group['volume_std']).astype(
            int)
    else:
        df_group['volume_mean'] = np.nan
        df_group['volume_std'] = np.nan
        df_group['volume_spike'] = 0

    if 'hour' in df_group.columns and pd.notna(df_group['hour']).any():
        df_group['hour_sin'] = np.sin(2 * np.pi * df_group['hour'].astype(float) / 24)
    else:
        df_group['hour_sin'] = np.nan

    if 'dayofweek' in df_group.columns and pd.notna(df_group['dayofweek']).any():
        df_group['dayofweek_sin'] = np.sin(2 * np.pi * df_group['dayofweek'].astype(float) / 7)
    else:
        df_group['dayofweek_sin'] = np.nan

    # üìä [2] –î–æ–±–∞–≤–ª–µ–Ω–∏–µ bb_width (—à–∏—Ä–∏–Ω–∞ –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞)
    if 'close' in df_group.columns and len(df_group['close']) >= 20:  # BollingerBands —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        try:
            bb = BollingerBands(close=df_group['close'], window=20, window_dev=2)
            df_group['bb_width'] = bb.bollinger_hband() - bb.bollinger_lband()
        except Exception as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å Bollinger Bands: {e}")
            df_group['bb_width'] = np.nan
    else:
        df_group['bb_width'] = np.nan
    
        # === [R1] Rolling —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–Ω–µ –∏ –æ–±—ä—ë–º—É ===
    rolling_windows = [5, 10, 20]

    for window in rolling_windows:
        df_group[f'close_rolling_mean_{window}'] = df_group['close'].rolling(window).mean()
        df_group[f'close_rolling_std_{window}'] = df_group['close'].rolling(window).std()
        df_group[f'volume_rolling_mean_{window}'] = df_group['volume'].rolling(window).mean()
        df_group[f'volume_rolling_std_{window}'] = df_group['volume'].rolling(window).std()
        df_group[f'returns_rolling_std_{window}'] = df_group['log_return_1'].rolling(window).std()

    # === [R2] –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞ –≤–≤–µ—Ä—Ö/–≤–Ω–∏–∑ ===
    df_group['trend_up'] = (df_group['close'] > df_group['close'].shift(1)).astype(int)
    df_group['trend_down'] = (df_group['close'] < df_group['close'].shift(1)).astype(int)

    df_group['consecutive_up'] = df_group['trend_up'] * (df_group['trend_up'].groupby((df_group['trend_up'] != df_group['trend_up'].shift()).cumsum()).cumcount() + 1)
    df_group['consecutive_down'] = df_group['trend_down'] * (df_group['trend_down'].groupby((df_group['trend_down'] != df_group['trend_down'].shift()).cumsum()).cumcount() + 1)

    df_group.drop(['trend_up', 'trend_down'], axis=1, inplace=True)


    return df_group


def compute_and_prepare_features(df_input, tf_name, btc_features_df):
    logging.info(f"–ù–∞—á–∞–ª–æ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {df_input['symbol'].nunique()} —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¢–§ {tf_name}...")
    all_symbols_features = []

    if df_input.empty:
        logging.warning(f"–í—Ö–æ–¥–Ω–æ–π DataFrame –¥–ª—è compute_and_prepare_features –ø—É—Å—Ç –¥–ª—è –¢–§ {tf_name}.")
        return pd.DataFrame()

    unique_symbols = df_input['symbol'].unique()
    if len(unique_symbols) == 0:
        logging.warning(f"–ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ compute_and_prepare_features –¥–ª—è –¢–§ {tf_name}.")
        return pd.DataFrame()

    for symbol_val in tqdm(unique_symbols, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {tf_name}", unit="symbol"):
        df_sym = df_input[df_input['symbol'] == symbol_val].copy()
        df_sym = df_sym.set_index('timestamp')

        if len(df_sym) < 100:
            logging.info(
                f"–°–∏–º–≤–æ–ª {symbol_val} –Ω–∞ –¢–§ {tf_name} –∏–º–µ–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(df_sym)} < 100), –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è.")
            continue

        df_sym['log_return_1'] = np.log(df_sym['close'] / df_sym['close'].shift(1))
        df_sym['future_close'] = df_sym['close'].shift(-TARGET_SHIFT)
        df_sym['delta'] = (df_sym['future_close'] / df_sym['close']) - 1

        future_max_high = df_sym['high'].rolling(window=TARGET_SHIFT).max().shift(-TARGET_SHIFT + 1)
        future_min_low = df_sym['low'].rolling(window=TARGET_SHIFT).min().shift(-TARGET_SHIFT + 1)
        df_sym['volatility'] = (future_max_high - future_min_low) / df_sym['close']

        df_sym['target_up'] = (df_sym['delta'] > 0).astype(int)
        df_sym['target_class'] = df_sym['delta'].apply(classify_delta_value)
        for thresh in [0.004, 0.005, 0.006]:
            df_sym['target_tp_hit'] = np.where(df_sym['delta'] > thresh, 1, 0)
            # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º _tpX, –≥–¥–µ X ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ
            # –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å F1/PR AUC
        df_sym = compute_ta_features(df_sym.copy())

        if btc_features_df is not None and not btc_features_df.empty:
            if isinstance(df_sym.index, pd.DatetimeIndex) and isinstance(btc_features_df.index, pd.DatetimeIndex):
                if df_sym.index.tz != btc_features_df.index.tz:
                    logging.debug(f"Normalizing timezone for merge with BTC features for {symbol_val}")
                    if df_sym.index.tz is not None:
                        df_sym.index = df_sym.index.tz_localize(None)
                    if btc_features_df.index.tz is not None:
                        btc_features_df = btc_features_df.copy()
                        btc_features_df.index = btc_features_df.index.tz_localize(None)

            df_sym = df_sym.merge(btc_features_df, left_index=True, right_index=True, how='left', suffixes=('', '_btc'))
            btc_cols_to_ffill = [col for col in df_sym.columns if '_btc' in col]
            if btc_cols_to_ffill:
                df_sym[btc_cols_to_ffill] = df_sym[btc_cols_to_ffill].ffill()

        if 'atr' in df_sym.columns and 'volume' in df_sym.columns:
            min_periods_rolling = min(20, len(df_sym))
            if min_periods_rolling > 0:
                condition_volume = df_sym['volume'] > df_sym['volume'].rolling(window=20,
                                                                               min_periods=min_periods_rolling).mean()
                condition_atr = df_sym['atr'] > df_sym['atr'].rolling(window=20, min_periods=min_periods_rolling).mean()
                condition_volume = condition_volume.fillna(False)
                condition_atr = condition_atr.fillna(False)
                df_sym_filtered = df_sym[condition_volume & condition_atr]

                if len(df_sym_filtered) < len(df_sym):
                    logging.debug(
                        f"–î–ª—è {symbol_val} –Ω–∞ {tf_name} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(df_sym) - len(df_sym_filtered)} —Å—Ç—Ä–æ–∫ –∏–∑-–∑–∞ —Ñ–ª—ç—Ç–∞/—Å–ª–∞–±—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤.")
                df_sym = df_sym_filtered
            else:
                logging.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol_val} –Ω–∞ {tf_name} –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ñ–ª—ç—Ç–∞.")
        else:
            logging.warning(
                f"–ü—Ä–æ–ø—É—Å–∫ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ñ–ª—ç—Ç–∞ –¥–ª—è {symbol_val} –Ω–∞ {tf_name}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'atr' –∏–ª–∏ 'volume'.")

        if not df_sym.empty:
            df_sym['symbol'] = symbol_val
            all_symbols_features.append(df_sym.reset_index())
        else:
            logging.info(
                f"–°–∏–º–≤–æ–ª {symbol_val} –Ω–∞ –¢–§ {tf_name} —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –Ω–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω.")

    if not all_symbols_features:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –Ω–∞ –¢–§ {tf_name}.")
        return pd.DataFrame()

    full_features_df = pd.concat(all_symbols_features).reset_index(drop=True)

    cols_for_dropna = ['delta', 'volatility', 'target_class', 'rsi', 'target_tp_hit',
                       'ema_20_slope', 'ema_50_slope']
    len_before_dropna = len(full_features_df)
    full_features_df = full_features_df.dropna(subset=cols_for_dropna)
    len_after_dropna = len(full_features_df)
    logging.info(
        f"–£–¥–∞–ª–µ–Ω–æ {len_before_dropna - len_after_dropna} —Å—Ç—Ä–æ–∫ –∏–∑-–∑–∞ NaN –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö.")
    logging.info(f"–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf_name} –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Ç–æ–≥–æ —Å—Ç—Ä–æ–∫: {len(full_features_df)}.")
    return full_features_df


def main_preprocess(args):
    tf_arg = args.tf
    
    # üîß 1.4 –î–ª—è model_name_suffix —Å–¥–µ–ª–∞–π:
    # –≠—Ç–æ—Ç —Å—É—Ñ—Ñ–∏–∫—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–º, –∫–∞–∫–æ–π –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ 
    # --symbol-group –∏–ª–∏ --symbol –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω, –∏–ª–∏ "all" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
    model_name_suffix_for_files = args.symbol_group or args.symbol or "all"

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–µ–π (–≤—Å–µ—Ö –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –¢–§)
    df_all_candles = load_candles_from_db(tf_arg)
    if df_all_candles.empty:
        logging.error(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î –¥–ª—è {tf_arg}. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
        return

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –¥–ª—è –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    log_processing_details = ""
    
    if args.symbol_list: # args.symbol_list –º–æ–≥ –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —á–µ—Ä–µ–∑ args.symbol_group
        if args.symbol_group:
            log_processing_details = f" –≥—Ä—É–ø–ø—ã '{args.symbol_group}' (—Å–∏–º–≤–æ–ª—ã: {', '.join(args.symbol_list)})"
        else:
            log_processing_details = f" —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ ({', '.join(args.symbol_list)})"
        
        logging.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details}. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É —Å–ø–∏—Å–∫—É.")
        
        if 'symbol' not in df_all_candles.columns:
            logging.error("–ö–æ–ª–æ–Ω–∫–∞ 'symbol' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
            return

        df_all_candles = df_all_candles[df_all_candles['symbol'].isin(args.symbol_list)]
        if df_all_candles.empty:
            logging.error(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º {args.symbol_list} –¥–ª—è –¢–§ {tf_arg} –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
            return
        logging.info(f"–î–∞–Ω–Ω—ã–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –ø–æ —Å–ø–∏—Å–∫—É. –û—Å—Ç–∞–ª–æ—Å—å {len(df_all_candles)} —Å—Ç—Ä–æ–∫.")
    
    elif args.symbol: # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ args.symbol_list –Ω–µ –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω
        log_processing_details = f" —Å–∏–º–≤–æ–ª–∞ '{args.symbol}'"
        logging.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details}. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É —Å–∏–º–≤–æ–ª—É.")
        
        original_symbols_count = df_all_candles['symbol'].nunique()
        original_rows_count = len(df_all_candles)

        if 'symbol' not in df_all_candles.columns:
            logging.error(f"–ö–æ–ª–æ–Ω–∫–∞ 'symbol' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ {args.symbol} –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
            return

        df_all_candles = df_all_candles[df_all_candles['symbol'] == args.symbol]

        if df_all_candles.empty:
            logging.warning(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å–∏–º–≤–æ–ª—É '{args.symbol}' –¥–ª—è –¢–§ {tf_arg} –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å. "
                            f"(–ò—Å—Ö–æ–¥–Ω–æ –±—ã–ª–æ {original_rows_count} —Å—Ç—Ä–æ–∫ –¥–ª—è {original_symbols_count} —Å–∏–º–≤–æ–ª–æ–≤). "
                            f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ –±—É–¥–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∞.")
            return
        else:
            logging.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ —Å–∏–º–≤–æ–ª—É {args.symbol}. –û—Å—Ç–∞–ª–æ—Å—å {len(df_all_candles)} —Å—Ç—Ä–æ–∫ "
                         f"(–∏–∑ {original_rows_count} —Å—Ç—Ä–æ–∫, {original_symbols_count} —Å–∏–º–≤–æ–ª–æ–≤).")
    
    else: # –ù–∏ --symbol-list, –Ω–∏ --symbol-group, –Ω–∏ --symbol –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
        log_processing_details = " (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –ë–î –¥–ª—è —ç—Ç–æ–≥–æ –¢–§)"
        logging.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details}. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ {df_all_candles['symbol'].nunique()} –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–∞ ({len(df_all_candles)} —Å—Ç—Ä–æ–∫).")

    logging.info(f"‚öôÔ∏è  –ù–∞—á–∞–ª–æ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf_arg} –¥–ª—è{log_processing_details}")
    logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤: {model_name_suffix_for_files}")


    btc_features_prepared = None
    if 'BTCUSDT' in df_all_candles['symbol'].unique():
        logging.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ BTCUSDT...")
        df_btc_raw = df_all_candles[df_all_candles['symbol'] == 'BTCUSDT'].copy()

        if not df_btc_raw.empty:
            df_btc_raw = df_btc_raw.set_index('timestamp')
            if len(df_btc_raw) >= 50:
                df_btc_raw['log_return_1_btc'] = np.log(df_btc_raw['close'] / df_btc_raw['close'].shift(1))
                df_btc_raw['rsi_btc'] = ta.momentum.RSIIndicator(close=df_btc_raw['close'], window=14).rsi()
                df_btc_raw['volume_btc'] = df_btc_raw['volume']
                btc_features_prepared = df_btc_raw[['log_return_1_btc', 'rsi_btc', 'volume_btc']].shift(1)
                btc_features_prepared.dropna(inplace=True)
                logging.info(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ BTCUSDT –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã. –°—Ç—Ä–æ–∫: {len(btc_features_prepared)}")
            else:
                logging.warning(
                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BTCUSDT ({len(df_btc_raw)} —Å—Ç—Ä–æ–∫) –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
        else:
            logging.info("–î–∞–Ω–Ω—ã–µ –ø–æ BTCUSDT –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ç–µ–∫—É—â–µ–º –Ω–∞–±–æ—Ä–µ —Å–≤–µ—á–µ–π (–≤–æ–∑–º–æ–∂–Ω–æ, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã). –ü—Ä–∏–∑–Ω–∞–∫–∏ BTC –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç.")
    else:
        logging.warning("–î–∞–Ω–Ω—ã–µ –ø–æ BTCUSDT –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö) —Å–≤–µ—á–∞—Ö. –ü—Ä–∏–∑–Ω–∞–∫–∏ BTC –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç.")

    df_to_process = df_all_candles

    if df_to_process.empty:
        logging.error(
            f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –¢–§ {tf_arg} –¥–ª—è{log_processing_details} (df_to_process –ø—É—Å—Ç).")
        return

    df_final_features = compute_and_prepare_features(df_to_process, tf_arg, btc_features_prepared)

    if df_final_features.empty:
        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf_arg} –¥–ª—è{log_processing_details}, —Ç–∞–∫ –∫–∞–∫ –∏—Ç–æ–≥–æ–≤—ã–π DataFrame –ø—É—Å—Ç.")
        return

    os.makedirs('data', exist_ok=True)
    
    output_pickle_path = f"data/features_{model_name_suffix_for_files}_{tf_arg}.pkl"
    output_sample_csv_path = f"data/sample_{model_name_suffix_for_files}_{tf_arg}.csv"

    try:
        df_final_features.to_pickle(output_pickle_path)
        logging.info(f"üíæ  –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_pickle_path}, —Ñ–æ—Ä–º–∞: {df_final_features.shape}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Pickle —Ñ–∞–π–ª–∞ {output_pickle_path}: {e}")

    try:
        sample_size = min(1000, len(df_final_features))
        if sample_size > 0:
            if 'timestamp' in df_final_features.columns and pd.api.types.is_datetime64_any_dtype(
                    df_final_features['timestamp']):
                df_sample = df_final_features.head(sample_size).copy()
                df_sample['timestamp'] = df_sample['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')
                df_sample.to_csv(output_sample_csv_path, index=False)
            else:
                df_final_features.head(sample_size).to_csv(output_sample_csv_path, index=False)
            logging.info(f"üìÑ  –°—ç–º–ø–ª –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_sample_csv_path} ({sample_size} —Å—Ç—Ä–æ–∫)")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV —Å—ç–º–ø–ª–∞ {output_sample_csv_path}: {e}")

    logging.info(f"‚úÖ  –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf_arg} –¥–ª—è{log_processing_details}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π –∏–∑ SQLite.")
    parser.add_argument('--tf', type=str, required=True, help='–¢–∞–π–º—Ñ—Ä–µ–π–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 5m, 15m)')
    parser.add_argument('--symbol', type=str, default=None,
                        help="–°–∏–º–≤–æ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: BTCUSDT)")
    parser.add_argument('--symbol-list', nargs='+', help="–°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: BTCUSDT ETHUSDT ...)")
    # üîß 1.1 –î–æ–±–∞–≤—å –∞—Ä–≥—É–º–µ–Ω—Ç --symbol-group:
    parser.add_argument('--symbol-group', type=str, help="–ü—Å–µ–≤–¥–æ–Ω–∏–º –≥—Ä—É–ø–ø—ã –º–æ–Ω–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: top8, meme)")

    args = parser.parse_args()

    # üîß 1.3 –ü–æ—Å–ª–µ args = parser.parse_args() –¥–æ–±–∞–≤—å:
    if args.symbol_group:
        if args.symbol_group in SYMBOL_GROUPS:
            if args.symbol_list:
                 logging.warning(f"–ê—Ä–≥—É–º–µ–Ω—Ç --symbol-list ('{args.symbol_list}') –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω –≥—Ä—É–ø–ø–æ–π --symbol-group ('{args.symbol_group}').")
            args.symbol_list = SYMBOL_GROUPS[args.symbol_group]
            args.symbol = args.symbol_group
            logging.info(f"üß© –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤ '{args.symbol_group}': {args.symbol_list}")
        else:
            logging.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤: {args.symbol_group}. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≥—Ä—É–ø–ø—ã: {list(SYMBOL_GROUPS.keys())}")
            sys.exit(1)

    try:
        main_preprocess(args)
    except KeyboardInterrupt:
        log_details_exit = ""
        if args.symbol_group: log_details_exit = f" –≥—Ä—É–ø–ø—ã '{args.symbol_group}'"
        elif args.symbol_list: log_details_exit = f" —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ ({', '.join(args.symbol_list)})"
        elif args.symbol: log_details_exit = f" —Å–∏–º–≤–æ–ª–∞ '{args.symbol}'"
        else: log_details_exit = " (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã)"
        print(f"\n[Preprocess] üõë –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {args.tf} –¥–ª—è{log_details_exit} –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C).")
        sys.exit(130)
    except Exception as e:
        log_details_exit = ""
        if args.symbol_group: log_details_exit = f" –≥—Ä—É–ø–ø—ã '{args.symbol_group}'"
        elif args.symbol_list: log_details_exit = f" —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ ({', '.join(args.symbol_list)})"
        elif args.symbol: log_details_exit = f" —Å–∏–º–≤–æ–ª–∞ '{args.symbol}'"
        else: log_details_exit = " (–≤—Å–µ —Å–∏–º–≤–æ–ª—ã)"
        logging.error(f"[Preprocess] üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {args.tf} –¥–ª—è{log_details_exit}: {e}",
                      exc_info=True)
        sys.exit(1)