import pandas as pd
import joblib
import os
from datetime import datetime
from tabulate import tabulate
import argparse
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np
import sys  # –î–ª—è Ctrl+C
import logging  # –î–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è


# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ (–º–æ–∂–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—å –≤ —Ñ–∞–π–ª –∏–ª–∏ —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Å–æ–ª—å)
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s [PredictAll] - %(message)s',
                    stream=sys.stdout)


# ‚úÖ 1. –î–æ–±–∞–≤—å –≤ –≤–µ—Ä—Ö–Ω—é—é —á–∞—Å—Ç—å predict_all.py (–ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤):
# import joblib # Already imported
# import os # Already imported

import joblib
import os

# –°–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø: —Å–∏–º–≤–æ–ª—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ –∏–º–µ–Ω–µ–º (–∫–ª—é—á–æ–º)
GROUP_MODELS = {
    "top8": [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT",
        "XRPUSDT", "ADAUSDT", "LINKUSDT", "AVAXUSDT"
    ],
    "meme": [
        "PEPEUSDT", "DOGEUSDT", "FLOKIUSDT", "WIFUSDT", "SHIBUSDT"
    ]
}

def load_model_with_fallback(symbol, tf, model_type):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å:
    1. –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: models/BTCUSDT_15m_clf_class.pkl
    2. –ì—Ä—É–ø–ø–æ–≤—É—é –º–æ–¥–µ–ª—å: models/top8_15m_clf_class.pkl (–µ—Å–ª–∏ symbol –≤—Ö–æ–¥–∏—Ç –≤ –≥—Ä—É–ø–ø—É)
    3. –û–±—â—É—é –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º—É: models/15m_clf_class.pkl
    """
    # 1. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    symbol_model_path = f"models/{symbol}_{tf}_{model_type}.pkl"
    if os.path.exists(symbol_model_path):
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {symbol}: {model_type}")
        return joblib.load(symbol_model_path)

    # 2. –ì—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å
    for group_name, symbol_list in GROUP_MODELS.items():
        if symbol in symbol_list:
            group_model_path = f"models/{group_name}_{tf}_{model_type}.pkl"
            if os.path.exists(group_model_path):
                print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å ({group_name}) –¥–ª—è {symbol}: {model_type}")
                return joblib.load(group_model_path)

    # 3. –û–±—â–∞—è –º–æ–¥–µ–ª—å
    default_model_path = f"models/{tf}_{model_type}.pkl"
    if os.path.exists(default_model_path):
        print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∞—è –º–æ–¥–µ–ª—å –ø–æ TF: {model_type} ‚Üí {default_model_path}")
        return joblib.load(default_model_path)

    print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol}, –Ω–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã, –Ω–∏ –æ–±—â–∞—è: {model_type}")
    return None

    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø–∞—Ä—É. –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â—É—é.
    model_type: clf_class, reg_delta, reg_vol, clf_tp_hit
    """
    symbol_model_path = f"models/{symbol}_{tf}_{model_type}.pkl"
    default_model_path = f"models/{tf}_{model_type}.pkl"

    if os.path.exists(symbol_model_path):
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –¥–ª—è {symbol}: {model_type}")
        try:
            return joblib.load(symbol_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {symbol_model_path}: {e}")
            # Fallback to default if symbol-specific fails to load
            if os.path.exists(default_model_path):
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {symbol_model_path}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∞—è: {model_type}")
                try:
                    return joblib.load(default_model_path)
                except Exception as e_default:
                    logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—â–µ–π –º–æ–¥–µ–ª–∏ {default_model_path}: {e_default}")
                    return None
            else:
                print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {symbol_model_path} (–æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏) –∏ –Ω–µ—Ç –æ–±—â–µ–π {default_model_path}")
                return None

    elif os.path.exists(default_model_path):
        print(f"‚ö†Ô∏è –ù–µ—Ç –º–æ–¥–µ–ª–∏ {symbol}_{tf}_{model_type}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∞—è: {model_type}")
        try:
            return joblib.load(default_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—â–µ–π –º–æ–¥–µ–ª–∏ {default_model_path}: {e}")
            return None
    else:
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: –Ω–∏ {symbol_model_path}, –Ω–∏ {default_model_path}")
        return None


TIMEFRAMES = ['5m', '15m', '30m', '1h', '4h', '1d']  # –û—Å–Ω–æ–≤–Ω—ã–µ –¢–§ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
FEATURES_PATH_TEMPLATE = 'data/features_{tf}.pkl'
MODEL_PATH_TEMPLATE = 'models/{tf}_{model_type}.pkl'  # Still used by old load_model, might be relevant if other models use it
LOG_DIR_PREDICT = 'logs'
LATEST_PREDICTIONS_FILE = os.path.join(LOG_DIR_PREDICT, 'latest_predictions.csv')
TRADE_PLAN_FILE = os.path.join(LOG_DIR_PREDICT, 'trade_plan.csv')

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤/—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(LOG_DIR_PREDICT, exist_ok=True)

TARGET_CLASS_NAMES = ['STRONG DOWN', 'DOWN', 'NEUTRAL', 'UP', 'STRONG UP']


def compute_final_delta(delta_model, delta_history, sigma_history):
    if pd.isna(delta_history) or pd.isna(sigma_history):
        return round(delta_model, 5)
    if sigma_history < 0.005:
        w1, w2 = 0.4, 0.6
    elif sigma_history > 0.02:
        w1, w2 = 0.8, 0.2
    else:
        alpha = (sigma_history - 0.005) / (0.02 - 0.005)
        w1 = 0.4 + alpha * (0.8 - 0.4)
        w2 = 1.0 - w1
    return round(w1 * delta_model + w2 * delta_history, 5)


def get_signal_strength(delta_final, confidence, sigma_history):
    if pd.isna(sigma_history):
        sigma_history = float('inf')

    if abs(delta_final) > 0.02 and sigma_history < 0.01 and confidence > 0.5:
        return "üü¢ –°–∏–ª—å–Ω—ã–π"
    elif abs(delta_final) > 0.01 and sigma_history < 0.02 and confidence > 0.2:
        return "üü° –£–º–µ—Ä–µ–Ω–Ω—ã–π"
    else:
        return "‚ö™ –°–ª–∞–±—ã–π"


def is_conflict(delta_model, delta_history):
    if pd.isna(delta_history) or pd.isna(delta_model):
        return False
    return (delta_model > 0 and delta_history < 0) or \
        (delta_model < 0 and delta_history > 0)


def load_model(tf, model_type):  # This function might still be used for other models, or can be deprecated if not
    path = MODEL_PATH_TEMPLATE.format(tf=tf, model_type=model_type)
    if os.path.exists(path):
        try:
            return joblib.load(path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {path}: {e}")
            return None
    logging.warning(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {path}")
    return None


def get_confidence_hint(score):
    if score > 0.2:
        return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.1:
        return "–•–æ—Ä–æ—à–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.05:
        return "–°–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª"
    else:
        return "–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Äî –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å"


def calculate_trade_levels(entry, direction, atr_value, rr=2.0):
    if pd.isna(atr_value) or atr_value <= 1e-9:
        return np.nan, np.nan
    if direction == 'long':
        sl = entry - atr_value
        tp = entry + atr_value * rr
    elif direction == 'short':
        sl = entry + atr_value
        tp = entry - atr_value * rr
    else:
        return np.nan, np.nan
    return round(sl, 6), round(tp, 6)


def similarity_analysis(X_live_df, X_hist_df, deltas_hist_series, top_n=15):
    if X_hist_df.empty or len(X_hist_df) < top_n:
        return np.nan, np.nan, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"

    common_cols = X_live_df.columns.intersection(X_hist_df.columns)
    if len(common_cols) != len(X_live_df.columns):
        missing_in_hist = list(set(X_live_df.columns) - set(common_cols))

    X_live_df_common = X_live_df[common_cols]
    X_hist_df_common = X_hist_df[common_cols]

    if X_live_df_common.empty or X_hist_df_common.empty:
        return np.nan, np.nan, "–ù–µ—Ç –æ–±—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"

    scaler = StandardScaler()
    try:
        X_hist_scaled = scaler.fit_transform(X_hist_df_common)
        x_live_scaled = scaler.transform(X_live_df_common)
    except ValueError as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ similarity_analysis: {e}")
        return np.nan, np.nan, "–û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"

    sims = cosine_similarity(X_hist_scaled, x_live_scaled).flatten()

    if len(sims) != len(deltas_hist_series.loc[X_hist_df_common.index]):
        aligned_deltas_hist_series = deltas_hist_series.loc[X_hist_df_common.index]
        if len(sims) != len(aligned_deltas_hist_series):
            logging.error(
                f"–û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω sims ({len(sims)}) –∏ deltas_hist_series ({len(aligned_deltas_hist_series)})")
            return np.nan, np.nan, "–û—à–∏–±–∫–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –¥–µ–ª—å—Ç–∞–º–∏"
    else:
        aligned_deltas_hist_series = deltas_hist_series.loc[X_hist_df_common.index]

    top_indices_in_sims = sims.argsort()[-top_n:][::-1]
    original_top_indices = X_hist_df_common.index[top_indices_in_sims]

    if len(original_top_indices) == 0:
        return np.nan, np.nan, "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"

    similar_deltas = aligned_deltas_hist_series.loc[original_top_indices]
    avg_delta = similar_deltas.mean()
    std_delta = similar_deltas.std()

    hint = (
        "–í—ã—Å–æ–∫–∏–π —Ä–∞–∑–±—Ä–æ—Å" if std_delta > 0.02 else
        "–£–º–µ—Ä–µ–Ω–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ" if std_delta > 0.01 else
        "–°—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω"
    )
    if pd.isna(avg_delta) or pd.isna(std_delta):
        hint = "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å—Ö–æ–∂–µ—Å—Ç–∏"

    return avg_delta, std_delta, hint


def predict_all_tf(save_output_flag):
    logging.info("üöÄ  –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤...")
    all_predictions_data = []
    trade_plan = []

    for tf in TIMEFRAMES:
        logging.info(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf} ---")
        features_path = FEATURES_PATH_TEMPLATE.format(tf=tf)
        if not os.path.exists(features_path):
            logging.warning(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {features_path}. –ü—Ä–æ–ø—É—Å–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf}.")
            continue

        try:
            df = pd.read_pickle(features_path)
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path}: {e}. –ü—Ä–æ–ø—É—Å–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf}.")
            continue

        if df.empty:
            logging.warning(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Å—Ç: {features_path}. –ü—Ä–æ–ø—É—Å–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf}.")
            continue

        features_list_path = f"models/{tf}_features_selected.txt"
        if not os.path.exists(features_list_path):
            logging.error(
                f"–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ '{features_list_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –¥–ª—è {tf}. –ü—Ä–æ–ø—É—Å–∫.")
            continue
        try:
            with open(features_list_path, "r", encoding="utf-8") as f:
                feature_cols_from_file = [line.strip() for line in f if line.strip()]
            if not feature_cols_from_file:
                logging.error(f"–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ '{features_list_path}' –ø—É—Å—Ç. –ü—Ä–æ–ø—É—Å–∫ {tf}.")
                continue
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ '{features_list_path}': {e}. –ü—Ä–æ–ø—É—Å–∫ {tf}.")
            continue

        # Model loading is now per-symbol, so it's moved inside the symbol loop.
        # The old loading location and logging for TP-hit classes per TF is removed from here.

        for symbol in df['symbol'].unique():
            df_sym = df[df['symbol'] == symbol].sort_values('timestamp').copy()
            if df_sym.empty:
                continue

            logging.info(f"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ {symbol} –Ω–∞ {tf} ---")

            # ‚úÖ 2. –ó–∞–º–µ–Ω–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π
            # –ë—ã–ª–æ:
            # clf_class = joblib.load(f"models/{tf}_clf_class.pkl")
            # reg_delta = joblib.load(f"models/{tf}_reg_delta.pkl")
            # reg_vol = joblib.load(f"models/{tf}_reg_vol.pkl")
            # clf_tp_hit = joblib.load(f"models/{tf}_clf_tp_hit.pkl")
            # –°—Ç–∞–ª–æ (–∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö model_class, model_delta –∏ —Ç.–¥.):
            model_class = load_model_with_fallback(symbol, tf, "clf_class")
            model_delta = load_model_with_fallback(symbol, tf, "reg_delta")
            model_vol = load_model_with_fallback(symbol, tf, "reg_vol")
            model_tp_hit = load_model_with_fallback(symbol, tf, "clf_tp_hit")

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ TP-hit –º–æ–¥–µ–ª–∏ (–ø–µ—Ä–µ–º–µ—â–µ–Ω–æ —Å—é–¥–∞, —Ç.–∫. –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¥–ª—è symbol, tf)
            if model_tp_hit:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º logging.info –≤–º–µ—Å—Ç–æ print –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –Ω–æ –µ—Å–ª–∏ –≤–∞–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å print, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å.
                # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞, —Å–æ—Ö—Ä–∞–Ω–∏–º print –∏–∑ load_model_with_fallback, –∞ –∑–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º logging.
                logging.info(f"–ö–ª–∞—Å—Å—ã TP-hit –º–æ–¥–µ–ª–∏ ({symbol}, {tf}): {getattr(model_tp_hit, 'classes_', 'N/A')}")

            if not all([model_class, model_delta, model_vol]):  # model_tp_hit is optional
                logging.warning(
                    f"–û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol} –Ω–∞ {tf} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–∞ —ç—Ç–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ.")
                continue  # Skip this symbol for this tf

            row_df = df_sym.iloc[-1:].copy()
            hist_df_full = df_sym.iloc[:-1].copy()

            if 'close' not in row_df.columns or 'timestamp' not in row_df.columns:
                logging.warning(f"–í –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} {tf} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç 'close' –∏–ª–∏ 'timestamp'. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ feature_cols_from_file –Ω–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ row_df (—É–∂–µ –µ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∏–∂–µ –Ω–∞ X_live)
            missing_cols_in_df_for_symbol = [col for col in feature_cols_from_file if col not in row_df.columns]
            if missing_cols_in_df_for_symbol:
                logging.error(
                    f"–í DataFrame –¥–ª—è {symbol} –Ω–∞ {tf} –∏–∑ {features_path} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã {missing_cols_in_df_for_symbol}, "
                    f"–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ (—Å–æ–≥–ª–∞—Å–Ω–æ {features_list_path}). –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞ {symbol}."
                )
                continue

            X_live = row_df[feature_cols_from_file]
            if X_live.isnull().values.any():
                nan_features = X_live.columns[X_live.isnull().any()].tolist()
                logging.warning(f"–í X_live –¥–ª—è {symbol} {tf} –µ—Å—Ç—å NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {nan_features}. –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞.")
                continue

            avg_delta_similar, std_delta_similar, similarity_hint = np.nan, np.nan, "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"
            if 'delta' not in hist_df_full.columns:
                logging.debug(f"–°—Ç–æ–ª–±–µ—Ü 'delta' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö hist_df_full –¥–ª—è {symbol} {tf}.")
            else:
                missing_hist_features = [col for col in feature_cols_from_file if col not in hist_df_full.columns]
                if missing_hist_features:
                    logging.debug(f"–í hist_df_full –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã {missing_hist_features} –¥–ª—è {symbol} {tf}.")
                else:
                    hist_for_sim_features = hist_df_full[feature_cols_from_file].copy()
                    hist_for_sim_deltas = hist_df_full['delta'].copy()

                    valid_indices_features = hist_for_sim_features.dropna().index
                    valid_indices_deltas = hist_for_sim_deltas.dropna().index
                    common_valid_indices = valid_indices_features.intersection(valid_indices_deltas)

                    hist_for_sim_features_clean = hist_for_sim_features.loc[common_valid_indices]
                    hist_for_sim_deltas_clean = hist_for_sim_deltas.loc[common_valid_indices]

                    if len(hist_for_sim_features_clean) >= 15:  # Ensure enough data for similarity analysis
                        # Check if X_live has all columns required by hist_for_sim_features_clean for similarity
                        if not X_live.columns.equals(hist_for_sim_features_clean.columns):
                            common_cols_sim = X_live.columns.intersection(hist_for_sim_features_clean.columns)
                            if len(common_cols_sim) < len(X_live.columns) or len(common_cols_sim) < len(
                                    hist_for_sim_features_clean.columns):
                                logging.warning(
                                    f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è similarity_analysis ({symbol}, {tf}). X_live: {len(X_live.columns)}, Hist: {len(hist_for_sim_features_clean.columns)}, Common: {len(common_cols_sim)}")
                                # Potentially skip similarity or use common_cols_sim cautiously
                        avg_delta_similar, std_delta_similar, similarity_hint = similarity_analysis(
                            X_live, hist_for_sim_features_clean, hist_for_sim_deltas_clean)
                    else:
                        similarity_hint = f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(hist_for_sim_features_clean)}) –¥–ª—è —Å—Ö–æ–∂–µ—Å—Ç–∏"

            try:
                proba_raw = model_class.predict_proba(X_live)[0]
            except Exception as e:
                logging.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ model_class.predict_proba –¥–ª—è {symbol} {tf} —Å X_live shape {X_live.shape}: {e}")
                continue

            pred_class_idx = proba_raw.argmax()
            if pred_class_idx >= len(TARGET_CLASS_NAMES):
                logging.error(
                    f"–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ ({pred_class_idx}) –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã TARGET_CLASS_NAMES –¥–ª—è {symbol} {tf}. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            signal = TARGET_CLASS_NAMES[pred_class_idx]

            if len(proba_raw) < 2:
                confidence = float(proba_raw.max()) if len(proba_raw) == 1 else 0.0
            else:
                sorted_probas = np.sort(proba_raw)
                confidence = float(sorted_probas[-1] - sorted_probas[-2])

            hint = get_confidence_hint(confidence)
            predicted_delta = model_delta.predict(X_live)[0]
            predicted_volatility = model_vol.predict(X_live)[0]

            tp_hit_proba = np.nan
            if model_tp_hit:
                try:
                    tp_hit_proba_all_classes = model_tp_hit.predict_proba(X_live)[0]
                    model_tp_classes = getattr(model_tp_hit, 'classes_', None)
                    if model_tp_classes is not None and len(model_tp_classes) == 2:
                        try:
                            class_1_idx = list(model_tp_classes).index(1)
                            tp_hit_proba = tp_hit_proba_all_classes[class_1_idx]
                        except ValueError:
                            logging.warning(
                                f"–ö–ª–∞—Å—Å '1' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ model_tp_hit.classes_ ({model_tp_classes}) –¥–ª—è {symbol} {tf}. –ò—Å–ø–æ–ª—å–∑—É—é –∏–Ω–¥–µ–∫—Å 1 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ.")
                            if len(tp_hit_proba_all_classes) > 1:
                                tp_hit_proba = tp_hit_proba_all_classes[1]  # Assume 1 is the positive class at index 1
                            else:
                                tp_hit_proba = 0.0  # Or handle as error / nan
                    elif len(tp_hit_proba_all_classes) > 1:
                        tp_hit_proba = tp_hit_proba_all_classes[1]
                        logging.debug(
                            f"–ê—Ç—Ä–∏–±—É—Ç classes_ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —É model_tp_hit –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ 2 –∫–ª–∞—Å—Å–∞ –¥–ª—è {symbol} {tf}. –ò—Å–ø–æ–ª—å–∑—É—é –∏–Ω–¥–µ–∫—Å 1 –¥–ª—è tp_hit_proba.")
                    elif len(tp_hit_proba_all_classes) == 1:
                        logging.warning(
                            f"–ú–æ–¥–µ–ª—å TP-hit –¥–ª—è {symbol} {tf} –≤–µ—Ä–Ω—É–ª–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {tp_hit_proba_all_classes[0]}. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å tp_hit_proba.")
                        # tp_hit_proba remains np.nan
                    else:
                        logging.warning(
                            f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ predict_proba –æ—Ç model_tp_hit –¥–ª—è {symbol} {tf}: {tp_hit_proba_all_classes}")

                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ TP-hit –¥–ª—è {symbol} {tf}: {e}")

            direction = 'long' if signal in ['UP', 'STRONG UP'] else 'short' if signal in ['DOWN',
                                                                                           'STRONG DOWN'] else 'none'
            ts_obj = pd.to_datetime(row_df['timestamp'].values[0])
            ts_str_log = ts_obj.strftime('%Y-%m-%d %H:%M:%S')
            ts_str_display = ts_obj.strftime('%Y-%m-%d %H:%M')

            proba_dict = {TARGET_CLASS_NAMES[i]: proba_raw[i] for i in range(len(proba_raw))}
            entry_price = float(row_df['close'].values[0])

            sl, tp = calculate_trade_levels(entry_price, direction, predicted_volatility)
            delta_final = compute_final_delta(predicted_delta, avg_delta_similar, std_delta_similar)
            signal_strength_val = get_signal_strength(delta_final, confidence, std_delta_similar)
            conflict_flag = is_conflict(predicted_delta, avg_delta_similar)

            prediction_entry = {
                'symbol': symbol, 'tf': tf, 'timestamp_obj': ts_obj,
                'timestamp_str_log': ts_str_log, 'timestamp_str_display': ts_str_display,
                'signal': signal, 'confidence_score': confidence, 'confidence_hint': hint,
                'proba_dict': proba_dict, 'predicted_delta': predicted_delta,
                'predicted_volatility': predicted_volatility, 'entry': entry_price,
                'sl': sl, 'tp': tp, 'direction': direction,
                'avg_delta_similar': avg_delta_similar, 'std_delta_similar': std_delta_similar,
                'similarity_hint': similarity_hint,
                'delta_final': delta_final, 'signal_strength': signal_strength_val, 'conflict': conflict_flag,
                'tp_hit_proba': tp_hit_proba,
                'error': None
            }
            all_predictions_data.append(prediction_entry)

            if direction != 'none' and confidence > 0.05 and (pd.isna(tp_hit_proba) or tp_hit_proba > 0.6):
                rr_value = 0
                if not pd.isna(sl) and not pd.isna(tp) and abs(
                        entry_price - sl) > 1e-9:  # Ensure SL is not zero or too close to entry
                    rr_value = round(abs(tp - entry_price) / abs(entry_price - sl), 2)

                trade_plan.append({
                    'symbol': symbol, 'tf': tf, 'entry': entry_price, 'direction': direction,
                    'sl': sl, 'tp': tp, 'rr': rr_value,
                    'confidence': confidence, 'signal': signal, 'timestamp': ts_str_log, 'hint': hint,
                    'tp_hit_proba': tp_hit_proba
                })

    if save_output_flag and all_predictions_data:
        logging.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        df_out_list = []
        proba_dict_keys_example = TARGET_CLASS_NAMES  # Default keys

        for r_item in all_predictions_data:
            item = {
                'symbol': r_item['symbol'], 'tf': r_item['tf'],
                'timestamp': r_item['timestamp_obj'].strftime('%Y-%m-%d %H:%M:%S'),
                'signal': r_item['signal'], 'confidence_score': r_item['confidence_score'],
                'predicted_delta': r_item['predicted_delta'],
                'predicted_volatility': r_item['predicted_volatility'],
                'avg_delta_similar': r_item['avg_delta_similar'],
                'std_delta_similar': r_item['std_delta_similar'],
                'delta_final': r_item['delta_final'],
                'entry': r_item['entry'], 'sl': r_item['sl'], 'tp': r_item['tp'],
                'direction': r_item['direction'],
                'signal_strength': r_item['signal_strength'], 'conflict': r_item['conflict'],
                'tp_hit_proba': r_item['tp_hit_proba'],
                'confidence_hint': r_item['confidence_hint'],
                'similarity_hint': r_item['similarity_hint'],
            }
            if r_item['proba_dict']:  # Should always be true if proba_raw was successful
                item.update(r_item['proba_dict'])
                # Update proba_dict_keys_example in case the number of classes varies (though TARGET_CLASS_NAMES should be fixed)
                proba_dict_keys_example = list(r_item['proba_dict'].keys())

            df_out_list.append(item)

        df_out = pd.DataFrame(df_out_list)
        csv_columns_order = [
            'symbol', 'tf', 'timestamp', 'signal', 'confidence_score', 'tp_hit_proba',
            'predicted_delta', 'avg_delta_similar', 'std_delta_similar', 'delta_final',
            'predicted_volatility', 'entry', 'sl', 'tp', 'direction',
            'signal_strength', 'conflict', 'confidence_hint', 'similarity_hint'
        ]
        # Add proba columns to the desired order
        csv_columns_order.extend(proba_dict_keys_example)  # Add keys like 'STRONG DOWN', 'DOWN', etc.

        # Ensure all columns in df_out are included, even if not in csv_columns_order
        final_csv_columns = [col for col in csv_columns_order if col in df_out.columns]
        for col in df_out.columns:
            if col not in final_csv_columns:
                final_csv_columns.append(col)

        if not df_out.empty:
            df_out = df_out[final_csv_columns]
            try:
                df_out.to_csv(LATEST_PREDICTIONS_FILE, index=False, float_format='%.6f')
                logging.info(f"üìÑ  –°–∏–≥–Ω–∞–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {LATEST_PREDICTIONS_FILE}")
            except Exception as e:
                logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {LATEST_PREDICTIONS_FILE}: {e}")
        else:
            logging.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ LATEST_PREDICTIONS_FILE.")

        if trade_plan:
            df_trade_plan = pd.DataFrame(trade_plan)
            trade_plan_cols_order = [
                'symbol', 'tf', 'timestamp', 'direction', 'signal', 'confidence', 'tp_hit_proba',
                'entry', 'sl', 'tp', 'rr', 'hint'
            ]
            final_trade_plan_cols = [col for col in trade_plan_cols_order if col in df_trade_plan.columns]
            for col in df_trade_plan.columns:
                if col not in final_trade_plan_cols:
                    final_trade_plan_cols.append(col)

            if not df_trade_plan.empty:
                df_trade_plan = df_trade_plan[final_trade_plan_cols]
                try:
                    df_trade_plan.to_csv(TRADE_PLAN_FILE, index=False, float_format='%.6f')
                    logging.info(f"üìà  –¢–æ—Ä–≥–æ–≤—ã–π –ø–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {TRADE_PLAN_FILE}")
                except Exception as e:
                    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {TRADE_PLAN_FILE}: {e}")
            else:
                logging.info("–¢–æ—Ä–≥–æ–≤—ã–π –ø–ª–∞–Ω –ø—É—Å—Ç, —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω.")
        else:
            logging.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞.")

    if all_predictions_data:
        headers_tabulate = ["TF", "Timestamp", "–°–∏–≥–Ω–∞–ª", "Conf.", "ŒîModel", "ŒîHist", "œÉHist", "ŒîFinal", "–ö–æ–Ω—Ñ?", "–°–∏–ª–∞",
                            "TP Hit%"]
        grouped_by_symbol = {}
        for row_data_item in all_predictions_data:
            grouped_by_symbol.setdefault(row_data_item['symbol'], []).append(row_data_item)

        for symbol_key, rows_list in grouped_by_symbol.items():
            try:
                # Sort by TIMEFRAMES order, then by confidence score descending
                sorted_rows = sorted(rows_list,
                                     key=lambda r_item_sort: (TIMEFRAMES.index(r_item_sort['tf'])
                                                              if r_item_sort['tf'] in TIMEFRAMES else float('inf'),
                                                              # Handle TFs not in TIMEFRAMES
                                                              -r_item_sort['confidence_score']))
            except ValueError:  # Should not happen if TIMEFRAMES list is correct
                logging.warning(
                    f"–û—à–∏–±–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è {symbol_key}, –≤–æ–∑–º–æ–∂–Ω–æ TF –Ω–µ –≤ —Å–ø–∏—Å–∫–µ TIMEFRAMES. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.")
                sorted_rows = sorted(rows_list, key=lambda r_item_sort: (-r_item_sort['confidence_score']))

            table_data_tabulate = []
            for r_tab in sorted_rows:
                table_data_tabulate.append([
                    r_tab['tf'],
                    r_tab['timestamp_str_display'],
                    r_tab['signal'],
                    f"{r_tab['confidence_score']:.3f}",
                    f"{r_tab['predicted_delta']:.2%}" if not pd.isna(r_tab['predicted_delta']) else "N/A",
                    f"{r_tab['avg_delta_similar']:.2%}" if not pd.isna(r_tab['avg_delta_similar']) else "N/A",
                    f"{r_tab['std_delta_similar']:.2%}" if not pd.isna(r_tab['std_delta_similar']) else "N/A",
                    f"{r_tab['delta_final']:.2%}" if not pd.isna(r_tab['delta_final']) else "N/A",
                    "‚ùó" if r_tab['conflict'] else " ",
                    r_tab['signal_strength'].replace(" –°–∏–ª—å–Ω—ã–π", "üü¢").replace(" –£–º–µ—Ä–µ–Ω–Ω—ã–π", "üü°").replace(" –°–ª–∞–±—ã–π",
                                                                                                         "‚ö™"),
                    f"{r_tab['tp_hit_proba']:.1%}" if not pd.isna(r_tab['tp_hit_proba']) else "N/A"
                ])
            print(f"\nüìä  –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ —Å–∏–º–≤–æ–ª—É: {symbol_key}")
            try:
                print(tabulate(table_data_tabulate, headers=headers_tabulate, tablefmt="pretty", stralign="right",
                               numalign="right"))
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è {symbol_key}: {e}")

    logging.info("‚úÖ  –ü—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω.")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")
    parser.add_argument('--save', action='store_true', help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤ CSV —Ñ–∞–π–ª—ã.")
    args = parser.parse_args()

    try:
        predict_all_tf(args.save)
    except KeyboardInterrupt:
        print("\n[PredictAll] üõë –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C).")
        sys.exit(130)
    except Exception as e:
        logging.error(f"[PredictAll] üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)