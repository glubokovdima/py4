import pandas as pd
import joblib
import os
from datetime import datetime
from tabulate import tabulate
import argparse
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np
import sys
import logging

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s [PredictAll] - %(message)s',
                    stream=sys.stdout)

GROUP_MODELS = {
    "top15": [
        "ETHUSDT", "SOLUSDT", "BNBUSDT", "BCHUSDT", "LTCUSDT",
    "BTCUSDT", "ADAUSDT", "AVAXUSDT", "DOTUSDT", "XRPUSDT",
    "MATICUSDT", "LINKUSDT", "NEARUSDT", "ATOMUSDT", "TRXUSDT"
    ],
    "meme": [
        "PEPEUSDT", "DOGEUSDT", "FLOKIUSDT", "WIFUSDT", "SHIBUSDT"
    ],
    "defi": [ # –ü—Ä–∏–º–µ—Ä, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–π—Ç–µ —Å preprocess_features.py
        "UNIUSDT", "AAVEUSDT", "SUSHIUSDT", "COMPUSDT"
    ],
    "top55": ["ETHUSDT", "SOLUSDT", "BNBUSDT", "BCHUSDT", "LTCUSDT"] # –î–æ–±–∞–≤–∏–º, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
}

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤—É—Ö –±–∏–Ω–∞—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
PROBA_LONG_THRESHOLD = 0.55  # –ü–æ—Ä–æ–≥ –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ LONG
PROBA_SHORT_THRESHOLD = 0.55 # –ü–æ—Ä–æ–≥ –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ SHORT
PROBA_CONFIRM_OTHER_SIDE_LOW = 0.45 # –ï—Å–ª–∏ LONG, proba_short[1] –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å < —ç—Ç–æ–≥–æ. –ï—Å–ª–∏ SHORT, proba_long[1] < —ç—Ç–æ–≥–æ.

TIMEFRAMES = ['5m', '15m', '30m', '1h', '4h', '1d']
LOG_DIR_PREDICT = 'logs'
os.makedirs(LOG_DIR_PREDICT, exist_ok=True)

# –°—Ç–∞—Ä–∞—è TARGET_CLASS_NAMES –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
# TARGET_CLASS_NAMES = ['STRONG DOWN', 'DOWN', 'NEUTRAL', 'UP', 'STRONG UP']


def load_model_with_fallback(symbol, tf, model_type, group_suffix=""):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å.
    model_type –º–æ–∂–µ—Ç –±—ã—Ç—å 'clf_long', 'clf_short', 'reg_delta', 'reg_vol', 'clf_tp_hit'.
    group_suffix - —ç—Ç–æ files_suffix (–∏–º—è –≥—Ä—É–ø–ø—ã, —Å–∏–º–≤–æ–ª –∏–ª–∏ "all").
    """
    # 1. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ group_suffix - —ç—Ç–æ –∏–º—è —Å–∏–º–≤–æ–ª–∞)
    # –∏–ª–∏ –≥—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ group_suffix - —ç—Ç–æ –∏–º—è –≥—Ä—É–ø–ø—ã)
    # –∏–ª–∏ –æ–±—â–∞—è –º–æ–¥–µ–ª—å –ø–æ –¢–§ (–µ—Å–ª–∏ group_suffix - "all", –Ω–æ –º–æ–¥–µ–ª—å –Ω–∞–∑–≤–∞–Ω–∞ —Å "all") - —ç—Ç–æ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—É—Ç—å —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º (–∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∏–º–≤–æ–ª–æ–º –∏–ª–∏ –≥—Ä—É–ø–ø–æ–π)
    # models/BTCUSDT_5m_clf_long.pkl –∏–ª–∏ models/top8_5m_clf_long.pkl
    specific_model_path = f"models/{group_suffix}_{tf}_{model_type}.pkl"
    if group_suffix and group_suffix != "all" and os.path.exists(specific_model_path):
        logging.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º '{group_suffix}' –¥–ª—è {symbol} ({tf}): {model_type}")
        try:
            return joblib.load(specific_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {specific_model_path}: {e}")
            # Fallback

    # 2. –ï—Å–ª–∏ group_suffix –±—ã–ª —Å–∏–º–≤–æ–ª–æ–º –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –≥—Ä—É–ø–ø–æ–≤—É—é
    # –≠—Ç–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ, –µ—Å–ª–∏ group_suffix != symbol, –Ω–æ –º—ã –ø–µ—Ä–µ–¥–∞–µ–º group_suffix = symbol_filter
    # –ü—Ä–∞–≤–∏–ª—å–Ω–µ–µ –±—É–¥–µ—Ç —Ç–∞–∫: –µ—Å–ª–∏ group_suffix - —ç—Ç–æ —Å–∏–º–≤–æ–ª, —Ç–æ —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é, –ø–æ—Ç–æ–º –≥—Ä—É–ø–ø–æ–≤—É—é –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞.
    # –ï—Å–ª–∏ group_suffix - —ç—Ç–æ –≥—Ä—É–ø–ø–∞, —Ç–æ –∏—â–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç—É –≥—Ä—É–ø–ø–æ–≤—É—é.
    
    # –ü—É—Ç—å –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ group_suffix —ç—Ç–æ –∏–º—è –≥—Ä—É–ø–ø—ã –∏–ª–∏ "all"
    # (—Ç.–µ. –º—ã –Ω–µ –Ω–∞—à–ª–∏ –º–æ–¥–µ–ª—å –ø–æ group_suffix –∏ —Ç–µ–ø–µ—Ä—å –∏—â–µ–º –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É symbol)
    symbol_specific_model_path = f"models/{symbol}_{tf}_{model_type}.pkl"
    if os.path.exists(symbol_specific_model_path):
        logging.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {symbol} ({tf}): {model_type}")
        try:
            return joblib.load(symbol_specific_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {symbol_specific_model_path}: {e}")

    # 3. –ì—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —Å–∏–º–≤–æ–ª –≤—Ö–æ–¥–∏—Ç –≤ –∫–∞–∫—É—é-–ª–∏–±–æ –≥—Ä—É–ø–ø—É)
    for group_name_iter, symbol_list_iter in GROUP_MODELS.items():
        if symbol in symbol_list_iter:
            group_model_path_iter = f"models/{group_name_iter}_{tf}_{model_type}.pkl"
            if os.path.exists(group_model_path_iter):
                logging.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å ({group_name_iter}) –¥–ª—è {symbol} ({tf}): {model_type}")
                try:
                    return joblib.load(group_model_path_iter)
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –º–æ–¥–µ–ª–∏ {group_model_path_iter}: {e}")
                break # –ù–∞—à–ª–∏ –≥—Ä—É–ø–ø—É, –≤—ã—Ö–æ–¥–∏–º

    # 4. –û–±—â–∞—è –º–æ–¥–µ–ª—å –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º—É (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞ —Å–∏–º–≤–æ–ª–∞/–≥—Ä—É–ø–ø—ã)
    default_model_path = f"models/{tf}_{model_type}.pkl"
    if os.path.exists(default_model_path):
        logging.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∞—è –º–æ–¥–µ–ª—å –ø–æ TF –¥–ª—è {symbol} ({tf}): {model_type}")
        try:
            return joblib.load(default_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—â–µ–π –º–æ–¥–µ–ª–∏ {default_model_path}: {e}")
            return None # –Ø–≤–Ω—ã–π –≤–æ–∑–≤—Ä–∞—Ç None –ø—Ä–∏ –æ—à–∏–±–∫–µ

    logging.warning(f"–ú–æ–¥–µ–ª—å {model_type} –¥–ª—è {symbol} ({tf}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (—Å—É—Ñ—Ñ–∏–∫—Å '{group_suffix}').")
    return None


def load_features_list(tf, model_suffix, features_type):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    model_suffix - —ç—Ç–æ files_suffix (–∏–º—è –≥—Ä—É–ø–ø—ã, —Å–∏–º–≤–æ–ª –∏–ª–∏ "all").
    features_type - 'long' –∏–ª–∏ 'short'.
    """
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º
    # models/BTCUSDT_5m_features_long_selected.txt –∏–ª–∏ models/top8_5m_features_long_selected.txt
    specific_features_path = f"models/{model_suffix}_{tf}_features_{features_type}_selected.txt"
    if model_suffix and model_suffix != "all" and os.path.exists(specific_features_path):
        logging.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {specific_features_path}")
        try:
            with open(specific_features_path, "r", encoding="utf-8") as f:
                return [line.strip() for line in f if line.strip()]
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {specific_features_path}: {e}")
            # Fallback

    # –û–±—â–∏–π —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –¢–§ (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞ –≥—Ä—É–ø–ø—ã/—Å–∏–º–≤–æ–ª–∞)
    # models/5m_features_long_selected.txt
    # –≠—Ç–æ –º–µ–Ω–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ, –µ—Å–ª–∏ train_model –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º
    generic_features_path = f"models/{tf}_features_{features_type}_selected.txt"
    if os.path.exists(generic_features_path):
        logging.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—â–µ–≥–æ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {generic_features_path}")
        try:
            with open(generic_features_path, "r", encoding="utf-8") as f:
                return [line.strip() for line in f if line.strip()]
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {generic_features_path}: {e}")
    
    logging.warning(f"–§–∞–π–ª —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {features_type} (—Å—É—Ñ—Ñ–∏–∫—Å '{model_suffix}', TF {tf}) –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    return None


def compute_final_delta(delta_model, delta_history, sigma_history):
    if pd.isna(delta_history) or pd.isna(sigma_history) or pd.isna(delta_model): # –î–æ–±–∞–≤–∏–ª –ø—Ä–æ–≤–µ—Ä–∫—É delta_model
        return delta_model if pd.notna(delta_model) else np.nan # –í–æ–∑–≤—Ä–∞—â–∞–µ–º delta_model –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å, –∏–Ω–∞—á–µ NaN

    if sigma_history < 1e-9:
         sigma_history = 1e-9

    min_sigma = 0.005
    max_sigma = 0.020
    weight_hist_at_min_sigma = 0.6
    weight_hist_at_max_sigma = 0.2

    if sigma_history <= min_sigma:
        w_hist = weight_hist_at_min_sigma
    elif sigma_history >= max_sigma:
        w_hist = weight_hist_at_max_sigma
    else:
        alpha = (sigma_history - min_sigma) / (max_sigma - min_sigma)
        w_hist = weight_hist_at_min_sigma - alpha * (weight_hist_at_min_sigma - weight_hist_at_max_sigma)
    w_model = 1.0 - w_hist
    return round(w_model * delta_model + w_hist * delta_history, 5)


def get_signal_strength(delta_final, confidence, sigma_history):
    """ –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∏–ª—É —Å–∏–≥–Ω–∞–ª–∞: STRONG, MODERATE, WEAK """
    if pd.isna(delta_final) or pd.isna(confidence):
        return "‚ö™ –°–ª–∞–±—ã–π" # –∏–ª–∏ N/A
    if pd.isna(sigma_history):
        sigma_history = float('inf')

    abs_delta = abs(delta_final)
    
    # –ü–æ—Ä–æ–≥–∏ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
    delta_strong = 0.015  # –î–ª—è STRONG —Å–∏–≥–Ω–∞–ª–∞ –¥–µ–ª—å—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π
    delta_moderate = 0.007

    conf_strong = 0.15    # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è STRONG
    conf_moderate = 0.05

    sigma_reliable = 0.010 # –ù–∞–¥–µ–∂–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è

    if abs_delta >= delta_strong and confidence >= conf_strong and sigma_history <= sigma_reliable:
        return "üü¢ –°–∏–ª—å–Ω—ã–π"
    elif abs_delta >= delta_moderate and confidence >= conf_moderate:
        return "üü° –£–º–µ—Ä–µ–Ω–Ω—ã–π"
    else:
        return "‚ö™ –°–ª–∞–±—ã–π"


def is_conflict(delta_model, delta_history):
    if pd.isna(delta_history) or pd.isna(delta_model):
        return False
    if abs(delta_history) < 0.005:
        return False
    return (delta_model > 0 and delta_history < 0) or \
           (delta_model < 0 and delta_history > 0)


def get_confidence_hint(score): # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –ø–æ–ª–µ–∑–Ω–æ–π
    if score > 0.20: return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.10: return "–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.05: return "–£–º–µ—Ä–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.02: return "–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –±—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã"
    else: return "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å"


def calculate_trade_levels(entry, direction, atr_value, rr=2.0):
    if pd.isna(atr_value) or atr_value <= 1e-9 or direction == 'none': # –î–æ–±–∞–≤–∏–ª direction == 'none'
        return np.nan, np.nan
    if direction == 'long':
        sl = entry - atr_value
        tp = entry + atr_value * rr
    elif direction == 'short':
        sl = entry + atr_value
        tp = entry - atr_value * rr
    else: # Should not happen if direction == 'none' is checked
        return np.nan, np.nan
    
    sl = round(max(0, sl), 6) if direction == 'short' else round(sl, 6)
    tp = round(max(0, tp), 6)
    return sl, tp


def similarity_analysis(X_live_df, X_hist_df, deltas_hist_series, top_n=15):
    if X_hist_df.empty or len(X_hist_df) < top_n or X_live_df.empty:
        return np.nan, np.nan, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"
    if not X_live_df.columns.equals(X_hist_df.columns):
        common_cols = X_live_df.columns.intersection(X_hist_df.columns)
        if len(common_cols) == 0: return np.nan, np.nan, "–ù–µ—Ç –æ–±—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
        X_live_df = X_live_df[common_cols]
        X_hist_df = X_hist_df[common_cols]

    if not deltas_hist_series.index.equals(X_hist_df.index):
        try:
            deltas_hist_series = deltas_hist_series.reindex(X_hist_df.index)
        except Exception: return np.nan, np.nan, "–û—à–∏–±–∫–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"
    
    valid_indices = X_hist_df.index[~(X_hist_df.isnull().any(axis=1) | deltas_hist_series.isnull())]
    X_hist_df_clean = X_hist_df.loc[valid_indices]
    deltas_hist_clean = deltas_hist_series.loc[valid_indices]

    if len(X_hist_df_clean) < top_n:
        return np.nan, np.nan, f"–ú–∞–ª–æ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö ({len(X_hist_df_clean)} < {top_n})"
    
    scaler = StandardScaler()
    try:
        X_hist_scaled = scaler.fit_transform(X_hist_df_clean)
        x_live_scaled = scaler.transform(X_live_df)
    except Exception: return np.nan, np.nan, "–û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è"

    sims = cosine_similarity(X_hist_scaled, x_live_scaled).flatten()
    actual_top_n = min(top_n, len(sims))
    if actual_top_n <= 0: return np.nan, np.nan, "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ–ø-N"
    
    top_indices_cleaned = sims.argsort()[-actual_top_n:][::-1]
    original_top_indices = X_hist_df_clean.iloc[top_indices_cleaned].index
    if len(original_top_indices) == 0: return np.nan, np.nan, "–ù–µ—Ç —Å—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π"
    
    similar_deltas = deltas_hist_clean.loc[original_top_indices]
    avg_delta = similar_deltas.mean()
    std_delta = similar_deltas.std() if len(similar_deltas) > 1 else 0.0

    hint_val = "–í—ã—Å–æ–∫–∏–π —Ä–∞–∑–±—Ä–æ—Å" if pd.notna(std_delta) and std_delta > 0.02 else \
               "–£–º–µ—Ä–µ–Ω–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ" if pd.notna(std_delta) and std_delta > 0.01 else \
               "–°—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω" if pd.notna(std_delta) else "N/A —Ä–∞–∑–±—Ä–æ—Å"
    if pd.isna(avg_delta) or pd.isna(std_delta): hint_val = "N/A —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
    return avg_delta, std_delta, hint_val


def predict_all_tf(save_output_flag, symbol_filter=None, group_filter=None):
    target_syms = None
    files_suffix = "all"
    if symbol_filter:
        target_syms = [symbol_filter.upper()]
        files_suffix = target_syms[0]
    elif group_filter:
        group_key = group_filter.lower()
        if group_key not in GROUP_MODELS:
            logging.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞: '{group_filter}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(GROUP_MODELS.keys())}")
            return
        target_syms = GROUP_MODELS[group_key]
        files_suffix = group_key
    
    LATEST_PREDICTIONS_FILE = os.path.join(LOG_DIR_PREDICT, f'latest_predictions_{files_suffix}.csv')
    TRADE_PLAN_FILE = os.path.join(LOG_DIR_PREDICT, f'trade_plan_{files_suffix}.csv')

    logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (—Ñ–∏–ª—å—Ç—Ä: {files_suffix})...")
    all_predictions_data = []
    trade_plan_data = []

    for tf in TIMEFRAMES:
        logging.info(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf} ---")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å —É—á–µ—Ç–æ–º files_suffix)
        features_path_specific = f"data/features_{files_suffix}_{tf}.pkl"
        features_path_generic = f"data/features_{tf}.pkl"
        features_path = None

        if files_suffix != "all" and os.path.exists(features_path_specific):
            features_path = features_path_specific
        elif os.path.exists(features_path_generic):
            features_path = features_path_generic
        elif os.path.exists(features_path_specific): # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å–ª–∏ generic –Ω–µ –Ω–∞–π–¥–µ–Ω
             features_path = features_path_specific
        else:
            logging.warning(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf} (—Å—É—Ñ—Ñ–∏–∫—Å '{files_suffix}') –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫ –¢–§.")
            continue
        
        logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_path}")
        try:
            df = pd.read_pickle(features_path)
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {features_path}: {e}. –ü—Ä–æ–ø—É—Å–∫ –¢–§.")
            continue
        if df.empty:
            logging.warning(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path} –ø—É—Å—Ç. –ü—Ä–æ–ø—É—Å–∫ –¢–§.")
            continue

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è DataFrame –µ—Å–ª–∏ target_syms –∑–∞–¥–∞–Ω –∏ features_path –±—ã–ª generic
        if target_syms and features_path == features_path_generic:
            df = df[df['symbol'].isin(target_syms)].copy()
            if df.empty:
                logging.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ '{files_suffix}' –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ {tf}. –ü—Ä–æ–ø—É—Å–∫ –¢–§.")
                continue
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ –∏ TP-hit –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å features_long_cols
        features_long_cols = load_features_list(tf, files_suffix, "long")
        features_short_cols = load_features_list(tf, files_suffix, "short")

        if not features_long_cols or not features_short_cols:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf} (—Å—É—Ñ—Ñ–∏–∫—Å '{files_suffix}'). –ü—Ä–æ–ø—É—Å–∫ –¢–§.")
            continue
        
        features_for_others_cols = features_long_cols # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ long –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ –∏ TP-hit

        symbols_to_process_this_tf = df['symbol'].unique().tolist()
        symbols_to_process_this_tf.sort()

        for symbol in symbols_to_process_this_tf:
            df_sym = df[df['symbol'] == symbol].sort_values('timestamp').copy()
            if df_sym.empty or len(df_sym) < 2:
                logging.debug(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} {tf}. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            
            logging.info(f"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ {symbol} –Ω–∞ {tf} ---")

            model_long = load_model_with_fallback(symbol, tf, "clf_long", files_suffix)
            model_short = load_model_with_fallback(symbol, tf, "clf_short", files_suffix)
            model_delta = load_model_with_fallback(symbol, tf, "reg_delta", files_suffix)
            model_vol = load_model_with_fallback(symbol, tf, "reg_vol", files_suffix)
            model_tp_hit = load_model_with_fallback(symbol, tf, "clf_tp_hit", files_suffix)

            if not model_long or not model_short or not model_delta or not model_vol:
                logging.warning(f"–û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} {tf} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            row_df = df_sym.iloc[-1:].copy()
            hist_df_full = df_sym.iloc[:-1].copy()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            missing_long_cols = [col for col in features_long_cols if col not in row_df.columns]
            missing_short_cols = [col for col in features_short_cols if col not in row_df.columns]
            missing_others_cols = [col for col in features_for_others_cols if col not in row_df.columns]

            if missing_long_cols or missing_short_cols or missing_others_cols:
                logging.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è {symbol} {tf}: long_missing={missing_long_cols}, short_missing={missing_short_cols}, others_missing={missing_others_cols}. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            X_live_long = row_df[features_long_cols]
            X_live_short = row_df[features_short_cols]
            X_live_others = row_df[features_for_others_cols]

            if X_live_long.isnull().values.any() or X_live_short.isnull().values.any() or X_live_others.isnull().values.any():
                logging.warning(f"NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –¥–ª—è {symbol} {tf}. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ "others", —Ç.–µ. –æ—Ç long)
            hist_for_sim_features = hist_df_full[features_for_others_cols] if features_for_others_cols and not hist_df_full.empty else pd.DataFrame()
            hist_for_sim_deltas = hist_df_full['delta'] if 'delta' in hist_df_full.columns and not hist_df_full.empty else pd.Series(dtype=float)
            avg_delta_similar, std_delta_similar, similarity_hint = similarity_analysis(
                X_live_others, hist_for_sim_features, hist_for_sim_deltas
            )

            # –ü—Ä–æ–≥–Ω–æ–∑—ã
            try:
                proba_long_raw = model_long.predict_proba(X_live_long)[0]
                proba_short_raw = model_short.predict_proba(X_live_short)[0]
                
                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1 (Long=1 –∏–ª–∏ Short=1)
                proba_long_1 = proba_long_raw[list(model_long.classes_).index(1)] if 1 in model_long.classes_ else 0.0
                proba_short_1 = proba_short_raw[list(model_short.classes_).index(1)] if 1 in model_short.classes_ else 0.0

                predicted_delta = model_delta.predict(X_live_others)[0]
                predicted_volatility = model_vol.predict(X_live_others)[0]
                if pd.notna(predicted_volatility) and predicted_volatility < 0: predicted_volatility = 0.0

                tp_hit_proba = np.nan
                if model_tp_hit:
                    tp_hit_all_classes = model_tp_hit.predict_proba(X_live_others)[0]
                    if 1 in model_tp_hit.classes_:
                        tp_hit_proba = tp_hit_all_classes[list(model_tp_hit.classes_).index(1)]
                    elif len(tp_hit_all_classes) > 1 : # Fallback
                        tp_hit_proba = tp_hit_all_classes[1]


            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {symbol} {tf}: {e}", exc_info=True)
                continue

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            signal = "NEUTRAL"
            confidence = 0.0
            direction = 'none'

            if proba_long_1 > PROBA_LONG_THRESHOLD and proba_short_1 < PROBA_CONFIRM_OTHER_SIDE_LOW:
                signal = "UP"
                confidence = proba_long_1
                direction = 'long'
            elif proba_short_1 > PROBA_SHORT_THRESHOLD and proba_long_1 < PROBA_CONFIRM_OTHER_SIDE_LOW:
                signal = "DOWN"
                confidence = proba_short_1
                direction = 'short'
            
            hint = get_confidence_hint(confidence)
            ts_obj = pd.to_datetime(row_df['timestamp'].values[0])
            entry_price = float(row_df['close'].values[0])
            sl, tp = calculate_trade_levels(entry_price, direction, predicted_volatility)
            delta_final = compute_final_delta(predicted_delta, avg_delta_similar, std_delta_similar)
            signal_strength_val = get_signal_strength(delta_final, confidence, std_delta_similar)
            conflict_flag = is_conflict(predicted_delta, avg_delta_similar)

            prediction_entry = {
                'symbol': symbol, 'tf': tf, 'timestamp_obj': ts_obj,
                'timestamp_str_log': ts_obj.strftime('%Y-%m-%d %H:%M:%S'),
                'timestamp_str_display': ts_obj.strftime('%Y-%m-%d %H:%M'),
                'signal': signal, 'confidence_score': confidence, 'confidence_hint': hint,
                'proba_long_1': proba_long_1, 'proba_short_1': proba_short_1,
                'predicted_delta': predicted_delta,
                'predicted_volatility': predicted_volatility, 'entry': entry_price,
                'sl': sl, 'tp': tp, 'direction': direction,
                'avg_delta_similar': avg_delta_similar, 'std_delta_similar': std_delta_similar,
                'similarity_hint': similarity_hint,
                'delta_final': delta_final, 'signal_strength': signal_strength_val, 'conflict': conflict_flag,
                'tp_hit_proba': tp_hit_proba
            }
            all_predictions_data.append(prediction_entry)

            TRADE_PLAN_CONFIDENCE_THRESHOLD = 0.08
            TRADE_PLAN_TP_HIT_THRESHOLD = 0.55
            if direction != 'none' and confidence >= TRADE_PLAN_CONFIDENCE_THRESHOLD:
                if pd.isna(tp_hit_proba) or tp_hit_proba >= TRADE_PLAN_TP_HIT_THRESHOLD:
                    rr_value = np.nan
                    if pd.notna(sl) and pd.notna(tp) and abs(entry_price - sl) > 1e-9:
                        try: rr_value = round(abs(tp - entry_price) / abs(entry_price - sl), 2)
                        except ZeroDivisionError: rr_value = np.nan
                    
                    trade_plan_data.append({
                        'symbol': symbol, 'tf': tf, 'entry': entry_price, 'direction': direction,
                        'sl': sl, 'tp': tp, 'rr': rr_value,
                        'confidence': confidence, 'signal': signal, 
                        'timestamp': prediction_entry['timestamp_str_log'], 'hint': hint,
                        'tp_hit_proba': tp_hit_proba,
                        'proba_long': proba_long_1, 'proba_short': proba_short_1 # –î–æ–±–∞–≤–∏–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    })

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥
    if save_output_flag and all_predictions_data:
        logging.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        df_out = pd.DataFrame(all_predictions_data)
        # –£–¥–∞–ª—è–µ–º proba_dict –µ—Å–ª–∏ –æ–Ω —Ç–∞–º —Å–ª—É—á–∞–π–Ω–æ –æ–∫–∞–∑–∞–ª—Å—è, –∏ timestamp_obj
        df_out_save = df_out.drop(columns=['proba_dict', 'timestamp_obj'], errors='ignore')
        
        csv_columns_order = [
            'symbol', 'tf', 'timestamp_str_log', 'signal', 'confidence_score', 'tp_hit_proba',
            'proba_long_1', 'proba_short_1', # –ù–æ–≤—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            'predicted_delta', 'avg_delta_similar', 'std_delta_similar', 'delta_final',
            'predicted_volatility', 'entry', 'sl', 'tp', 'direction',
            'signal_strength', 'conflict', 'confidence_hint', 'similarity_hint'
        ]
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ df_out_save –≤–∫–ª—é—á–µ–Ω—ã
        final_csv_columns = [col for col in csv_columns_order if col in df_out_save.columns]
        for col in df_out_save.columns:
            if col not in final_csv_columns:
                final_csv_columns.append(col)
        
        if not df_out_save.empty:
            df_out_save = df_out_save[final_csv_columns] # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º
            df_out_save.rename(columns={'timestamp_str_log': 'timestamp'}, inplace=True)
            df_out_save.to_csv(LATEST_PREDICTIONS_FILE, index=False, float_format='%.6f')
            logging.info(f"üìÑ –°–∏–≥–Ω–∞–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {LATEST_PREDICTIONS_FILE}")

        if trade_plan_data:
            df_trade_plan = pd.DataFrame(trade_plan_data)
            trade_plan_cols_order = [
                'symbol', 'tf', 'timestamp', 'direction', 'signal', 'confidence', 'tp_hit_proba',
                'proba_long', 'proba_short', # –ù–æ–≤—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                'entry', 'sl', 'tp', 'rr', 'hint'
            ]
            final_trade_plan_cols = [col for col in trade_plan_cols_order if col in df_trade_plan.columns]
            for col in df_trade_plan.columns:
                if col not in final_trade_plan_cols:
                    final_trade_plan_cols.append(col)

            if not df_trade_plan.empty:
                df_trade_plan = df_trade_plan[final_trade_plan_cols] # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º
                df_trade_plan.to_csv(TRADE_PLAN_FILE, index=False, float_format='%.6f')
                logging.info(f"üìà –¢–æ—Ä–≥–æ–≤—ã–π –ø–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {TRADE_PLAN_FILE}")
        else:
            logging.info("ü§∑ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞.")


    if all_predictions_data:
        headers_tabulate = ["TF", "Timestamp", "–°–∏–≥–Ω–∞–ª", "Conf.", "P(L)", "P(S)", "ŒîModel", "ŒîHist", "œÉHist", "ŒîFinal", "–ö–æ–Ω—Ñ?", "–°–∏–ª–∞", "TP Hit%"]
        grouped_by_symbol = {}
        for item in all_predictions_data: # –ò—Å–ø–æ–ª—å–∑—É–µ–º all_predictions_data
            grouped_by_symbol.setdefault(item['symbol'], []).append(item)
        
        sorted_symbols_for_display = sorted(grouped_by_symbol.keys())
        for symbol_key in sorted_symbols_for_display:
            rows_list = grouped_by_symbol[symbol_key]
            try:
                sorted_rows = sorted(rows_list, key=lambda r: (TIMEFRAMES.index(r['tf']) if r['tf'] in TIMEFRAMES else float('inf'), -r['confidence_score']))
            except ValueError:
                sorted_rows = sorted(rows_list, key=lambda r: -r['confidence_score'])

            table_data_tabulate = []
            for r_tab in sorted_rows:
                table_data_tabulate.append([
                    r_tab['tf'], r_tab['timestamp_str_display'], r_tab['signal'],
                    f"{r_tab['confidence_score']:.3f}",
                    f"{r_tab['proba_long_1']:.2f}", f"{r_tab['proba_short_1']:.2f}", # –ù–æ–≤—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                    f"{r_tab['predicted_delta']:.2%}" if pd.notna(r_tab['predicted_delta']) else "N/A",
                    f"{r_tab['avg_delta_similar']:.2%}" if pd.notna(r_tab['avg_delta_similar']) else "N/A",
                    f"{r_tab['std_delta_similar']:.2%}" if pd.notna(r_tab['std_delta_similar']) else "N/A",
                    f"{r_tab['delta_final']:.2%}" if pd.notna(r_tab['delta_final']) else "N/A",
                    "‚ùó" if r_tab['conflict'] else " ",
                    r_tab['signal_strength'].replace(" –°–∏–ª—å–Ω—ã–π", "üü¢").replace(" –£–º–µ—Ä–µ–Ω–Ω—ã–π", "üü°").replace(" –°–ª–∞–±—ã–π", "‚ö™"),
                    f"{r_tab['tp_hit_proba']:.1%}" if pd.notna(r_tab['tp_hit_proba']) else "N/A"
                ])
            print(f"\nüìä –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ —Å–∏–º–≤–æ–ª—É: {symbol_key}")
            try:
                print(tabulate(table_data_tabulate, headers=headers_tabulate, tablefmt="pretty", stralign="right", numalign="right"))
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –≤—ã–≤–æ–¥–∞ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è {symbol_key}: {e}")
                for row in table_data_tabulate: print(row)
    
    logging.info("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω.")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")
    parser.add_argument('--save', action='store_true', help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤ CSV —Ñ–∞–π–ª—ã.")
    parser.add_argument('--symbol', type=str, help="–û–¥–∏–Ω —Å–∏–º–≤–æ–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ –∏–º—è –≥—Ä—É–ø–ø—ã).")
    parser.add_argument('--symbol-group', type=str, help="–ì—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, –µ—Å–ª–∏ --symbol —ç—Ç–æ –∏–º—è –≥—Ä—É–ø–ø—ã).")
    args = parser.parse_args()

    symbol_filter_arg = None
    group_filter_arg = None

    if args.symbol_group: # –ï—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–∞ –≥—Ä—É–ø–ø–∞
        group_filter_arg = args.symbol_group.lower()
        if group_filter_arg not in GROUP_MODELS:
            logging.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞: '{args.symbol_group}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(GROUP_MODELS.keys())}")
            sys.exit(1)
    elif args.symbol: # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Å–∏–º–≤–æ–ª (–∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–º–µ–Ω–µ–º –≥—Ä—É–ø–ø—ã)
        symbol_lower_arg = args.symbol.lower()
        if symbol_lower_arg in GROUP_MODELS:
            group_filter_arg = symbol_lower_arg
            logging.info(f"–ê—Ä–≥—É–º–µ–Ω—Ç --symbol '{args.symbol}' —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–∞–∫ –≥—Ä—É–ø–ø–∞ '{group_filter_arg}'.")
        else:
            symbol_filter_arg = args.symbol.upper()
    
    try:
        predict_all_tf(
            args.save,
            symbol_filter=symbol_filter_arg,
            group_filter=group_filter_arg
        )
    except KeyboardInterrupt:
        print("\n[PredictAll] üõë –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–µ—Ä–≤–∞–Ω–∞ (Ctrl+C).")
        sys.exit(130)
    except Exception as e:
        logging.error(f"[PredictAll] üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)