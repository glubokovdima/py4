import pandas as pd
import joblib
import os
from datetime import datetime
from tabulate import tabulate
import argparse
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np
import sys  # –î–ª—è Ctrl+C
import logging  # –î–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è


# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ (–º–æ–∂–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—å –≤ —Ñ–∞–π–ª –∏–ª–∏ —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Å–æ–ª—å)
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s [PredictAll] - %(message)s',
                    stream=sys.stdout)

# –°–ª–æ–≤–∞—Ä—å –≥—Ä—É–ø–ø: —Å–∏–º–≤–æ–ª—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ –∏–º–µ–Ω–µ–º (–∫–ª—é—á–æ–º)
# >>> –î–û–ë–ê–í–õ–ï–ù–û/–û–ë–ù–û–í–õ–ï–ù–û —Å–æ–≥–ª–∞—Å–Ω–æ –ø–∞—Ç—á—É
GROUP_MODELS = {
    "top8": [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT",
        "XRPUSDT", "ADAUSDT", "LINKUSDT", "AVAXUSDT"
    ],
    "meme": [
        "PEPEUSDT", "DOGEUSDT", "FLOKIUSDT", "WIFUSDT", "SHIBUSDT"
    ]
}

def load_model_with_fallback(symbol, tf, model_type):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å:
    1. –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: models/BTCUSDT_15m_clf_class.pkl
    2. –ì—Ä—É–ø–ø–æ–≤—É—é –º–æ–¥–µ–ª—å: models/top8_15m_clf_class.pkl (–µ—Å–ª–∏ symbol –≤—Ö–æ–¥–∏—Ç –≤ –≥—Ä—É–ø–ø—É)
    3. –û–±—â—É—é –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º—É: models/15m_clf_class.pkl
    """
    # 1. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    symbol_model_path = f"models/{symbol}_{tf}_{model_type}.pkl"
    if os.path.exists(symbol_model_path):
        # print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {symbol}: {model_type}") # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª print, –ø–µ—Ä–µ–∫–ª—é—á—É –Ω–∞ logging
        logging.debug(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {symbol}: {model_type}")
        try:
            return joblib.load(symbol_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {symbol_model_path}: {e}")
            # Fallback to next option

    # 2. –ì—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å
    for group_name, symbol_list in GROUP_MODELS.items():
        if symbol in symbol_list:
            group_model_path = f"models/{group_name}_{tf}_{model_type}.pkl"
            if os.path.exists(group_model_path):
                # print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å ({group_name}) –¥–ª—è {symbol}: {model_type}") # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª print, –ø–µ—Ä–µ–∫–ª—é—á—É –Ω–∞ logging
                logging.debug(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å ({group_name}) –¥–ª—è {symbol}: {model_type}")
                try:
                    return joblib.load(group_model_path)
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –º–æ–¥–µ–ª–∏ {group_model_path}: {e}")
                    # Fallback to next option
                # Found group model, no need to check other groups for this symbol
                break # Exit the group loop

    # 3. –û–±—â–∞—è –º–æ–¥–µ–ª—å
    default_model_path = f"models/{tf}_{model_type}.pkl"
    if os.path.exists(default_model_path):
        # print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∞—è –º–æ–¥–µ–ª—å –ø–æ TF: {model_type} ‚Üí {default_model_path}") # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª print, –ø–µ—Ä–µ–∫–ª—é—á—É –Ω–∞ logging
        logging.debug(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—â–∞—è –º–æ–¥–µ–ª—å –ø–æ TF: {model_type} ‚Üí {default_model_path}")
        try:
            return joblib.load(default_model_path)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—â–µ–π –º–æ–¥–µ–ª–∏ {default_model_path}: {e}")
            return None

    # print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol}, –Ω–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã, –Ω–∏ –æ–±—â–∞—è: {model_type}") # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª print, –ø–µ—Ä–µ–∫–ª—é—á—É –Ω–∞ logging
    logging.warning(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol}, –Ω–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã, –Ω–∏ –æ–±—â–∞—è: {model_type} –Ω–∞ {tf}")
    return None

# –£–¥–∞–ª–∏–ª —Å—Ç–∞—Ä—É—é –¥—É–±–ª–∏—Ä—É—é—â—É—é—Å—è —Ñ—É–Ω–∫—Ü–∏—é load_model (–∫–æ—Ç–æ—Ä–∞—è load_model(tf, model_type)),
# —Ç–∞–∫ –∫–∞–∫ load_model_with_fallback —Ç–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –≤–∫–ª—é—á–∞–µ—Ç –ª–æ–≥–∏–∫—É fallback.
# –ï—Å–ª–∏ —ç—Ç–∞ —Å—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≥–¥–µ-—Ç–æ –µ—â–µ —Å –¥—Ä—É–≥–∏–º–∏ –ø—É—Ç—è–º–∏, –≤–æ–∑–º–æ–∂–Ω–æ, –µ–µ –Ω—É–∂–Ω–æ
# –æ—Å—Ç–∞–≤–∏—Ç—å –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å, –Ω–æ –¥–ª—è —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏ predict_all_tf –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ load_model_with_fallback.
# –°—É–¥—è –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É –∫–æ–¥—É, —Å—Ç–∞—Ä–∞—è load_model –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ predict_all_tf
# –∏ –µ–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–ø–µ—Ä—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç–∞ load_model_with_fallback.

TIMEFRAMES = ['5m', '15m', '30m', '1h', '4h', '1d']  # –û—Å–Ω–æ–≤–Ω—ã–µ –¢–§ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
FEATURES_PATH_TEMPLATE = 'data/features_{tf}.pkl'
# MODEL_PATH_TEMPLATE = 'models/{tf}_{model_type}.pkl' # –£–¥–∞–ª–∏–ª, —Ç–∞–∫ –∫–∞–∫ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è load_model_with_fallback –ø—É—Ç—è–º–∏

LOG_DIR_PREDICT = 'logs'
LATEST_PREDICTIONS_FILE = os.path.join(LOG_DIR_PREDICT, 'latest_predictions.csv')
TRADE_PLAN_FILE = os.path.join(LOG_DIR_PREDICT, 'trade_plan.csv')

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤/—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(LOG_DIR_PREDICT, exist_ok=True)

TARGET_CLASS_NAMES = ['STRONG DOWN', 'DOWN', 'NEUTRAL', 'UP', 'STRONG UP']


def compute_final_delta(delta_model, delta_history, sigma_history):
    if pd.isna(delta_history) or pd.isna(sigma_history):
        return round(delta_model, 5)
    if sigma_history < 0.005:
        w1, w2 = 0.4, 0.6
    elif sigma_history > 0.02:
        w1, w2 = 0.8, 0.2
    else:
        alpha = (sigma_history - 0.005) / (0.02 - 0.005)
        w1 = 0.4 + alpha * (0.8 - 0.4)
        w2 = 1.0 - w1
    return round(w1 * delta_model + w2 * delta_history, 5)


def get_signal_strength(delta_final, confidence, sigma_history):
    if pd.isna(sigma_history):
        sigma_history = float('inf')

    # –ù–µ–º–Ω–æ–≥–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–ª –ª–æ–≥–∏–∫—É —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞, —á—Ç–æ–±—ã –æ–Ω–∞ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º confidence –∏ sigma
    # –≠—Ç–æ –º–æ–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ, –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ –∂–µ–ª–∞–Ω–∏—é
    if abs(delta_final) > 0.02 and confidence > 0.5 and sigma_history < 0.015: # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –°–∏–ª—å–Ω–æ–≥–æ
        return "üü¢ –°–∏–ª—å–Ω—ã–π"
    elif abs(delta_final) > 0.01 and confidence > 0.2 and sigma_history < 0.025: # –£–º–µ—Ä–µ–Ω–Ω—ã–π
        return "üü° –£–º–µ—Ä–µ–Ω–Ω—ã–π"
    else: # –°–ª–∞–±—ã–π –∏–ª–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π
        return "‚ö™ –°–ª–∞–±—ã–π"

def is_conflict(delta_model, delta_history):
    if pd.isna(delta_history) or pd.isna(delta_model):
        return False
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–µ–ª—å—Ç–∞ –∏—Å—Ç–æ—Ä–∏–∏ –Ω–µ –±–ª–∏–∑–∫–∞ –∫ –Ω—É–ª—é
    if abs(delta_history) < 0.005: # –ü–æ—Ä–æ–≥ –¥–ª—è "–±–ª–∏–∑–∫–æ –∫ –Ω—É–ª—é", –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å
        return False
    return (delta_model > 0 and delta_history < 0) or \
        (delta_model < 0 and delta_history > 0)


# –£–¥–∞–ª–∏–ª —Å—Ç–∞—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é load_model, –µ–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–ø–µ—Ä—å –≤ load_model_with_fallback.


def get_confidence_hint(score):
    if score > 0.2:
        return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.1:
        return "–•–æ—Ä–æ—à–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"
    elif score > 0.05:
        return "–°–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª" # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –≥—Ä–∞–Ω–∏—Ü–µ–π, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π —Å–∏–≥–Ω–∞–ª –Ω–µ —Ç–æ—Ä–≥—É–µ–º
    else:
        return "–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Äî –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å" # –ù–∏–∂–µ 0.05 —Å–æ–≤—Å–µ–º –Ω–∏–∑–∫–æ


def calculate_trade_levels(entry, direction, atr_value, rr=2.0):
    if pd.isna(atr_value) or atr_value <= 1e-9:
        return np.nan, np.nan
    if direction == 'long':
        sl = entry - atr_value
        tp = entry + atr_value * rr
    elif direction == 'short':
        sl = entry + atr_value
        tp = entry - atr_value * rr
    else:
        return np.nan, np.nan
    # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ü–µ–Ω—ã –∞–∫—Ç–∏–≤–∞
    # –î–ª—è –∫—Ä–∏–ø—Ç—ã 6 –∑–Ω–∞–∫–æ–≤ –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –Ω–æ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è –æ—á–µ–Ω—å –¥–µ—à–µ–≤—ã—Ö –º–æ–Ω–µ—Ç
    return round(sl, 6), round(tp, 6)


def similarity_analysis(X_live_df, X_hist_df, deltas_hist_series, top_n=15):
    if X_hist_df.empty or len(X_hist_df) < top_n:
        return np.nan, np.nan, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"

    common_cols = X_live_df.columns.intersection(X_hist_df.columns)
    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ X_live, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ X_hist_df
    missing_in_hist = list(set(X_live_df.columns) - set(common_cols))
    if missing_in_hist:
         logging.warning(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ {missing_in_hist} –∏–∑ X_live –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ X_hist_df. –ê–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –ø–æ –æ–±—â–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º.")

    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ X_hist_df, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ X_live
    missing_in_live = list(set(X_hist_df.columns) - set(common_cols))
    if missing_in_live:
         logging.warning(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ {missing_in_live} –∏–∑ X_hist_df –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ X_live. –ê–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –ø–æ –æ–±—â–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º.")


    X_live_df_common = X_live_df[common_cols]
    X_hist_df_common = X_hist_df[common_cols]

    if X_live_df_common.empty or X_hist_df_common.empty:
        return np.nan, np.nan, "–ù–µ—Ç –æ–±—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ X_hist_df_common –∏ deltas_hist_series –∏–º–µ—é—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã
    # –∏ —É–¥–∞–ª–∏–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –∞–Ω–∞–ª–∏–∑–æ–º —Å—Ö–æ–∂–µ—Å—Ç–∏
    hist_df_aligned = X_hist_df_common.copy()
    hist_deltas_aligned = deltas_hist_series.loc[hist_df_aligned.index].copy() # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –¥–µ–ª—å—Ç—ã –∫ —Ç–µ–∫—É—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –µ—Å—Ç—å NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –ò–õ–ò –≥–¥–µ –µ—Å—Ç—å NaN –≤ –¥–µ–ª—å—Ç–∞—Ö –∏—Å—Ç–æ—Ä–∏–∏
    valid_indices = hist_df_aligned.dropna(how='any').index.intersection(hist_deltas_aligned.dropna().index)

    hist_df_aligned_clean = hist_df_aligned.loc[valid_indices]
    hist_deltas_aligned_clean = hist_deltas_aligned.loc[valid_indices]


    if len(hist_df_aligned_clean) < top_n:
         return np.nan, np.nan, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å—Ç—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö ({len(hist_df_aligned_clean)} < {top_n}) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"

    scaler = StandardScaler()
    try:
        X_hist_scaled = scaler.fit_transform(hist_df_aligned_clean)
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ X_live_df_common –∏–º–µ–µ—Ç —Ç—É –∂–µ —Ñ–æ—Ä–º—É –∏ –∫–æ–ª–æ–Ω–∫–∏, —á—Ç–æ –∏ hist_df_aligned_clean
        # –ø–µ—Ä–µ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –ï—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –≤—ã–≤–µ–¥–µ–º –æ—à–∏–±–∫—É.
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ common_cols —É–∂–µ –æ–±–µ—Å–ø–µ—á–∏–ª –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π...
        if not X_live_df_common.columns.equals(hist_df_aligned_clean.columns):
             logging.error("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –º–µ–∂–¥—É X_live_df_common –∏ hist_df_aligned_clean –ø–µ—Ä–µ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏!")
             return np.nan, np.nan, "–û—à–∏–±–∫–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å—Ö–æ–∂–µ—Å—Ç–∏"

        x_live_scaled = scaler.transform(X_live_df_common) # X_live_df_common - —ç—Ç–æ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞

    except ValueError as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ similarity_analysis: {e}")
        return np.nan, np.nan, "–û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
    except Exception as e:
         logging.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ similarity_analysis: {e}", exc_info=True)
         return np.nan, np.nan, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"


    sims = cosine_similarity(X_hist_scaled, x_live_scaled).flatten()

    # Indices of the top N similar historical points in the *cleaned and scaled* history data
    top_indices_in_cleaned_hist = sims.argsort()[-top_n:][::-1]

    # Get the original indices from the full historical data (or from the cleaned data index)
    # Use the indices from the cleaned data dataframe
    original_top_indices = hist_df_aligned_clean.iloc[top_indices_in_cleaned_hist].index

    if len(original_top_indices) == 0:
        return np.nan, np.nan, "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"

    # Get the deltas for these original indices from the cleaned deltas series
    similar_deltas = hist_deltas_aligned_clean.loc[original_top_indices]

    # Calculate mean and std of these deltas
    avg_delta = similar_deltas.mean()
    std_delta = similar_deltas.std()

    # Ensure std_delta is not NaN if there's only one point (std is 0)
    if len(similar_deltas) == 1:
        std_delta = 0.0

    hint = (
        "–í—ã—Å–æ–∫–∏–π —Ä–∞–∑–±—Ä–æ—Å" if std_delta > 0.02 else
        "–£–º–µ—Ä–µ–Ω–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ" if std_delta > 0.01 else
        "–°—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω"
    )
    if pd.isna(avg_delta) or pd.isna(std_delta):
        hint = "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å—Ö–æ–∂–µ—Å—Ç–∏"

    return avg_delta, std_delta, hint


# >>> –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã symbol_filter –∏ group_filter
def predict_all_tf(save_output_flag, symbol_filter=None, group_filter=None):
    # ----- –ù–æ–≤–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª—É –∏–ª–∏ –≥—Ä—É–ø–ø–µ -----
    # >>> –î–û–ë–ê–í–õ–ï–ù–û —Å–æ–≥–ª–∞—Å–Ω–æ –ø–∞—Ç—á—É
    target_syms = None
    if symbol_filter:
        target_syms = [symbol_filter.upper()] # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        logging.info(f"üõ†Ô∏è –§–∏–ª—å—Ç—Ä –ø–æ —Å–∏–º–≤–æ–ª—É: {target_syms[0]}")
    elif group_filter:
        group_key = group_filter.lower() # –ü—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á –≥—Ä—É–ø–ø—ã –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        if group_key not in GROUP_MODELS:
            logging.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤: '{group_filter}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≥—Ä—É–ø–ø—ã: {list(GROUP_MODELS.keys())}")
            return # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏, –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        target_syms = GROUP_MODELS[group_key]
        logging.info(f"üõ†Ô∏è –§–∏–ª—å—Ç—Ä –ø–æ –≥—Ä—É–ø–ø–µ: '{group_filter}' ({len(target_syms)} —Å–∏–º–≤–æ–ª–æ–≤)")
    else:
        logging.info("üõ†Ô∏è –ë–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ —Å–∏–º–≤–æ–ª—É/–≥—Ä—É–ø–ø–µ (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö).")


    logging.info("üöÄ  –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤...")
    all_predictions_data = []
    trade_plan = []

    for tf in TIMEFRAMES:
        logging.info(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf} ---")
        features_path = FEATURES_PATH_TEMPLATE.format(tf=tf)
        if not os.path.exists(features_path):
            logging.warning(f"‚ö†Ô∏è –§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {features_path}. –ü—Ä–æ–ø—É—Å–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf}.")
            continue

        try:
            df = pd.read_pickle(features_path)
        except Exception as e:
            logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path}: {e}. –ü—Ä–æ–ø—É—Å–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf}.")
            continue

        if df.empty:
            logging.warning(f"‚ö†Ô∏è –§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Å—Ç: {features_path}. –ü—Ä–æ–ø—É—Å–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf}.")
            continue

        features_list_path = f"models/{tf}_features_selected.txt"
        if not os.path.exists(features_list_path):
            logging.error(
                f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ '{features_list_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –¥–ª—è {tf}. –ü—Ä–æ–ø—É—Å–∫.")
            continue
        try:
            with open(features_list_path, "r", encoding="utf-8") as f:
                feature_cols_from_file = [line.strip() for line in f if line.strip()]
            if not feature_cols_from_file:
                logging.error(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ '{features_list_path}' –ø—É—Å—Ç. –ü—Ä–æ–ø—É—Å–∫ {tf}.")
                continue
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ '{features_list_path}': {e}. –ü—Ä–æ–ø—É—Å–∫ {tf}.")
            continue

        # Model loading is now per-symbol, so it's moved inside the symbol loop.
        # The old loading location and logging for TP-hit classes per TF is removed from here.

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        available_symbols_in_data = df['symbol'].unique()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –º—ã –±—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤ —ç—Ç–æ–º TF
        symbols_to_process_this_tf = []
        if target_syms is None:
             # –ù–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
             symbols_to_process_this_tf = available_symbols_in_data
        else:
             # –ï—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª—ã –∏–∑ target_syms, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
             symbols_to_process_this_tf = [sym for sym in target_syms if sym in available_symbols_in_data]
             missing_filtered_symbols = [sym for sym in target_syms if sym not in available_symbols_in_data]
             if missing_filtered_symbols:
                 logging.warning(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–∞ {tf}: {missing_filtered_symbols}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ö.")


        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏–º–≤–æ–ª—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–≥–æ –≤—ã–≤–æ–¥–∞
        symbols_to_process_this_tf.sort()

        if not symbols_to_process_this_tf:
             logging.info(f"ü§∑ –ù–µ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ {tf} –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–∞ –∏–ª–∏ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥–∞–Ω–Ω—ã—Ö.")
             continue # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Ç–∞–π–º—Ñ—Ä–µ–π–º—É

        for symbol in symbols_to_process_this_tf:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–∞ –∑–¥–µ—Å—å, —Ç–∞–∫ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ symbols_to_process_this_tf —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω

            df_sym = df[df['symbol'] == symbol].sort_values('timestamp').copy()
            # –ü—É—Å—Ç–æ–π df_sym –Ω–µ –¥–æ–ª–∂–µ–Ω –≤–æ–∑–Ω–∏–∫–∞—Ç—å —Ç—É—Ç, —Ç.–∫. —Å–∏–º–≤–æ–ª –≤–∑—è—Ç –∏–∑ available_symbols_in_data,
            # –Ω–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø–æ–≤—Ä–µ–¥–∏—Ç.
            if df_sym.empty:
                logging.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –ø—É—Å—Ç–æ–π DataFrame –¥–ª—è {symbol} –Ω–∞ {tf}. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            logging.info(f"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ {symbol} –Ω–∞ {tf} ---")

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏ –¢–§
            model_class = load_model_with_fallback(symbol, tf, "clf_class")
            model_delta = load_model_with_fallback(symbol, tf, "reg_delta")
            model_vol = load_model_with_fallback(symbol, tf, "reg_vol")
            model_tp_hit = load_model_with_fallback(symbol, tf, "clf_tp_hit")

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ TP-hit –º–æ–¥–µ–ª–∏ (–ø–µ—Ä–µ–º–µ—â–µ–Ω–æ —Å—é–¥–∞)
            if model_tp_hit:
                logging.debug(f"–ö–ª–∞—Å—Å—ã TP-hit –º–æ–¥–µ–ª–∏ ({symbol}, {tf}): {getattr(model_tp_hit, 'classes_', 'N/A')}")

            if not all([model_class, model_delta, model_vol]):  # model_tp_hit is optional
                logging.warning(
                    f"‚ö†Ô∏è –û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol} –Ω–∞ {tf} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–∞ —ç—Ç–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ.")
                continue  # Skip this symbol for this tf

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –∏ –æ—Å—Ç–∞–ª—å–Ω–æ–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
            row_df = df_sym.iloc[-1:].copy()
            hist_df_full = df_sym.iloc[:-1].copy()

            if 'close' not in row_df.columns or 'timestamp' not in row_df.columns:
                logging.warning(f"‚ö†Ô∏è –í –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} {tf} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç 'close' –∏–ª–∏ 'timestamp'. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ feature_cols_from_file –Ω–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ row_df
            missing_cols_in_df_for_symbol = [col for col in feature_cols_from_file if col not in row_df.columns]
            if missing_cols_in_df_for_symbol:
                logging.error(
                    f"‚ùå –í DataFrame –¥–ª—è {symbol} –Ω–∞ {tf} –∏–∑ {features_path} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã {missing_cols_in_df_for_symbol}, "
                    f"–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ (—Å–æ–≥–ª–∞—Å–Ω–æ {features_list_path}). –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–∞ —ç—Ç–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ."
                )
                continue

            X_live = row_df[feature_cols_from_file]
            if X_live.isnull().values.any():
                nan_features = X_live.columns[X_live.isnull().any()].tolist()
                logging.warning(f"‚ö†Ô∏è –í X_live –¥–ª—è {symbol} {tf} –µ—Å—Ç—å NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {nan_features}. –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–∞ —ç—Ç–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ.")
                continue

            # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            avg_delta_similar, std_delta_similar, similarity_hint = np.nan, np.nan, "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"
            if 'delta' not in hist_df_full.columns:
                logging.debug(f"–°—Ç–æ–ª–±–µ—Ü 'delta' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö hist_df_full –¥–ª—è {symbol} {tf}.")
            else:
                 # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ò —Å—Ç–æ–ª–±—Ü–∞ 'delta' –≤ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                required_hist_cols = feature_cols_from_file + ['delta']
                missing_hist_features_and_delta = [col for col in required_hist_cols if col not in hist_df_full.columns]

                if missing_hist_features_and_delta:
                    logging.debug(f"–í hist_df_full –¥–ª—è {symbol} {tf} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã {missing_hist_features_and_delta} –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏.")
                    similarity_hint = "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"
                else:
                    hist_for_sim_features = hist_df_full[feature_cols_from_file].copy()
                    hist_for_sim_deltas = hist_df_full['delta'].copy()

                    # –ß–∏—Å—Ç–∫–∞ –æ—Ç NaN –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ similarity_analysis

                    # Ensure enough data for similarity analysis - moved check inside similarity_analysis
                    avg_delta_similar, std_delta_similar, similarity_hint = similarity_analysis(
                        X_live, hist_for_sim_features, hist_for_sim_deltas, top_n=min(15, len(hist_for_sim_features)-1)) # top_n –Ω–µ –±–æ–ª—å—à–µ, —á–µ–º —Å—Ç—Ä–æ–∫-1


            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            try:
                proba_raw = model_class.predict_proba(X_live)[0]
            except Exception as e:
                logging.error(
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ model_class.predict_proba –¥–ª—è {symbol} {tf} —Å X_live shape {X_live.shape}: {e}. –ü—Ä–æ–ø—É—Å–∫ —Å–∏–º–≤–æ–ª–∞.")
                continue

            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ proba_raw —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª–∞—Å—Å–æ–≤ –º–æ–¥–µ–ª–∏
            model_classes = getattr(model_class, 'classes_', None)
            if model_classes is None or len(proba_raw) != len(model_classes):
                 logging.error(f"‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π ({len(proba_raw)}) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–ª–∞—Å—Å–∞–º –º–æ–¥–µ–ª–∏ ({len(model_classes) if model_classes is not None else 'N/A'}) –¥–ª—è {symbol} {tf}. –ü—Ä–æ–ø—É—Å–∫.")
                 continue

            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å –∏–º–µ–Ω–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤ –º–æ–¥–µ–ª–∏
            proba_dict_model_order = {model_classes[i]: proba_raw[i] for i in range(len(proba_raw))}

            # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ –º–æ–¥–µ–ª–∏)
            pred_class_label = model_classes[proba_raw.argmax()]
            signal = pred_class_label # –°–∏–≥–Ω–∞–ª —Ç–µ–ø–µ—Ä—å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∫–ª–∞—Å—Å–∞ –º–æ–¥–µ–ª–∏

            # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ª—É—á—à–µ–π –∏ –≤—Ç–æ—Ä–æ–π –ª—É—á—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
            if len(proba_raw) < 2:
                confidence = float(proba_raw.max()) if len(proba_raw) == 1 else 0.0
            else:
                sorted_probas = np.sort(proba_raw)
                confidence = float(sorted_probas[-1] - sorted_probas[-2])

            hint = get_confidence_hint(confidence)

            try:
                predicted_delta = model_delta.predict(X_live)[0]
            except Exception as e:
                 logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ model_delta.predict –¥–ª—è {symbol} {tf}: {e}. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é predicted_delta –≤ NaN.")
                 predicted_delta = np.nan

            try:
                predicted_volatility = model_vol.predict(X_live)[0]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞
                if predicted_volatility < 0:
                    logging.warning(f"‚ö†Ô∏è –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞ ({predicted_volatility:.6f}) –¥–ª—è {symbol} {tf}. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ 0.")
                    predicted_volatility = 0.0
            except Exception as e:
                 logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ model_vol.predict –¥–ª—è {symbol} {tf}: {e}. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é predicted_volatility –≤ NaN.")
                 predicted_volatility = np.nan


            # TP-hit probability
            tp_hit_proba = np.nan
            if model_tp_hit:
                try:
                    tp_hit_proba_all_classes = model_tp_hit.predict_proba(X_live)[0]
                    model_tp_classes = getattr(model_tp_hit, 'classes_', None)

                    # –ò—â–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ '1' (—É—Å–ø–µ—Ö TP)
                    if model_tp_classes is not None and 1 in model_tp_classes:
                        try:
                            class_1_idx = list(model_tp_classes).index(1)
                            tp_hit_proba = tp_hit_proba_all_classes[class_1_idx]
                        except ValueError:
                            # –≠—Ç–æ–≥–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏–∑–æ–π—Ç–∏, –µ—Å–ª–∏ 1 –≤ model_tp_classes, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
                            logging.error(f"‚ùå –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: –∫–ª–∞—Å—Å '1' –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –∏–Ω–¥–µ–∫—Å—É, —Ö–æ—Ç—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ model_tp_hit.classes_ ({model_tp_classes}) –¥–ª—è {symbol} {tf}. TP Hit% –±—É–¥–µ—Ç NaN.")
                            tp_hit_proba = np.nan # –û—Å—Ç–∞–µ—Ç—Å—è NaN
                    elif len(tp_hit_proba_all_classes) > 1:
                         # Fallback: –ï—Å–ª–∏ classes_ –Ω–µ—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 1, –Ω–æ predict_proba –≤–µ—Ä–Ω—É–ª >1 –∑–Ω–∞—á–µ–Ω–∏–µ,
                         # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –∫–ª–∞—Å—Å 1 –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∏–Ω–¥–µ–∫—Å—É 1
                         logging.warning(f"‚ö†Ô∏è –ö–ª–∞—Å—Å '1' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ model_tp_hit.classes_ ({model_tp_classes if model_tp_classes is not None else 'N/A'}) –¥–ª—è {symbol} {tf}. –ò—Å–ø–æ–ª—å–∑—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ –∏–Ω–¥–µ–∫—Å—É 1.")
                         tp_hit_proba = tp_hit_proba_all_classes[1] # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∏–Ω–¥–µ–∫—Å 1 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–ª–∞—Å—Å—É 1
                    else:
                         logging.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å TP-hit –¥–ª—è {symbol} {tf} –Ω–µ –∏–º–µ–µ—Ç –∫–ª–∞—Å—Å–∞ '1' –∏ predict_proba –≤–µ—Ä–Ω—É–ª–∞ {len(tp_hit_proba_all_classes)} –∑–Ω–∞—á–µ–Ω–∏–π. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å tp_hit_proba.")
                         # tp_hit_proba –æ—Å—Ç–∞–µ—Ç—Å—è np.nan

                except Exception as e:
                    logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ model_tp_hit.predict_proba –¥–ª—è {symbol} {tf}: {e}. TP Hit% –±—É–¥–µ—Ç NaN.")
                    # tp_hit_proba –æ—Å—Ç–∞–µ—Ç—Å—è np.nan


            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º—É —Å–∏–≥–Ω–∞–ª—É
            direction = 'long' if signal in ['UP', 'STRONG UP'] else 'short' if signal in ['DOWN',
                                                                                           'STRONG DOWN'] else 'none'
            ts_obj = pd.to_datetime(row_df['timestamp'].values[0])
            ts_str_log = ts_obj.strftime('%Y-%m-%d %H:%M:%S')
            ts_str_display = ts_obj.strftime('%Y-%m-%d %H:%M')

            entry_price = float(row_df['close'].values[0])

            # –†–∞—Å—á–µ—Ç SL/TP —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            sl, tp = calculate_trade_levels(entry_price, direction, predicted_volatility)

            # –†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –¥–µ–ª—å—Ç—ã –∏ —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞
            delta_final = compute_final_delta(predicted_delta, avg_delta_similar, std_delta_similar)
            signal_strength_val = get_signal_strength(delta_final, confidence, std_delta_similar)
            conflict_flag = is_conflict(predicted_delta, avg_delta_similar)

            # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Ä—è–¥–∫–µ TARGET_CLASS_NAMES –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            # –ï—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –∫–ª–∞—Å—Å—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –º–æ–¥–µ–ª–∏, –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç 0
            proba_dict_ordered = {cls_name: proba_dict_model_order.get(cls_name, 0.0) for cls_name in TARGET_CLASS_NAMES}


            prediction_entry = {
                'symbol': symbol, 'tf': tf, 'timestamp_obj': ts_obj,
                'timestamp_str_log': ts_str_log, 'timestamp_str_display': ts_str_display,
                'signal': signal, 'confidence_score': confidence, 'confidence_hint': hint,
                # 'proba_dict': proba_dict_model_order, # –ò—Å–ø–æ–ª—å–∑—É–µ–º proba_dict_ordered –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞
                'proba_dict': proba_dict_ordered,
                'predicted_delta': predicted_delta,
                'predicted_volatility': predicted_volatility, 'entry': entry_price,
                'sl': sl, 'tp': tp, 'direction': direction,
                'avg_delta_similar': avg_delta_similar, 'std_delta_similar': std_delta_similar,
                'similarity_hint': similarity_hint,
                'delta_final': delta_final, 'signal_strength': signal_strength_val, 'conflict': conflict_flag,
                'tp_hit_proba': tp_hit_proba,
                'error': None # –ü–æ–ª–µ –¥–ª—è –±—É–¥—É—â–∏—Ö –æ—à–∏–±–æ–∫ –ø–æ —Å–∏–º–≤–æ–ª—É/–¢–§
            }
            all_predictions_data.append(prediction_entry)

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞
            # –£—Å–ª–æ–≤–∏–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ —Ç–æ—Ä–≥–æ–≤—ã–π –ø–ª–∞–Ω: –Ω–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞,
            # –∏ TP-hit –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ª–∏–±–æ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –ª–∏–±–æ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞.
            TRADE_PLAN_CONFIDENCE_THRESHOLD = 0.08 # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –ø–ª–∞–Ω–∞
            TRADE_PLAN_TP_HIT_THRESHOLD = 0.55 # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è TP Hit% –¥–ª—è –ø–ª–∞–Ω–∞ (–µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞)

            if direction != 'none' and confidence >= TRADE_PLAN_CONFIDENCE_THRESHOLD:
                 if pd.isna(prediction_entry['tp_hit_proba']) or prediction_entry['tp_hit_proba'] >= TRADE_PLAN_TP_HIT_THRESHOLD:
                     rr_value = np.nan # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∫ NaN
                     if not pd.isna(sl) and not pd.isna(tp):
                         # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ SL –Ω–µ —Ä–∞–≤–µ–Ω Entry, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                         if abs(entry_price - sl) > 1e-9:
                             rr_value = round(abs(tp - entry_price) / abs(entry_price - sl), 2)
                         else:
                             logging.warning(f"‚ö†Ô∏è SL —Ä–∞–≤–µ–Ω —Ü–µ–Ω–µ –≤—Ö–æ–¥–∞ –¥–ª—è {symbol} {tf}. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å RR.")


                     trade_plan.append({
                         'symbol': symbol, 'tf': tf, 'entry': entry_price, 'direction': direction,
                         'sl': sl, 'tp': tp, 'rr': rr_value,
                         'confidence': confidence, 'signal': signal, 'timestamp': ts_str_log, 'hint': hint,
                         'tp_hit_proba': tp_hit_proba
                     })
                 else:
                     logging.debug(f"–ü—Ä–æ–ø—É—Å–∫ {symbol} {tf} –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞: TP Hit ({prediction_entry['tp_hit_proba']:.1%}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ {TRADE_PLAN_TP_HIT_THRESHOLD:.1%}.")
            else:
                 logging.debug(f"–ü—Ä–æ–ø—É—Å–∫ {symbol} {tf} –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞: –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ {direction} –∏–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ({confidence:.3f}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ {TRADE_PLAN_CONFIDENCE_THRESHOLD:.3f}.")


    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if save_output_flag and all_predictions_data:
        logging.info("üíæ  –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        df_out_list = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TARGET_CLASS_NAMES –∫–∞–∫ –±–∞–∑–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        proba_dict_keys_order = TARGET_CLASS_NAMES

        for r_item in all_predictions_data:
            item = {
                'symbol': r_item['symbol'], 'tf': r_item['tf'],
                'timestamp': r_item['timestamp_obj'].strftime('%Y-%m-%d %H:%M:%S'),
                'signal': r_item['signal'], 'confidence_score': r_item['confidence_score'],
                'tp_hit_proba': r_item['tp_hit_proba'],
                'predicted_delta': r_item['predicted_delta'],
                'predicted_volatility': r_item['predicted_volatility'],
                'avg_delta_similar': r_item['avg_delta_similar'],
                'std_delta_similar': r_item['std_delta_similar'],
                'delta_final': r_item['delta_final'],
                'entry': r_item['entry'], 'sl': r_item['sl'], 'tp': r_item['tp'],
                'direction': r_item['direction'],
                'signal_strength': r_item['signal_strength'], 'conflict': r_item['conflict'],
                'confidence_hint': r_item['confidence_hint'],
                'similarity_hint': r_item['similarity_hint'],
            }
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–∏ –∏–∑ proba_dict_ordered,
            # –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å TARGET_CLASS_NAMES
            item.update(r_item['proba_dict'])

            df_out_list.append(item)

        df_out = pd.DataFrame(df_out_list)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∑–∞—Ç–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤, –∑–∞—Ç–µ–º –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è
        csv_columns_order = [
            'symbol', 'tf', 'timestamp', 'signal', 'confidence_score', 'tp_hit_proba',
            'predicted_delta', 'avg_delta_similar', 'std_delta_similar', 'delta_final',
            'predicted_volatility', 'entry', 'sl', 'tp', 'direction',
            'signal_strength', 'conflict', 'confidence_hint', 'similarity_hint'
        ]
        csv_columns_order.extend(proba_dict_keys_order) # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–∏ —Ç–∏–ø–∞ 'STRONG DOWN', 'DOWN' –∏ —Ç.–¥.

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ df_out –≤–∫–ª—é—á–µ–Ω—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ csv_columns_order
        final_csv_columns = [col for col in csv_columns_order if col in df_out.columns]
        for col in df_out.columns:
            if col not in final_csv_columns:
                final_csv_columns.append(col)

        if not df_out.empty:
            try:
                # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                df_out = df_out[final_csv_columns]
                df_out.to_csv(LATEST_PREDICTIONS_FILE, index=False, float_format='%.6f')
                logging.info(f"üìÑ  –°–∏–≥–Ω–∞–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {LATEST_PREDICTIONS_FILE}")
            except Exception as e:
                logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {LATEST_PREDICTIONS_FILE}: {e}")
        else:
            logging.info("ü§∑ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ LATEST_PREDICTIONS_FILE.")

        if trade_plan:
            df_trade_plan = pd.DataFrame(trade_plan)
            trade_plan_cols_order = [
                'symbol', 'tf', 'timestamp', 'direction', 'signal', 'confidence', 'tp_hit_proba',
                'entry', 'sl', 'tp', 'rr', 'hint'
            ]
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ df_trade_plan –≤–∫–ª—é—á–µ–Ω—ã
            final_trade_plan_cols = [col for col in trade_plan_cols_order if col in df_trade_plan.columns]
            for col in df_trade_plan.columns:
                if col not in final_trade_plan_cols:
                    final_trade_plan_cols.append(col)

            if not df_trade_plan.empty:
                try:
                    # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                    df_trade_plan = df_trade_plan[final_trade_plan_cols]
                    df_trade_plan.to_csv(TRADE_PLAN_FILE, index=False, float_format='%.6f')
                    logging.info(f"üìà  –¢–æ—Ä–≥–æ–≤—ã–π –ø–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {TRADE_PLAN_FILE}")
                except Exception as e:
                    logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {TRADE_PLAN_FILE}: {e}")
            else:
                logging.info("ü§∑ –¢–æ—Ä–≥–æ–≤—ã–π –ø–ª–∞–Ω –ø—É—Å—Ç, —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω.")
        else:
            logging.info("ü§∑ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞.")

    # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    if all_predictions_data:
        headers_tabulate = ["TF", "Timestamp", "–°–∏–≥–Ω–∞–ª", "Conf.", "ŒîModel", "ŒîHist", "œÉHist", "ŒîFinal", "–ö–æ–Ω—Ñ?", "–°–∏–ª–∞",
                            "TP Hit%"]
        grouped_by_symbol = {}
        for row_data_item in all_predictions_data:
            grouped_by_symbol.setdefault(row_data_item['symbol'], []).append(row_data_item)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏–º–≤–æ–ª—ã –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        sorted_symbols_for_display = sorted(grouped_by_symbol.keys())

        for symbol_key in sorted_symbols_for_display:
            rows_list = grouped_by_symbol[symbol_key]
            try:
                # Sort by TIMEFRAMES order, then by confidence score descending
                sorted_rows = sorted(rows_list,
                                     key=lambda r_item_sort: (TIMEFRAMES.index(r_item_sort['tf'])
                                                              if r_item_sort['tf'] in TIMEFRAMES else float('inf'),
                                                              # Handle TFs not in TIMEFRAMES gracefully
                                                              -r_item_sort['confidence_score']))
            except ValueError:
                logging.warning(
                    f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è {symbol_key}, –≤–æ–∑–º–æ–∂–Ω–æ TF –Ω–µ –≤ —Å–ø–∏—Å–∫–µ TIMEFRAMES. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.")
                sorted_rows = sorted(rows_list, key=lambda r_item_sort: (-r_item_sort['confidence_score']))

            table_data_tabulate = []
            for r_tab in sorted_rows:
                table_data_tabulate.append([
                    r_tab['tf'],
                    r_tab['timestamp_str_display'],
                    r_tab['signal'],
                    f"{r_tab['confidence_score']:.3f}",
                    f"{r_tab['predicted_delta']:.2%}" if pd.notna(r_tab['predicted_delta']) else "N/A",
                    f"{r_tab['avg_delta_similar']:.2%}" if pd.notna(r_tab['avg_delta_similar']) else "N/A",
                    f"{r_tab['std_delta_similar']:.2%}" if pd.notna(r_tab['std_delta_similar']) else "N/A",
                    f"{r_tab['delta_final']:.2%}" if pd.notna(r_tab['delta_final']) else "N/A",
                    "‚ùó" if r_tab['conflict'] else " ",
                    r_tab['signal_strength'].replace(" –°–∏–ª—å–Ω—ã–π", "üü¢").replace(" –£–º–µ—Ä–µ–Ω–Ω—ã–π", "üü°").replace(" –°–ª–∞–±—ã–π",
                                                                                                         "‚ö™"),
                    f"{r_tab['tp_hit_proba']:.1%}" if pd.notna(r_tab['tp_hit_proba']) else "N/A"
                ])
            print(f"\nüìä  –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ —Å–∏–º–≤–æ–ª—É: {symbol_key}")
            try:
                print(tabulate(table_data_tabulate, headers=headers_tabulate, tablefmt="pretty", stralign="right",
                               numalign="right"))
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è {symbol_key}: {e}")
                # –ü–æ–ø—Ä–æ–±—É–µ–º –≤—ã–≤–µ—Å—Ç–∏ –ø—Ä–æ—Å—Ç–æ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ tabulate –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                print("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É. –î–∞–Ω–Ω—ã–µ:")
                for row in table_data_tabulate:
                    print(row)


    logging.info("‚úÖ  –ü—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω.")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")
    parser.add_argument('--save', action='store_true', help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤ CSV —Ñ–∞–π–ª—ã.")
    # >>> –î–û–ë–ê–í–õ–ï–ù–û —Å–æ–≥–ª–∞—Å–Ω–æ –ø–∞—Ç—á—É
    parser.add_argument('--symbol',       type=str, help="–û–¥–∏–Ω —Å–∏–º–≤–æ–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –Ω–∞–ø—Ä. BTCUSDT")
    parser.add_argument('--symbol-group', type=str, help="–ì—Ä—É–ø–ø–∞ —Å–∏–º–≤–æ–ª–æ–≤, –Ω–∞–ø—Ä. top8 –∏–ª–∏ meme")
    args = parser.parse_args()

    # >>> –ò–ó–ú–ï–ù–ï–ù–û: –ü–µ—Ä–µ–¥–∞—á–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –≤ predict_all_tf
    # –í–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –ø–µ—Ä–µ–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã —è–≤–Ω–æ –≤ —Ñ—É–Ω–∫—Ü–∏—é
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–∏ —É–∫–∞–∑–∞–Ω—ã –æ–±–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        if args.symbol and args.symbol_group:
            logging.error("‚ùå –ù–µ–ª—å–∑—è —É–∫–∞–∑—ã–≤–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ --symbol –∏ --symbol-group.")
            sys.exit(1)

        predict_all_tf(args.save, symbol_filter=args.symbol, group_filter=args.symbol_group)

    except KeyboardInterrupt:
        print("\n[PredictAll] üõë –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C).")
        sys.exit(130)
    except Exception as e:
        logging.error(f"[PredictAll] üí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)