# --- START OF FILE train_model.py ---

import pandas as pd
import numpy as np
import argparse
import os
from catboost import CatBoostClassifier, CatBoostRegressor, Pool, cv
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import classification_report, mean_absolute_error, accuracy_score, recall_score
from sklearn.metrics import f1_score, precision_score, confusion_matrix, roc_auc_score, average_precision_score
import joblib
import sys
import logging
import random
import json
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

import optuna
from sklearn.calibration import CalibratedClassifierCV
from sklearn.inspection import permutation_importance

SYMBOL_GROUPS = {
    "top15": [
        "ETHUSDT", "SOLUSDT", "BNBUSDT", "BCHUSDT", "LTCUSDT",
        "BTCUSDT", "ADAUSDT", "AVAXUSDT", "DOTUSDT", "XRPUSDT",
        "MATICUSDT", "LINKUSDT", "NEARUSDT", "ATOMUSDT", "TRXUSDT"
    ],
    "meme": [
        "DOGEUSDT", "PEPEUSDT", "FLOKIUSDT", "WIFUSDT", "SHIBUSDT"
    ],
    "top8": [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT",
        "XRPUSDT", "ADAUSDT", "LINKUSDT", "AVAXUSDT"
    ],
    "top3": ["BTCUSDT", "ETHUSDT", "SOLUSDT"],
    "top5": ["ETHUSDT", "SOLUSDT", "BNBUSDT", "BCHUSDT", "LTCUSDT"]
}

FEATURE_BLACKLIST = {
    "long": [],
    "short": [],
    "tp_hit": []
}

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s [TrainModel] - %(message)s',
                    stream=sys.stdout)

os.makedirs("models", exist_ok=True)
os.makedirs("logs", exist_ok=True)

N_CV_SPLITS = 5
DEFAULT_N_TOP_FEATURES = 20
DEFAULT_OPTUNA_TRIALS_CLASSIFIER = 30
DEFAULT_OPTUNA_TRIALS_REGRESSOR = 20
DEFAULT_MINORITY_WEIGHT_BOOST = 1.0


def find_optimal_threshold(model, X_val, y_val, pos_label=1):
    if X_val.empty or y_val.empty or y_val.nunique() < 2:
        logging.warning("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ—Ä–æ–≥: –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–¥–∏–Ω –∫–ª–∞—Å—Å.")
        return 0.5
    try:
        model_classes_list = list(model.classes_)
        if pos_label not in model_classes_list:
            logging.error(f"–ö–ª–∞—Å—Å {pos_label} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ model.classes_ ({model_classes_list}) –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –ø–æ—Ä–æ–≥–∞.")
            return 0.5
        proba_col_idx = model_classes_list.index(pos_label)
        y_pred_proba = model.predict_proba(X_val)[:, proba_col_idx]
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ predict_proba –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –ø–æ—Ä–æ–≥–∞: {e}")
        return 0.5
    best_threshold = 0.5
    best_f1 = 0.0
    thresholds = np.arange(0.05, 0.96, 0.01)
    for threshold in thresholds:
        y_pred = (y_pred_proba >= threshold).astype(int)
        current_f1 = f1_score(y_val, y_pred, pos_label=pos_label, zero_division=0)
        if current_f1 > best_f1:
            best_f1 = current_f1
            best_threshold = threshold
    logging.info(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞–π–¥–µ–Ω: {best_threshold:.2f} —Å F1-score: {best_f1:.4f}")
    return best_threshold


class EarlyStoppingCallback:
    def __init__(self, patience=10, min_delta=0.0005, direction='maximize'):  # –î–æ–±–∞–≤–ª–µ–Ω direction
        self.patience = patience
        self.min_delta = min_delta
        self.wait = 0
        self.direction = direction
        if self.direction == 'maximize':
            self.best_value = -float('inf')
        else:  # minimize
            self.best_value = float('inf')

    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial):
        current_value = study.best_value
        if current_value is None: return

        improved = False
        if self.direction == 'maximize':
            if current_value > self.best_value + self.min_delta:
                improved = True
        else:  # minimize
            if current_value < self.best_value - self.min_delta:
                improved = True

        if improved:
            self.best_value = current_value
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                logging.info(f"Optuna HPO –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –¥–æ—Å—Ä–æ—á–Ω–æ: –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –∑–∞ {self.patience} –ø–æ–ø—ã—Ç–æ–∫.")
                study.stop()


def _train_binary_classifier(
        X_cv_pool_full: pd.DataFrame, y_cv_pool_target: pd.Series,
        X_test_full: pd.DataFrame,
        y_test_target: pd.Series,
        target_name: str, log_context_str: str, file_prefix: str,
        n_top_features: int,
        n_optuna_trials: int,
        minority_weight_boost_factor: float,
        apply_smote_default: bool = True
):
    model_log_prefix = f"clf_{target_name}"
    logging.info(f"\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {model_log_prefix} –¥–ª—è {log_context_str}")

    if y_cv_pool_target.nunique() < 2:
        logging.error(f"–¶–µ–ª—å '{target_name}' < 2 —É–Ω–∏–∫. –∫–ª–∞—Å—Å–æ–≤ –≤ CV –ø—É–ª–µ. –û–±—É—á–µ–Ω–∏–µ {model_log_prefix} –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        return None, [], 0.5

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è X_val_for_threshold —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ –∏–∑ X_cv_pool_full
    X_optuna_train_pool, y_optuna_train_pool = X_cv_pool_full.copy(), y_cv_pool_target.copy()  # –†–∞–±–æ—Ç–∞–µ–º —Å –∫–æ–ø–∏—è–º–∏
    X_val_for_threshold, y_val_for_threshold = pd.DataFrame(
        columns=X_cv_pool_full.columns if not X_cv_pool_full.empty else None), pd.Series(dtype='int')

    min_samples_for_val_split = N_CV_SPLITS * 20
    if len(X_cv_pool_full) > min_samples_for_val_split + (N_CV_SPLITS * 2):  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        val_threshold_size = int(len(X_cv_pool_full) * 0.20)
        val_threshold_size = max(N_CV_SPLITS * 2, val_threshold_size)
        val_threshold_size = min(val_threshold_size, len(X_cv_pool_full) - min_samples_for_val_split)

        if val_threshold_size > 0 and (len(X_cv_pool_full) - val_threshold_size) >= min_samples_for_val_split:
            split_idx_cv = len(X_cv_pool_full) - val_threshold_size
            X_optuna_train_pool = X_cv_pool_full.iloc[:split_idx_cv].copy()
            y_optuna_train_pool = y_cv_pool_target.iloc[:split_idx_cv].copy()
            X_val_for_threshold = X_cv_pool_full.iloc[split_idx_cv:].copy()
            y_val_for_threshold = y_cv_pool_target.iloc[split_idx_cv:].copy()
            logging.info(
                f"CV –ø—É–ª —Ä–∞–∑–¥–µ–ª–µ–Ω: Optuna/Train: {len(X_optuna_train_pool)}, –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞/–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {len(X_val_for_threshold)}")

            if X_val_for_threshold.empty or y_val_for_threshold.empty or y_val_for_threshold.nunique() < 2:
                logging.warning(
                    "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –ø–æ—Ä–æ–≥–∞/–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω (–ø—É—Å—Ç –∏–ª–∏ 1 –∫–ª–∞—Å—Å). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Å—å CV –ø—É–ª –¥–ª—è Optuna/Train.")
                X_optuna_train_pool, y_optuna_train_pool = X_cv_pool_full.copy(), y_cv_pool_target.copy()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –¥–ª—è Optuna
                X_val_for_threshold, y_val_for_threshold = pd.DataFrame(columns=X_cv_pool_full.columns), pd.Series(
                    dtype='int')  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º val –Ω–∞–±–æ—Ä
        else:
            logging.warning(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–¥–µ–ª–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ä–∞—Å—á–µ—Ç–∞ val_threshold_size). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Å—å CV –ø—É–ª –¥–ª—è Optuna/Train.")
    else:
        logging.warning(
            "CV –ø—É–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Ä–æ–≥–∞/–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Å—å CV –ø—É–ª –¥–ª—è Optuna/Train.")

    selected_features = []
    # ... (–∫–æ–¥ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–∞–∫ –±—ã–ª, —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏–µ–º –≤ —Ü–∏–∫–ª–µ)
    if X_optuna_train_pool.empty or y_optuna_train_pool.empty or len(X_optuna_train_pool.columns) == 0:
        logging.warning(f"X_optuna_train_pool –ø—É—Å—Ç –∏–ª–∏ –Ω–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {target_name}. –ü—Ä–æ–ø—É—Å–∫ –æ—Ç–±–æ—Ä–∞.")
        if not X_cv_pool_full.empty and len(X_cv_pool_full.columns) > 0:
            selected_features = X_cv_pool_full.columns.tolist()
        else:
            return None, [], 0.5
    elif len(X_optuna_train_pool.columns) <= n_top_features:
        selected_features = X_optuna_train_pool.columns.tolist()
        logging.info(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ ({len(selected_features)}) <= {n_top_features}. –û—Ç–±–æ—Ä –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
    else:
        tscv_fs = TimeSeriesSplit(n_splits=N_CV_SPLITS)
        feature_importances_fi = np.zeros(X_optuna_train_pool.shape[1])
        num_successful_folds = 0
        logging.info(
            f"üöÄ –û—Ç–±–æ—Ä –¢–æ–ø-{n_top_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_log_prefix} –Ω–∞ {len(X_optuna_train_pool)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")
        for fold, (train_idx, val_idx) in enumerate(tscv_fs.split(X_optuna_train_pool, y_optuna_train_pool)):
            X_f, y_f = X_optuna_train_pool.iloc[train_idx], y_optuna_train_pool.iloc[train_idx]
            X_v, y_v = X_optuna_train_pool.iloc[val_idx], y_optuna_train_pool.iloc[val_idx]
            if X_f.empty or y_f.empty or X_v.empty or y_v.empty or \
                    y_f.nunique() < 2 or y_v.nunique() < 2:  # –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –£–°–õ–û–í–ò–ï
                logging.warning(
                    f"–§–æ–ª–¥ {fold + 1} –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Ñ–æ–ª–¥–∞.")
                continue
            clf_fi = CatBoostClassifier(iterations=200, learning_rate=0.05, early_stopping_rounds=25,
                                        loss_function='Logloss', eval_metric='F1', task_type="GPU", devices='0',
                                        random_seed=42, verbose=0)
            try:
                clf_fi.fit(X_f, y_f, eval_set=(X_v, y_v))
                feature_importances_fi += clf_fi.get_feature_importance(type='FeatureImportance')
                num_successful_folds += 1
            except Exception as e:
                logging.warning(f"–û—à–∏–±–∫–∞ –≤ —Ñ–æ–ª–¥–µ {fold + 1} –æ—Ç–±–æ—Ä–∞ {target_name}: {e}")
        if num_successful_folds > 0:
            fi_mean = pd.Series(feature_importances_fi / num_successful_folds,
                                index=X_optuna_train_pool.columns).sort_values(ascending=False)
            selected_features = fi_mean.head(n_top_features).index.tolist()
            logging.info(f"–¢–æ–ø-{n_top_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_log_prefix}: {selected_features}")
        else:
            selected_features = X_optuna_train_pool.columns.tolist()
            logging.warning(
                f"–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {target_name} –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í—Å–µ ({len(selected_features)}) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è.")

    if target_name in FEATURE_BLACKLIST:  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
        # ... (–∫–æ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è FEATURE_BLACKLIST, –∫–∞–∫ –±—ã–ª)
        current_blacklist = FEATURE_BLACKLIST[target_name]
        if current_blacklist:
            original_len = len(selected_features)
            selected_features_before_blacklist = selected_features.copy()
            selected_features = [f for f in selected_features if f not in current_blacklist]
            if len(selected_features) < original_len:
                logging.info(
                    f"–ò—Å–∫–ª—é—á–µ–Ω–æ –ø–æ —á–µ—Ä–Ω–æ–º—É —Å–ø–∏—Å–∫—É –¥–ª—è {model_log_prefix}: {original_len - len(selected_features)} —à—Ç. –û—Å—Ç–∞–ª–æ—Å—å: {len(selected_features)}")
            if not selected_features and original_len > 0:
                logging.warning(
                    f"–ü–æ—Å–ª–µ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –¥–ª—è {model_log_prefix} –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–±–æ—Ä.")
                selected_features = selected_features_before_blacklist

    if not selected_features: return None, [], 0.5
    X_optuna_train_sel = X_optuna_train_pool[selected_features]
    X_val_for_threshold_sel = X_val_for_threshold[selected_features] if not X_val_for_threshold.empty else pd.DataFrame(
        columns=selected_features)

    # ... (–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) ...
    try:
        features_list_path = f"models/{file_prefix}_features_{target_name}_selected.txt"
        with open(features_list_path, "w", encoding="utf-8") as f:
            json.dump(selected_features, f, indent=2)
        logging.info(f"–û—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è {model_log_prefix} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {features_list_path}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    # ... (Optuna HPO, –∫–∞–∫ –±—ã–ª, —Å –ø–µ—Ä–µ–¥–∞—á–µ–π n_optuna_trials –∏ minority_weight_boost_factor –≤ objective) ...
    logging.info(f"\nüîÑ –ù–∞—á–∞–ª–æ Optuna HPO –¥–ª—è {model_log_prefix} ({log_context_str}) –Ω–∞ {n_optuna_trials} –ø–æ–ø—ã—Ç–æ–∫...")
    trial_counter = 0

    def objective(trial):
        nonlocal trial_counter;
        trial_counter += 1
        logging.info(f"Optuna HPO –¥–ª—è {model_log_prefix}: –ü–æ–ø—ã—Ç–∫–∞ {trial_counter}/{n_optuna_trials}")
        current_y_train_for_weights = y_optuna_train_pool
        counts_trial = current_y_train_for_weights.value_counts().to_dict()
        total_trial, num_classes_trial = sum(counts_trial.values()), len(counts_trial) if len(counts_trial) > 0 else 1
        class_weights_objective = {}
        minority_label_obj = 1 if counts_trial.get(1, 0) < counts_trial.get(0, 0) else (
            0 if counts_trial.get(0, 0) < counts_trial.get(1, 0) else -1)
        for cls_label, count_val in counts_trial.items():
            base_weight = total_trial / (num_classes_trial * count_val) if count_val > 0 else 1.0
            class_weights_objective[
                cls_label] = base_weight * minority_weight_boost_factor if cls_label == minority_label_obj and minority_label_obj != -1 else base_weight
        if 0 not in class_weights_objective: class_weights_objective[0] = 1.0 * (
            minority_weight_boost_factor if 0 == minority_label_obj and minority_label_obj != -1 else 1.0)
        if 1 not in class_weights_objective: class_weights_objective[1] = 1.0 * (
            minority_weight_boost_factor if 1 == minority_label_obj and minority_label_obj != -1 else 1.0)
        class_weights_list_objective = [class_weights_objective.get(0, 1.0), class_weights_objective.get(1, 1.0)]
        params = {'iterations': trial.suggest_int('iterations', 200, 1500, step=100),
                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
                  'depth': trial.suggest_int('depth', 4, 8),
                  'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),  # –û–≥—Ä–∞–Ω–∏—á–∏–ª depth
                  'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
                  'random_strength': trial.suggest_float('random_strength', 1e-9, 1.0, log=True),
                  'border_count': trial.suggest_categorical('border_count', [32, 64, 128]),
                  # 'rsm': trial.suggest_float('rsm', 0.6, 0.95), # <--- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–æ
                  'loss_function': 'Logloss', 'eval_metric': 'F1', 'early_stopping_rounds': 50, 'random_seed': 42,
                  'task_type': 'GPU', 'devices': '0', 'verbose': 0,
                  'class_weights': class_weights_list_objective}
        tscv_optuna = TimeSeriesSplit(n_splits=N_CV_SPLITS)
        try:
            cv_results = cv(Pool(X_optuna_train_sel, y_optuna_train_pool.astype(int),
                                 feature_names=list(X_optuna_train_sel.columns)), params, folds=tscv_optuna, plot=False)
            mean_f1 = cv_results[f'test-F1-mean'].iloc[-1]
            logging.info(f"  Optuna –ü–æ–ø—ã—Ç–∫–∞ {trial_counter}: F1={mean_f1:.4f}, –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {trial.params}")
            return mean_f1
        except Exception as e_cv:
            logging.error(
                f"  –û—à–∏–±–∫–∞ –≤ Optuna –ü–æ–ø—ã—Ç–∫–µ {trial_counter} –≤–æ –≤—Ä–µ–º—è CV: {e_cv}\n  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {trial.params}"); return -1.0

    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
    early_stopping_cb = EarlyStoppingCallback(patience=10, min_delta=0.0005)
    try:
        study.optimize(objective, n_trials=n_optuna_trials, n_jobs=1, callbacks=[early_stopping_cb])
        best_params_optuna = study.best_params
        logging.info(
            f"üéØ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Optuna –¥–ª—è {model_log_prefix}: {best_params_optuna}, F1_cv_score: {study.best_value:.4f}")
    except Exception as e:
        logging.warning(f"Optuna –¥–ª—è {model_log_prefix} —Å –æ—à–∏–±–∫–æ–π: {e}. –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã."); best_params_optuna = {}
    final_model_params = {'loss_function': 'Logloss', 'eval_metric': 'F1', 'early_stopping_rounds': 50,
                          'random_seed': 42, 'task_type': 'GPU', 'devices': '0', 'verbose': 100}
    final_model_params.update(best_params_optuna)
    counts_final_train = y_cv_pool_target.value_counts().to_dict()
    total_final_train, num_classes_final_train = sum(counts_final_train.values()), len(counts_final_train) if len(
        counts_final_train) > 0 else 1
    class_weights_final_fit = {}
    minority_label_final = 1 if counts_final_train.get(1, 0) < counts_final_train.get(0, 0) else (
        0 if counts_final_train.get(0, 0) < counts_final_train.get(1, 0) else -1)
    for cls_label, count_val in counts_final_train.items():
        base_weight = total_final_train / (num_classes_final_train * count_val) if count_val > 0 else 1.0
        class_weights_final_fit[
            cls_label] = base_weight * minority_weight_boost_factor if cls_label == minority_label_final and minority_label_final != -1 else base_weight
    if 0 not in class_weights_final_fit: class_weights_final_fit[0] = 1.0 * (
        minority_weight_boost_factor if 0 == minority_label_final and minority_label_final != -1 else 1.0)
    if 1 not in class_weights_final_fit: class_weights_final_fit[1] = 1.0 * (
        minority_weight_boost_factor if 1 == minority_label_final and minority_label_final != -1 else 1.0)
    final_model_params['class_weights'] = class_weights_final_fit
    logging.info(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {model_log_prefix} (fit): {class_weights_final_fit}")

    X_train_for_final_model_sel = X_cv_pool_full[selected_features]
    y_train_for_final_model_target = y_cv_pool_target
    apply_smote_final = apply_smote_default
    if target_name == "tp_hit":  # ... (–ª–æ–≥–∏–∫–∞ SMOTE –¥–ª—è tp_hit)
        counts_tp_hit_train = y_train_for_final_model_target.value_counts()
        if len(counts_tp_hit_train) == 2 and counts_tp_hit_train.get(1, 0) >= counts_tp_hit_train.get(0, 0):
            logging.info(
                f"–î–ª—è {model_log_prefix} –∫–ª–∞—Å—Å 1 –Ω–µ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π ({counts_tp_hit_train.to_dict()}). SMOTE –ø—Ä–æ–ø—É—â–µ–Ω.")
            apply_smote_final = False
    if apply_smote_final:  # ... (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ SMOTE)
        min_class_count = y_train_for_final_model_target.value_counts().min()
        value_counts_before_smote = y_train_for_final_model_target.value_counts().to_dict()
        if y_train_for_final_model_target.nunique() < 2:
            logging.warning(f"–ü—Ä–æ–ø—É—Å–∫ SMOTE –¥–ª—è {target_name}: —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å.")
        elif min_class_count <= 1:
            logging.warning(
                f"–ü—Ä–æ–ø—É—Å–∫ SMOTE –¥–ª—è {target_name}: –≤ –º–∏–Ω–æ—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ <=1 –ø—Ä–∏–º–µ—Ä ({min_class_count}). –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {value_counts_before_smote}")
        else:
            k_val = max(1, min(4, min_class_count - 1))
            smote = SMOTE(random_state=42, k_neighbors=k_val)
            logging.info(
                f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è {model_log_prefix} —Å k_neighbors={k_val}. –î–æ: {value_counts_before_smote}")
            try:
                X_train_for_final_model_sel, y_train_for_final_model_target = smote.fit_resample(
                    X_train_for_final_model_sel, y_train_for_final_model_target)
                logging.info(
                    f"–ü–æ—Å–ª–µ SMOTE –¥–ª—è {target_name}: {pd.Series(y_train_for_final_model_target).value_counts().to_dict()}")
            except Exception as e:
                logging.warning(f"SMOTE –¥–ª—è {target_name} –Ω–µ —É–¥–∞–ª—Å—è: {e}. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö.")

    logging.info(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {model_log_prefix}...")
    final_model_uncalibrated = CatBoostClassifier(**final_model_params)
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ eval_set –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    can_use_eval_set_for_final_fit = not X_val_for_threshold_sel.empty and \
                                     not y_val_for_threshold.empty and \
                                     y_val_for_threshold.nunique() >= 2
    eval_set_for_final_fit = (X_val_for_threshold_sel,
                              y_val_for_threshold.astype(int)) if can_use_eval_set_for_final_fit else None
    if not can_use_eval_set_for_final_fit:
        logging.warning(
            f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω–æ–≥–æ eval_set –¥–ª—è early stopping —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {model_log_prefix}. Early stopping –æ—Ç–∫–ª—é—á–µ–Ω (–µ—Å–ª–∏ iterations –Ω–µ –∑–∞–¥–∞–Ω —è–≤–Ω–æ –∏ –≤–µ–ª–∏–∫).")
        # –ï—Å–ª–∏ iterations –Ω–µ –∑–∞–¥–∞–Ω –≤ best_params_optuna, CatBoost –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ 1000.
        # –ï—Å–ª–∏ iterations –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π, –∞ eval_set –Ω–µ—Ç, –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –¥–æ –∫–æ–Ω—Ü–∞.
        # final_model_params.pop('early_stopping_rounds', None) # –ú–æ–∂–Ω–æ —è–≤–Ω–æ —É–±—Ä–∞—Ç—å, –Ω–æ CatBoost —Å–∞–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç

    if not X_train_for_final_model_sel.empty and not y_train_for_final_model_target.empty:
        try:
            final_model_uncalibrated.fit(X_train_for_final_model_sel, y_train_for_final_model_target.astype(int),
                                         eval_set=eval_set_for_final_fit, plot=False)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {model_log_prefix}: {e}",
                          exc_info=True); return None, selected_features, 0.5
    else:
        logging.error(f"–û–±—É—á–µ–Ω–∏–µ {model_log_prefix} –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ: X –∏–ª–∏ y –ø—É—Å—Ç—ã."); return None, selected_features, 0.5

    final_model_calibrated = final_model_uncalibrated
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∏ —Å–∞–º –ø—Ä–æ—Ü–µ—Å—Å
    can_calibrate = not X_val_for_threshold_sel.empty and \
                    not y_val_for_threshold.empty and \
                    y_val_for_threshold.nunique() >= 2
    if target_name in ["tp_hit", "long", "short"] and can_calibrate:
        logging.info(
            f"–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è {model_log_prefix} –Ω–∞ X_val_for_threshold (–∫–ª–∞—Å—Å—ã: {y_val_for_threshold.value_counts().to_dict()})...")
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º sigmoid, –æ–Ω –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤
            calibrated_clf = CalibratedClassifierCV(final_model_uncalibrated, method='sigmoid', cv=None)
            calibrated_clf.fit(X_val_for_threshold_sel, y_val_for_threshold.astype(int))
            final_model_calibrated = calibrated_clf
            logging.info(f"–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ (sigmoid) –¥–ª—è {model_log_prefix} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        except ValueError as ve:  # –õ–æ–≤–∏–º ValueError, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å, –µ—Å–ª–∏ predict_proba –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –∫–ª–∞—Å—Å
            logging.warning(f"–û—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (sigmoid) –¥–ª—è {model_log_prefix}: {ve}. –ü–æ–ø—ã—Ç–∫–∞ —Å isotonic.")
            try:
                calibrated_clf_iso = CalibratedClassifierCV(final_model_uncalibrated, method='isotonic', cv=None)
                calibrated_clf_iso.fit(X_val_for_threshold_sel, y_val_for_threshold.astype(int))
                final_model_calibrated = calibrated_clf_iso
                logging.info(f"–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ (isotonic) –¥–ª—è {model_log_prefix} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            except Exception as e_iso:
                logging.warning(
                    f"–û—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (isotonic) –¥–ª—è {model_log_prefix}: {e_iso}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.")
        except Exception as e_cal:  # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
            logging.warning(
                f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –¥–ª—è {model_log_prefix}: {e_cal}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.")
    elif target_name in ["tp_hit", "long", "short"]:
        logging.warning(
            f"–ü—Ä–æ–ø—É—Å–∫ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –¥–ª—è {model_log_prefix}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (X_val_for_threshold_sel –ø—É—Å—Ç –∏–ª–∏ y_val_for_threshold –∏–º–µ–µ—Ç <2 –∫–ª–∞—Å—Å–æ–≤).")

    optimal_threshold = 0.5
    if can_calibrate:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ —É—Å–ª–æ–≤–∏–µ, —á—Ç–æ –∏ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
        optimal_threshold = find_optimal_threshold(final_model_calibrated, X_val_for_threshold_sel,
                                                   y_val_for_threshold.astype(int))
    else:
        logging.warning(
            f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è {model_log_prefix} (–Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 0.5.")

    # Permutation Importance (–∏—Å–ø–æ–ª—å–∑—É–µ–º X_test_full, –µ—Å–ª–∏ X_val_for_threshold_sel –Ω–µ–≤–∞–ª–∏–¥–µ–Ω)
    # ... (–∫–æ–¥ Permutation Importance, –∫–∞–∫ –±—ã–ª) ...
    perm_eval_data_valid = False
    if not X_val_for_threshold_sel.empty and not y_val_for_threshold.empty and y_val_for_threshold.nunique() >= 2:
        X_perm_eval, y_perm_eval = X_val_for_threshold_sel, y_val_for_threshold  # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π
        perm_eval_data_valid = True
        logging.info(f"Permutation Importance –±—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ X_val_for_threshold_sel –¥–ª—è {model_log_prefix}.")
    elif not X_test_full[selected_features].empty and not y_test_target.empty and y_test_target.nunique() >= 2:
        X_perm_eval, y_perm_eval = X_test_full[selected_features], y_test_target
        perm_eval_data_valid = True
        logging.warning(
            f"X_val_for_threshold_sel –Ω–µ–≤–∞–ª–∏–¥–µ–Ω, Permutation Importance –±—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ X_test_full –¥–ª—è {model_log_prefix}.")
    else:
        logging.warning(
            f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∏ –≤ X_val_for_threshold_sel, –Ω–∏ –≤ X_test_full –¥–ª—è Permutation Importance {model_log_prefix}. –ü—Ä–æ–ø—É—Å–∫.")

    if perm_eval_data_valid:
        logging.info(f"–†–∞—Å—á–µ—Ç Permutation Importance –¥–ª—è {model_log_prefix}...")
        try:
            perm_importance_result = permutation_importance(final_model_calibrated, X_perm_eval,
                                                            y_perm_eval.astype(int), n_repeats=10, random_state=42,
                                                            scoring='f1_weighted')
            sorted_idx = perm_importance_result.importances_mean.argsort()[::-1]
            perm_importance_df = pd.DataFrame({'feature': X_perm_eval.columns[sorted_idx],
                                               'importance_mean': perm_importance_result.importances_mean[sorted_idx],
                                               'importance_std': perm_importance_result.importances_std[sorted_idx]})
            logging.info(
                f"\nPermutation Importance –¥–ª—è {model_log_prefix} (—Ç–æ–ø-10):\n{perm_importance_df.head(10).to_string(index=False)}")
            perm_importance_path = f"logs/perm_importance_{file_prefix}_{target_name}.csv"
            perm_importance_df.to_csv(perm_importance_path, index=False)
            logging.info(f"Permutation Importance —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {perm_importance_path}")
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ Permutation Importance –¥–ª—è {model_log_prefix}: {e}")

    return final_model_calibrated, selected_features, optimal_threshold


# --- train_all_models ---
def train_all_models(tf_train, symbol=None, cli_args=None):
    # ... (–∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏ train_all_models, –∫–∞–∫ –±—ã–ª, —Å –ø–µ—Ä–µ–¥–∞—á–µ–π cli_args –≤ _train_binary_classifier
    # –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º cli_args.optuna_trials_reg –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤)
    file_prefix = f"{symbol}_{tf_train}" if symbol else tf_train
    log_context_str = f"—Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf_train}" + (f", —Å–∏–º–≤–æ–ª–∞: {symbol}" if symbol else " (–æ–±—â–∞—è –º–æ–¥–µ–ª—å)")
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è {log_context_str}")

    features_path = f"data/features_{symbol}_{tf_train}.pkl" if symbol else f"data/features_{tf_train}.pkl"
    if not os.path.exists(features_path): logging.error(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path} –Ω–µ –Ω–∞–π–¥–µ–Ω."); return
    try:
        df = pd.read_pickle(features_path)
        if 'timestamp' in df.columns: df = df.sort_values(by='timestamp').reset_index(drop=True); logging.info(
            f"–î–∞–Ω–Ω—ã–µ –∏–∑ {features_path} –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {features_path}: {e}"); return
    if df.empty: logging.error(f"–§–∞–π–ª {features_path} –ø—É—Å—Ç."); return

    required_targets_for_models = ['target_long', 'target_short', 'delta', 'volatility']
    if any(t not in df.columns for t in required_targets_for_models): logging.error(
        f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–∞—Ä–≥–µ—Ç—ã –≤ {features_path}."); return

    df_cleaned = df.dropna(subset=required_targets_for_models).copy()
    target_tp_hit_col = 'target_tp_hit'
    train_tp_hit_model_flag = False
    if target_tp_hit_col in df.columns:
        if not df_cleaned[target_tp_hit_col].isnull().all() and df_cleaned[target_tp_hit_col].nunique(dropna=True) >= 2:
            df_cleaned.dropna(subset=[target_tp_hit_col], inplace=True)
            tp_hit_counts = df_cleaned[target_tp_hit_col].value_counts()
            if tp_hit_counts.min() >= N_CV_SPLITS:
                train_tp_hit_model_flag = True; logging.info(
                    f"'{target_tp_hit_col}'. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {tp_hit_counts.to_dict()}")
            else:
                logging.warning(
                    f"'{target_tp_hit_col}' –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ ({tp_hit_counts.to_dict()}). TP-hit –Ω–µ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω.")
        else:
            logging.warning(f"'{target_tp_hit_col}' –≤—Å–µ NaN –∏–ª–∏ 1 –∫–ª–∞—Å—Å. TP-hit –Ω–µ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω.")
    else:
        logging.warning(f"'{target_tp_hit_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω. TP-hit –Ω–µ –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω.")

    if df_cleaned.empty: logging.error("DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ NaN clear."); return
    logging.info(f"–†–∞–∑–º–µ—Ä DataFrame –ø–æ—Å–ª–µ NaN clear: {df_cleaned.shape}")

    excluded_cols_for_features = ['timestamp', 'symbol', 'target_long', 'target_short', 'delta', 'volatility',
                                  'future_close', 'future_max_high', 'future_min_low']
    if train_tp_hit_model_flag and target_tp_hit_col in df_cleaned.columns: excluded_cols_for_features.append(
        target_tp_hit_col)
    potential_leak_cols = ['target_class', 'target_up']
    for col in potential_leak_cols:
        if col in df_cleaned.columns and col not in excluded_cols_for_features: excluded_cols_for_features.append(
            col); logging.warning(f"'{col}' –∏—Å–∫–ª—é—á–µ–Ω–∞.")
    feature_cols_initial = [c for c in df_cleaned.columns if c not in excluded_cols_for_features and \
                            df_cleaned[c].dtype in [np.int64, np.float64, np.int32, np.float32, bool] and \
                            not pd.api.types.is_datetime64_any_dtype(df_cleaned[c])]
    if not feature_cols_initial: logging.error(f"–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {log_context_str}."); return
    logging.info(
        f"–ù–∞—á–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({log_context_str}): {len(feature_cols_initial)}. –ü–µ—Ä–≤—ã–µ 5: {feature_cols_initial[:5]}")

    X_all_data = df_cleaned[feature_cols_initial].copy()
    y_long_all, y_short_all = df_cleaned['target_long'].astype(int), df_cleaned['target_short'].astype(int)
    y_delta_all, y_vol_all = df_cleaned['delta'], df_cleaned['volatility']
    y_tp_hit_all = df_cleaned[target_tp_hit_col].astype(
        int) if train_tp_hit_model_flag and target_tp_hit_col in df_cleaned.columns else None

    min_data_needed_for_robust_split = N_CV_SPLITS * 10
    if len(X_all_data) < min_data_needed_for_robust_split: logging.warning(
        f"–î–∞–Ω–Ω—ã—Ö ({len(X_all_data)}) –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞–ª–æ.")
    test_set_size_ratio, min_test_samples = 0.15, N_CV_SPLITS * 2
    actual_test_size = int(len(X_all_data) * test_set_size_ratio)
    if actual_test_size < min_test_samples:
        if len(X_all_data) > min_test_samples * 2:
            actual_test_size = min_test_samples; logging.warning(
                f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –º–∞–ª–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {actual_test_size}.")
        else:
            logging.error(f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(X_all_data)}) –¥–ª—è —Ç–µ—Å—Ç–∞. –ü—Ä–µ—Ä–≤–∞–Ω–æ."); return
    split_idx_main = len(X_all_data) - actual_test_size
    X_cv_pool, X_test_full = X_all_data.iloc[:split_idx_main].copy(), X_all_data.iloc[split_idx_main:].copy()
    y_long_cv_pool, y_test_long = y_long_all.iloc[:split_idx_main].copy(), y_long_all.iloc[split_idx_main:].copy()
    y_short_cv_pool, y_test_short = y_short_all.iloc[:split_idx_main].copy(), y_short_all.iloc[split_idx_main:].copy()
    y_delta_cv_pool, y_test_delta = y_delta_all.iloc[:split_idx_main].copy(), y_delta_all.iloc[split_idx_main:].copy()
    y_vol_cv_pool, y_test_vol = y_vol_all.iloc[:split_idx_main].copy(), y_vol_all.iloc[split_idx_main:].copy()
    y_tp_hit_cv_pool = y_tp_hit_all.iloc[:split_idx_main].copy() if y_tp_hit_all is not None else None
    y_test_tp_hit = y_tp_hit_all.iloc[split_idx_main:].copy() if y_tp_hit_all is not None else None
    logging.info(f"–†–∞–∑–º–µ—Ä—ã: CV Pool: {len(X_cv_pool)}, –§–∏–Ω–∞–ª—å–Ω—ã–π –¢–µ—Å—Ç: {len(X_test_full)}")
    if X_cv_pool.empty or X_test_full.empty: logging.error("CV Pool –∏–ª–∏ –¢–µ—Å—Ç –ø—É—Å—Ç—ã. –ü—Ä–µ—Ä–≤–∞–Ω–æ."); return

    optimal_thresholds = {}
    clf_long_model, features_long, opt_thresh_long = None, [], 0.5
    clf_short_model, features_short, opt_thresh_short = None, [], 0.5
    reg_delta_model, reg_vol_model = None, None
    clf_tp_hit_model, opt_thresh_tp_hit = None, 0.5

    n_top_features_to_use = cli_args.n_top_features if cli_args else DEFAULT_N_TOP_FEATURES
    optuna_trials_clf_to_use = cli_args.optuna_trials_clf if cli_args else DEFAULT_OPTUNA_TRIALS_CLASSIFIER
    optuna_trials_reg_to_use = cli_args.optuna_trials_reg if cli_args else DEFAULT_OPTUNA_TRIALS_REGRESSOR
    minority_boost_to_use = cli_args.minority_weight_boost if cli_args else DEFAULT_MINORITY_WEIGHT_BOOST

    if not (cli_args and cli_args.skip_long):
        clf_long_model, features_long, opt_thresh_long = _train_binary_classifier(X_cv_pool, y_long_cv_pool,
                                                                                  X_test_full, y_test_long, "long",
                                                                                  log_context_str, file_prefix,
                                                                                  n_top_features_to_use,
                                                                                  optuna_trials_clf_to_use,
                                                                                  minority_boost_to_use)
        if clf_long_model: optimal_thresholds["long"] = opt_thresh_long
    else:
        logging.info(f"–ü—Ä–æ–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è clf_long –¥–ª—è {log_context_str}.")
        model_path, features_path_txt, threshold_path = f"models/{file_prefix}_clf_long.pkl", f"models/{file_prefix}_features_long_selected.txt", f"models/{file_prefix}_optimal_thresholds.json"
        if os.path.exists(model_path) and os.path.exists(features_path_txt):
            try:
                clf_long_model, features_long = joblib.load(model_path), json.load(open(features_path_txt, "r"))
                if os.path.exists(threshold_path): opt_thresh_long = json.load(open(threshold_path, "r")).get("long",
                                                                                                              0.5)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ clf_long. –ü–æ—Ä–æ–≥: {opt_thresh_long}")
                if clf_long_model: optimal_thresholds["long"] = opt_thresh_long
            except Exception as e:
                logging.error(
                    f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ clf_long: {e}."); clf_long_model, features_long, opt_thresh_long = None, [], 0.5
        else:
            logging.warning(f"–ü—Ä–æ–ø—É—Å–∫ clf_long, –º–æ–¥–µ–ª—å/–ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    if not (cli_args and cli_args.skip_short):
        clf_short_model, features_short, opt_thresh_short = _train_binary_classifier(X_cv_pool, y_short_cv_pool,
                                                                                     X_test_full, y_test_short, "short",
                                                                                     log_context_str, file_prefix,
                                                                                     n_top_features_to_use,
                                                                                     optuna_trials_clf_to_use,
                                                                                     minority_boost_to_use)
        if clf_short_model: optimal_thresholds["short"] = opt_thresh_short
    else:
        logging.info(f"–ü—Ä–æ–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è clf_short –¥–ª—è {log_context_str}.")
        model_path, features_path_txt, threshold_path = f"models/{file_prefix}_clf_short.pkl", f"models/{file_prefix}_features_short_selected.txt", f"models/{file_prefix}_optimal_thresholds.json"
        if os.path.exists(model_path) and os.path.exists(features_path_txt):
            try:
                clf_short_model, features_short = joblib.load(model_path), json.load(open(features_path_txt, "r"))
                if os.path.exists(threshold_path): opt_thresh_short = json.load(open(threshold_path, "r")).get("short",
                                                                                                               0.5)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ clf_short. –ü–æ—Ä–æ–≥: {opt_thresh_short}")
                if clf_short_model: optimal_thresholds["short"] = opt_thresh_short
            except Exception as e:
                logging.error(
                    f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ clf_short: {e}."); clf_short_model, features_short, opt_thresh_short = None, [], 0.5
        else:
            logging.warning(f"–ü—Ä–æ–ø—É—Å–∫ clf_short, –º–æ–¥–µ–ª—å/–ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    if not features_long:
        features_long_path_check = f"models/{file_prefix}_features_long_selected.txt"
        if os.path.exists(features_long_path_check):
            try:
                with open(features_long_path_check, "r") as f:
                    features_long = json.load(f)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ clf_long (–æ–±—É—á–µ–Ω–∏–µ –±—ã–ª–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ).")
            except Exception as e:
                logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ clf_long: {e}")
        else:
            logging.warning("–û–±—É—á–µ–Ω–∏–µ clf_long –ø—Ä–æ–ø—É—â–µ–Ω–æ, —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    active_feature_set_for_others = features_long if features_long else feature_cols_initial
    if not active_feature_set_for_others:
        logging.error(f"–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤/TP-hit –¥–ª—è {log_context_str}.")
    else:
        X_cv_pool_others_sel, X_test_others_sel = X_cv_pool[active_feature_set_for_others], X_test_full[
            active_feature_set_for_others]
        if not (cli_args and cli_args.skip_regressors):
            def objective_regressor(trial, X_tr, y_tr, X_v, y_v, name, trial_num, total_trials):
                logging.info(f"Optuna HPO –¥–ª—è {name}: –ü–æ–ø—ã—Ç–∫–∞ {trial_num}/{total_trials}")
                params = {  # ... (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞) ...
                    'iterations': trial.suggest_int('iterations', 200, 1000, step=100),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
                    'depth': trial.suggest_int('depth', 3, 8),
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
                    'loss_function': 'RMSE', 'eval_metric': 'MAE', 'random_seed': 42,
                    'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'early_stopping_rounds': 30}
                model_r = CatBoostRegressor(**params)
                try:
                    model_r.fit(X_tr, y_tr, eval_set=(X_v, y_v), plot=False)
                    preds_r = model_r.predict(X_v)
                    mae_r = mean_absolute_error(y_v, preds_r)
                    logging.info(f"  Optuna –ü–æ–ø—ã—Ç–∫–∞ {trial_num} –¥–ª—è {name}: MAE={mae_r:.6f}, –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {trial.params}")
                    return mae_r
                except Exception as e_cv_r:
                    logging.error(
                        f"  –û—à–∏–±–∫–∞ –≤ Optuna –ü–æ–ø—ã—Ç–∫–µ {trial_num} –¥–ª—è {name}: {e_cv_r}\n  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {trial.params}")
                    return float('inf')

            val_size_reg = int(len(X_cv_pool_others_sel) * 0.20)
            if val_size_reg < N_CV_SPLITS * 2: val_size_reg = min(len(X_cv_pool_others_sel) // 2, N_CV_SPLITS * 5)
            if len(X_cv_pool_others_sel) - val_size_reg >= N_CV_SPLITS * 5:
                split_idx_r = len(X_cv_pool_others_sel) - val_size_reg
                X_tr_r_opt, X_v_r_opt = X_cv_pool_others_sel.iloc[:split_idx_r], X_cv_pool_others_sel.iloc[split_idx_r:]
                y_d_tr_r_opt, y_d_v_r_opt = y_delta_cv_pool.iloc[:split_idx_r], y_delta_cv_pool.iloc[split_idx_r:]
                y_v_tr_r_opt, y_v_v_r_opt = y_vol_cv_pool.iloc[:split_idx_r], y_vol_cv_pool.iloc[split_idx_r:]

                if not X_v_r_opt.empty and not y_d_v_r_opt.empty and not y_v_v_r_opt.empty:
                    logging.info(f"\n--- Optuna HPO –¥–ª—è reg_delta ({optuna_trials_reg_to_use} –ø–æ–ø—ã—Ç–æ–∫) ---")
                    study_d = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
                    early_stopping_cb_delta = EarlyStoppingCallback(patience=7, min_delta=1e-5, direction='minimize')
                    study_d.optimize(
                        lambda t: objective_regressor(t, X_tr_r_opt, y_d_tr_r_opt, X_v_r_opt, y_d_v_r_opt, "reg_delta",
                                                      t.number + 1, optuna_trials_reg_to_use),
                        n_trials=optuna_trials_reg_to_use, n_jobs=1, callbacks=[early_stopping_cb_delta])
                    bp_d = study_d.best_params
                    reg_delta_model = CatBoostRegressor(**bp_d, loss_function='RMSE', eval_metric='MAE', random_seed=42,
                                                        task_type="GPU", devices='0', verbose=100,
                                                        early_stopping_rounds=30)
                    reg_delta_model.fit(X_cv_pool_others_sel, y_delta_cv_pool, eval_set=(X_test_others_sel,
                                                                                         y_test_delta) if not X_test_others_sel.empty and not y_test_delta.empty else None)
                    logging.info(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã reg_delta: {bp_d}, MAE_val_optuna: {study_d.best_value:.6f}")

                    logging.info(f"\n--- Optuna HPO –¥–ª—è reg_vol ({optuna_trials_reg_to_use} –ø–æ–ø—ã—Ç–æ–∫) ---")
                    study_v = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
                    early_stopping_cb_vol = EarlyStoppingCallback(patience=7, min_delta=1e-5, direction='minimize')
                    study_v.optimize(
                        lambda t: objective_regressor(t, X_tr_r_opt, y_v_tr_r_opt, X_v_r_opt, y_v_v_r_opt, "reg_vol",
                                                      t.number + 1, optuna_trials_reg_to_use),
                        n_trials=optuna_trials_reg_to_use, n_jobs=1, callbacks=[early_stopping_cb_vol])
                    bp_v = study_v.best_params
                    reg_vol_model = CatBoostRegressor(**bp_v, loss_function='RMSE', eval_metric='MAE', random_seed=42,
                                                      task_type="GPU", devices='0', verbose=100,
                                                      early_stopping_rounds=30)
                    reg_vol_model.fit(X_cv_pool_others_sel, y_vol_cv_pool, eval_set=(X_test_others_sel,
                                                                                     y_test_vol) if not X_test_others_sel.empty and not y_test_vol.empty else None)
                    logging.info(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã reg_vol: {bp_v}, MAE_val_optuna: {study_v.best_value:.6f}")
                else:
                    logging.warning(
                        "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è HPO —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ –ø—É—Å—Ç—ã –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–¥–∏–Ω –∫–ª–∞—Å—Å. –û–±—É—á–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
                    # ... (–±–ª–æ–∫ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤)
                    reg_delta_model = CatBoostRegressor(iterations=500, learning_rate=0.03, depth=6,
                                                        loss_function='RMSE', eval_metric='MAE', task_type="GPU",
                                                        devices='0', random_seed=42, verbose=100,
                                                        early_stopping_rounds=50)
                    if not X_cv_pool_others_sel.empty and not y_delta_cv_pool.empty: reg_delta_model.fit(
                        X_cv_pool_others_sel, y_delta_cv_pool, eval_set=(X_test_others_sel,
                                                                         y_test_delta) if not X_test_others_sel.empty and not y_test_delta.empty else None)
                    reg_vol_model = CatBoostRegressor(iterations=500, learning_rate=0.03, depth=6, loss_function='RMSE',
                                                      eval_metric='MAE', task_type="GPU", devices='0', random_seed=42,
                                                      verbose=100, early_stopping_rounds=50)
                    if not X_cv_pool_others_sel.empty and not y_vol_cv_pool.empty: reg_vol_model.fit(
                        X_cv_pool_others_sel, y_vol_cv_pool, eval_set=(X_test_others_sel,
                                                                       y_test_vol) if not X_test_others_sel.empty and not y_test_vol.empty else None)
            else:  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è HPO –º–∞–ª–æ
                logging.warning("–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Optuna HPO —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤. –û–±—É—á–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
                reg_delta_model = CatBoostRegressor(iterations=500, learning_rate=0.03, depth=6, loss_function='RMSE',
                                                    eval_metric='MAE', task_type="GPU", devices='0', random_seed=42,
                                                    verbose=100, early_stopping_rounds=50)
                if not X_cv_pool_others_sel.empty and not y_delta_cv_pool.empty: reg_delta_model.fit(
                    X_cv_pool_others_sel, y_delta_cv_pool, eval_set=(X_test_others_sel,
                                                                     y_test_delta) if not X_test_others_sel.empty and not y_test_delta.empty else None)
                reg_vol_model = CatBoostRegressor(iterations=500, learning_rate=0.03, depth=6, loss_function='RMSE',
                                                  eval_metric='MAE', task_type="GPU", devices='0', random_seed=42,
                                                  verbose=100, early_stopping_rounds=50)
                if not X_cv_pool_others_sel.empty and not y_vol_cv_pool.empty: reg_vol_model.fit(X_cv_pool_others_sel,
                                                                                                 y_vol_cv_pool,
                                                                                                 eval_set=(
                                                                                                     X_test_others_sel,
                                                                                                     y_test_vol) if not X_test_others_sel.empty and not y_test_vol.empty else None)
        else:  # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤
            logging.info(f"–ü—Ä–æ–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è {log_context_str}.")
            if os.path.exists(f"models/{file_prefix}_reg_delta.pkl"): reg_delta_model = joblib.load(
                f"models/{file_prefix}_reg_delta.pkl")
            if os.path.exists(f"models/{file_prefix}_reg_vol.pkl"): reg_vol_model = joblib.load(
                f"models/{file_prefix}_reg_vol.pkl")
            if reg_delta_model and reg_vol_model:
                logging.info("–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤.")
            else:
                logging.warning("–ü—Ä–æ–ø—É—Å–∫ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤, –Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

        if not (cli_args and cli_args.skip_tp_hit):
            if train_tp_hit_model_flag and y_tp_hit_cv_pool is not None:
                if y_tp_hit_cv_pool.nunique() >= 2 and y_tp_hit_cv_pool.value_counts().min() >= N_CV_SPLITS:
                    clf_tp_hit_model, _, opt_thresh_tp_hit = _train_binary_classifier(X_cv_pool_others_sel,
                                                                                      y_tp_hit_cv_pool,
                                                                                      X_test_others_sel, y_test_tp_hit,
                                                                                      "tp_hit", log_context_str,
                                                                                      file_prefix,
                                                                                      n_top_features_to_use,
                                                                                      optuna_trials_clf_to_use,
                                                                                      minority_boost_to_use)
                    if clf_tp_hit_model: optimal_thresholds["tp_hit"] = opt_thresh_tp_hit
                else:
                    logging.warning(f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö/–∫–ª–∞—Å—Å–æ–≤ –¥–ª—è clf_tp_hit. –ü—Ä–æ–ø—É—Å–∫.")
        else:  # –ó–∞–≥—Ä—É–∑–∫–∞ clf_tp_hit
            logging.info(f"–ü—Ä–æ–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è clf_tp_hit –¥–ª—è {log_context_str}.")
            model_path, threshold_path = f"models/{file_prefix}_clf_tp_hit.pkl", f"models/{file_prefix}_optimal_thresholds.json"
            if os.path.exists(model_path):
                clf_tp_hit_model = joblib.load(model_path)
                if os.path.exists(threshold_path): opt_thresh_tp_hit = json.load(open(threshold_path, "r")).get(
                    "tp_hit", 0.5)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ clf_tp_hit. –ü–æ—Ä–æ–≥: {opt_thresh_tp_hit}")
                if clf_tp_hit_model: optimal_thresholds["tp_hit"] = opt_thresh_tp_hit
            else:
                logging.warning("–ü—Ä–æ–ø—É—Å–∫ clf_tp_hit, –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")

    thresholds_path = f"models/{file_prefix}_optimal_thresholds.json"
    try:
        existing_thresholds = {}
        if os.path.exists(thresholds_path):
            try:
                with open(thresholds_path, "r", encoding="utf-8") as f:
                    existing_thresholds = json.load(f)
                if not isinstance(existing_thresholds, dict): existing_thresholds = {}
            except json.JSONDecodeError:
                existing_thresholds = {}; logging.warning(f"–§–∞–π–ª {thresholds_path} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω.")
        existing_thresholds.update(optimal_thresholds)
        with open(thresholds_path, "w", encoding="utf-8") as f:
            json.dump(existing_thresholds, f, indent=2)
        logging.info(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã/–æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ {thresholds_path}: {existing_thresholds}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å –ø–æ—Ä–æ–≥–∞–º–∏ {thresholds_path}: {e}")

    logging.info(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {log_context_str}...")
    if clf_long_model: joblib.dump(clf_long_model, f"models/{file_prefix}_clf_long.pkl"); logging.info(
        "–ú–æ–¥–µ–ª—å clf_long —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    if clf_short_model: joblib.dump(clf_short_model, f"models/{file_prefix}_clf_short.pkl"); logging.info(
        "–ú–æ–¥–µ–ª—å clf_short —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    if reg_delta_model: joblib.dump(reg_delta_model, f"models/{file_prefix}_reg_delta.pkl"); logging.info(
        "–ú–æ–¥–µ–ª—å reg_delta —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    if reg_vol_model: joblib.dump(reg_vol_model, f"models/{file_prefix}_reg_vol.pkl"); logging.info(
        "–ú–æ–¥–µ–ª—å reg_vol —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    if clf_tp_hit_model: joblib.dump(clf_tp_hit_model, f"models/{file_prefix}_clf_tp_hit.pkl"); logging.info(
        "–ú–æ–¥–µ–ª—å clf_tp_hit —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    metrics_output = f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ ({log_context_str}):\n"

    def get_classification_metrics_with_threshold(model, X_eval, y_eval, model_name, threshold,
                                                  class_names_map={0: 'NO', 1: 'YES'}):
        if model is None or X_eval.empty or y_eval.empty or y_eval.nunique() < 2:
            return f"--- {model_name} ---\n–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª–∏ –∏–ª–∏ 1 –∫–ª–∞—Å—Å –≤ y_eval.\n"
        out_str = f"--- {model_name} (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥={threshold:.2f}) ---\n"
        try:
            model_classes_list = list(model.classes_)
            if 1 not in model_classes_list:
                logging.warning(
                    f"–ö–ª–∞—Å—Å 1 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –º–æ–¥–µ–ª–∏ {model_name}. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∞ 1 –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã.")
                y_proba_positive = np.zeros(len(X_eval))
            else:
                proba_col_idx = model_classes_list.index(1)
                y_proba_positive = model.predict_proba(X_eval)[:, proba_col_idx]
            y_pred = (y_proba_positive >= threshold).astype(int)
            present_labels = sorted(list(set(y_eval.unique()) | set(pd.Series(y_pred).unique())))
            if len(present_labels) < 2:
                out_str += f"–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å ({present_labels[0]}) –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ y_true/y_pred. –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.\n"
                out_str += f"   Accuracy: {accuracy_score(y_eval, y_pred):.4f}\n"
                if present_labels[0] == 1 and 1 in model_classes_list:
                    out_str += f"   F1-score ({class_names_map.get(1, '1')}): {f1_score(y_eval, y_pred, pos_label=1, zero_division=0):.4f}\n"
                    out_str += f"   Precision ({class_names_map.get(1, '1')}): {precision_score(y_eval, y_pred, pos_label=1, zero_division=0):.4f}\n"
                    out_str += f"   Recall ({class_names_map.get(1, '1')}): {recall_score(y_eval, y_pred, pos_label=1, zero_division=0):.4f}\n"
            else:
                target_names_for_report = [class_names_map.get(lbl, f"–ö–ª–∞—Å—Å_{lbl}") for lbl in present_labels]
                out_str += classification_report(y_eval, y_pred, labels=present_labels,
                                                 target_names=target_names_for_report, digits=4, zero_division=0) + "\n"
                out_str += f"   Accuracy: {accuracy_score(y_eval, y_pred):.4f}\n"
                if 1 in present_labels and 1 in model_classes_list:
                    out_str += f"   F1-score ({class_names_map.get(1, '1')}): {f1_score(y_eval, y_pred, pos_label=1, zero_division=0):.4f}\n"
                    out_str += f"   Precision ({class_names_map.get(1, '1')}): {precision_score(y_eval, y_pred, pos_label=1, zero_division=0):.4f}\n"
                    out_str += f"   Recall ({class_names_map.get(1, '1')}): {recall_score(y_eval, y_pred, pos_label=1, zero_division=0):.4f}\n"
                if y_eval.nunique() > 1 and 1 in model_classes_list:
                    out_str += f"   ROC AUC: {roc_auc_score(y_eval, y_proba_positive):.4f}\n"
                    out_str += f"   PR AUC: {average_precision_score(y_eval, y_proba_positive):.4f}\n"
                else:
                    out_str += "   ROC AUC: N/A (–æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ y_true –∏–ª–∏ –∫–ª–∞—Å—Å 1 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –º–æ–¥–µ–ª–∏)\n"
                    out_str += "   PR AUC: N/A (–æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ y_true –∏–ª–∏ –∫–ª–∞—Å—Å 1 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –º–æ–¥–µ–ª–∏)\n"
            cm_labels = [0, 1] if all(l in present_labels for l in [0, 1]) else present_labels
            cm_target_names = [class_names_map.get(lbl, f"–ö–ª–∞—Å—Å_{lbl}") for lbl in cm_labels]
            if len(cm_labels) >= 2:
                cm = confusion_matrix(y_eval, y_pred, labels=cm_labels)
                cm_df = pd.DataFrame(cm, index=[f"True_{n}" for n in cm_target_names],
                                     columns=[f"Pred_{n}" for n in cm_target_names])
                out_str += f"   Confusion Matrix:\n{cm_df.to_string()}\n"
            else:
                out_str += "   Confusion Matrix: N/A (–º–µ–Ω–µ–µ 2 –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã)\n"
        except Exception as e:
            out_str += f"–û—à–∏–±–∫–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è {model_name}: {e}\n"
        return out_str

    if clf_long_model and features_long and not y_test_long.empty:
        X_test_long_sel = X_test_full[features_long] if features_long and all(
            f in X_test_full.columns for f in features_long) else pd.DataFrame()
        if not X_test_long_sel.empty:
            metrics_output += get_classification_metrics_with_threshold(clf_long_model, X_test_long_sel, y_test_long,
                                                                        "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä Long",
                                                                        optimal_thresholds.get("long", 0.5),
                                                                        {0: 'NO_LONG', 1: 'LONG'})
        else:
            metrics_output += "--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä Long ---\n–ü—Ä–æ–ø—É—â–µ–Ω–æ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ—Å—Ç–µ.\n"
    if clf_short_model and features_short and not y_test_short.empty:
        X_test_short_sel = X_test_full[features_short] if features_short and all(
            f in X_test_full.columns for f in features_short) else pd.DataFrame()
        if not X_test_short_sel.empty:
            metrics_output += get_classification_metrics_with_threshold(clf_short_model, X_test_short_sel, y_test_short,
                                                                        "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä Short",
                                                                        optimal_thresholds.get("short", 0.5),
                                                                        {0: 'NO_SHORT', 1: 'SHORT'})
        else:
            metrics_output += "--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä Short ---\n–ü—Ä–æ–ø—É—â–µ–Ω–æ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ—Å—Ç–µ.\n"

    if active_feature_set_for_others and not X_test_others_sel.empty:
        if reg_delta_model and not y_test_delta.empty: metrics_output += f"   üìà MAE Delta: {mean_absolute_error(y_test_delta, reg_delta_model.predict(X_test_others_sel)):.6f}\n"
        if reg_vol_model and not y_test_vol.empty: metrics_output += f"   ‚ö° MAE Volatility: {mean_absolute_error(y_test_vol, reg_vol_model.predict(X_test_others_sel)):.6f}\n"
        if clf_tp_hit_model and y_test_tp_hit is not None and not y_test_tp_hit.empty: metrics_output += get_classification_metrics_with_threshold(
            clf_tp_hit_model, X_test_others_sel, y_test_tp_hit, "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä TP-Hit",
            optimal_thresholds.get("tp_hit", 0.5), {0: 'NO_TP_HIT', 1: 'TP_HIT'})

    print(metrics_output)
    log_file_path = f"logs/train_metrics_{file_prefix}.txt"
    try:
        with open(log_file_path, "a", encoding="utf-8") as f_log:
            f_log.write(metrics_output)
            if features_long: f_log.write(
                f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è LONG ({len(features_long)}):\n" + ", ".join(features_long) + "\n")
            if features_short: f_log.write(
                f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è SHORT ({len(features_short)}):\n" + ", ".join(features_short) + "\n")
            if active_feature_set_for_others and (reg_delta_model or reg_vol_model or clf_tp_hit_model):
                f_log.write(
                    f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤/TP-hit ({len(active_feature_set_for_others)}):\n" + ", ".join(
                        active_feature_set_for_others) + "\n")
            f_log.write("\n" + "=" * 50 + "\n\n")
        logging.info(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ª–æ–≥: {log_file_path}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∞ –º–µ—Ç—Ä–∏–∫: {e}")
    logging.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {log_context_str} –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π CatBoost.")
    parser.add_argument('--tf', type=str, required=True, help="–¢–∞–π–º—Ñ—Ä–µ–π–º (1m, 5m, ...)")
    parser.add_argument('--symbol-group', type=str, help="–ü—Å–µ–≤–¥–æ–Ω–∏–º –≥—Ä—É–ø–ø—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: top5, meme)")
    parser.add_argument('--symbol', type=str, default=None, help="–°–∏–º–≤–æ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: BTCUSDT)")
    # --- –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ---
    parser.add_argument('--n-top-features', type=int, default=DEFAULT_N_TOP_FEATURES,
                        help=f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–±–æ—Ä–∞ (default: {DEFAULT_N_TOP_FEATURES})")
    parser.add_argument('--optuna-trials-clf', type=int, default=DEFAULT_OPTUNA_TRIALS_CLASSIFIER,
                        help=f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ Optuna –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ (default: {DEFAULT_OPTUNA_TRIALS_CLASSIFIER})")
    parser.add_argument('--optuna-trials-reg', type=int, default=DEFAULT_OPTUNA_TRIALS_REGRESSOR,
                        help=f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ Optuna –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ (default: {DEFAULT_OPTUNA_TRIALS_REGRESSOR})")
    parser.add_argument('--minority-weight-boost', type=float, default=DEFAULT_MINORITY_WEIGHT_BOOST,
                        help=f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É—Å–∏–ª–µ–Ω–∏—è –≤–µ—Å–∞ –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (default: {DEFAULT_MINORITY_WEIGHT_BOOST})")
    # --- –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ —ç—Ç–∞–ø–æ–≤ ---
    parser.add_argument('--skip-long', action='store_true', help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ clf_long")
    parser.add_argument('--skip-short', action='store_true', help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ clf_short")
    parser.add_argument('--skip-regressors', action='store_true', help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤")
    parser.add_argument('--skip-tp-hit', action='store_true', help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ clf_tp_hit")
    args = parser.parse_args()

    entities_to_process = []
    is_group_run = False

    if args.symbol_group:
        if args.symbol_group in SYMBOL_GROUPS:
            entities_to_process = [args.symbol_group]
            is_group_run = True
            logging.info(f"üß© –û–±—É—á–µ–Ω–∏–µ –≥—Ä—É–ø–ø–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è: {args.symbol_group}")
        else:
            logging.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞: {args.symbol_group}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(SYMBOL_GROUPS.keys())}")
            sys.exit(1)
    elif args.symbol:
        if args.symbol.lower() in SYMBOL_GROUPS:
            args.symbol_group = args.symbol.lower()
            entities_to_process = [args.symbol_group]
            is_group_run = True
            logging.info(
                f"üß© –ê—Ä–≥—É–º–µ–Ω—Ç --symbol ('{args.symbol}') —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–∞–∫ –≥—Ä—É–ø–ø–∞. –û–±—É—á–µ–Ω–∏–µ –¥–ª—è –≥—Ä—É–ø–ø—ã: {args.symbol_group}")
            args.symbol = None
        else:
            entities_to_process = [args.symbol.upper()]
            is_group_run = False
            logging.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞: {args.symbol.upper()}")
    else:
        logging.error("–ù–µ —É–∫–∞–∑–∞–Ω –Ω–∏ --symbol, –Ω–∏ --symbol-group. –£–∫–∞–∂–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –Ω–∏—Ö.")
        sys.exit(1)

    for entity_name in entities_to_process:
        current_log_file_prefix = f"{entity_name}_{args.tf}"
        current_log_context_str = f"—Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {args.tf}, " + \
                                  (
                                      f"–≥—Ä—É–ø–ø—ã {entity_name}" if is_group_run or args.symbol_group else f"—Å–∏–º–≤–æ–ª–∞ {entity_name}")

        current_main_log_path = f"logs/train_metrics_{current_log_file_prefix}.txt"
        if not (args.skip_long and args.skip_short and args.skip_regressors and args.skip_tp_hit):
            try:
                with open(current_main_log_path, "w", encoding="utf-8") as f_clear:
                    f_clear.write(
                        f"=== –ù–∞—á–∞–ª–æ –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {current_log_context_str} ({pd.Timestamp.now()}) ===\n\n")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è/–æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥-—Ñ–∞–π–ª–∞ {current_main_log_path}: {e}")

        try:
            train_all_models(args.tf, entity_name, args)
        except KeyboardInterrupt:
            logging.info(f"üõë –û–±—É—á–µ–Ω–∏–µ –¥–ª—è {current_log_context_str} –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
            sys.exit(130)
        except Exception as e:
            logging.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ {current_log_context_str}: {e}", exc_info=True)
            if len(entities_to_process) == 1:
                sys.exit(1)
            else:
                logging.warning(f"–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –Ω–∞ {current_log_context_str}.")