import pandas as pd
import numpy as np
import argparse
import os
from catboost import CatBoostClassifier, CatBoostRegressor, Pool, cv
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, mean_absolute_error, accuracy_score
from sklearn.metrics import f1_score, precision_score, confusion_matrix
# ‚úÖ –ò–º–ø–æ—Ä—Ç –¥–ª—è ROC AUC
from sklearn.metrics import roc_auc_score
import joblib
import sys
import logging
import random

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s [TrainModel] - %(message)s',
                    stream=sys.stdout)

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
os.makedirs("models", exist_ok=True)
os.makedirs("logs", exist_ok=True)

TARGET_CLASS_NAMES = ['DOWN', 'UP']  # For display purposes in reports
PREDICT_PROBA_THRESHOLD_CLASS = 0.55


def train_all_models(tf_train):
    logging.info(f"üöÄ  –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞: {tf_train}")
    features_path = f"data/features_{tf_train}.pkl"
    if not os.path.exists(features_path):
        logging.error(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–±—É—á–µ–Ω–∏–µ –¥–ª—è {tf_train} –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        return

    try:
        df = pd.read_pickle(features_path)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path}: {e}")
        return

    if df.empty:
        logging.error(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path} –ø—É—Å—Ç. –û–±—É—á–µ–Ω–∏–µ –¥–ª—è {tf_train} –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        return

    required_cols_for_dropna = ['target_class', 'delta', 'volatility']
    target_tp_hit_col = 'target_tp_hit'

    train_tp_hit_model_flag = False
    use_stratified_cv_for_tp_hit = False

    if target_tp_hit_col in df.columns:
        logging.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '{target_tp_hit_col}'. –ü–æ–ø—ã—Ç–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ TP-hit.")
        if df[target_tp_hit_col].isnull().all() or df[target_tp_hit_col].nunique() < 2:
            logging.warning(
                f"–ö–æ–ª–æ–Ω–∫–∞ '{target_tp_hit_col}' —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ NaN –∏–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å TP-hit –ù–ï –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞.")
        elif df[target_tp_hit_col].value_counts().min() < 10:
            logging.warning(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –æ–¥–Ω–æ–º –∏–∑ –∫–ª–∞—Å—Å–æ–≤ '{target_tp_hit_col}' ({df[target_tp_hit_col].value_counts().to_dict()}). "
                f"Stratified CV –¥–ª—è TP-hit –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–æ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º."
            )
            train_tp_hit_model_flag = True
            use_stratified_cv_for_tp_hit = False
        else:
            train_tp_hit_model_flag = True
            use_stratified_cv_for_tp_hit = True
            logging.info(
                f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ '{target_tp_hit_col}' (–¥–æ –æ—á–∏—Å—Ç–∫–∏ NaN): {df[target_tp_hit_col].value_counts().to_dict()}")
        required_cols_for_dropna.append(target_tp_hit_col)
    else:
        logging.warning(
            f"–ö–æ–ª–æ–Ω–∫–∞ '{target_tp_hit_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {features_path}. "
            f"–ú–æ–¥–µ–ª—å TP-hit (clf_tp_hit) –ù–ï –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞."
        )

    df_cleaned = df.dropna(subset=required_cols_for_dropna).copy()
    if df_cleaned.empty:
        logging.error(
            f"DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º {required_cols_for_dropna} –¥–ª—è {tf_train}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
        return

    logging.info(f"–†–∞–∑–º–µ—Ä DataFrame –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN: {df_cleaned.shape}")
    if train_tp_hit_model_flag and target_tp_hit_col in df_cleaned.columns:
        logging.info(
            f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ '{target_tp_hit_col}' (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN): {df_cleaned[target_tp_hit_col].value_counts().to_dict()}")
        if df_cleaned[target_tp_hit_col].nunique() < 2 or df_cleaned[
            target_tp_hit_col].value_counts().min() < 5:
            logging.warning(
                f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–ª–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ '{target_tp_hit_col}' ({df_cleaned[target_tp_hit_col].value_counts().to_dict()}). "
                f"Stratified CV –¥–ª—è TP-hit –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–æ."
            )
            use_stratified_cv_for_tp_hit = False

    excluded_cols_for_features = ['timestamp', 'symbol', 'target_class', 'delta', 'volatility', 'future_close']
    if train_tp_hit_model_flag and target_tp_hit_col in df_cleaned.columns:
        excluded_cols_for_features.append(target_tp_hit_col)

    potential_leak_cols = ['target_up']
    for col in potential_leak_cols:
        if col not in excluded_cols_for_features:
            excluded_cols_for_features.append(col)

    feature_cols_initial = [col for col in df_cleaned.columns if
                            col not in excluded_cols_for_features and df_cleaned[col].dtype in [np.int64, np.float64,
                                                                                                np.int32, np.float32,
                                                                                                bool]]
    feature_cols_initial = [col for col in feature_cols_initial if not pd.api.types.is_datetime64_any_dtype(
        df_cleaned[col]) and not pd.api.types.is_string_dtype(df_cleaned[col])]

    if not feature_cols_initial:
        logging.error(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {tf_train}.")
        return

    logging.info(
        f"–ù–∞—á–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(feature_cols_initial)}. –ü–µ—Ä–≤—ã–µ 5: {feature_cols_initial[:5]}")

    X_all_data = df_cleaned[feature_cols_initial]
    y_class_all = df_cleaned['target_class']
    y_delta_all = df_cleaned['delta']
    y_vol_all = df_cleaned['volatility']
    y_tp_hit_all = df_cleaned[
        target_tp_hit_col] if train_tp_hit_model_flag and target_tp_hit_col in df_cleaned.columns else None

    if X_all_data.empty:
        logging.error(
            f"DataFrame X_all_data (–ø—Ä–∏–∑–Ω–∞–∫–∏) –ø—É—Å—Ç –¥–ª—è {tf_train}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ feature_cols_initial –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
        return

    if len(X_all_data) < 20:
        logging.error(
            f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(X_all_data)}) –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train/test –¥–ª—è {tf_train}. –û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        return

    try:
        X_train_cv_pool, X_test_full, y_class_train_cv_pool, y_test_class, \
            y_delta_train_cv_pool, y_test_delta, y_vol_train_cv_pool, y_test_vol = train_test_split(
            X_all_data, y_class_all, y_delta_all, y_vol_all,
            test_size=0.15, random_state=42, shuffle=False  # Ensure y_class_all is used here
        )

        X_cv_pool = X_train_cv_pool
        y_class_cv_pool = y_class_train_cv_pool  # At this point, these are string labels

        X_train_full = X_train_cv_pool
        y_train_class = y_class_train_cv_pool  # At this point, these are string labels
        y_train_delta = y_delta_train_cv_pool
        y_train_vol = y_vol_train_cv_pool

        if train_tp_hit_model_flag and y_tp_hit_all is not None:
            y_tp_hit_train_cv_pool = y_tp_hit_all.loc[X_train_cv_pool.index]
            y_test_tp_hit = y_tp_hit_all.loc[X_test_full.index]
            y_tp_hit_cv_pool = y_tp_hit_train_cv_pool
            y_train_tp_hit = y_tp_hit_train_cv_pool

            if not y_train_tp_hit.empty:
                logging.info(
                    f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_train_tp_hit: {y_train_tp_hit.value_counts().to_dict()}")
            if not y_test_tp_hit.empty:
                logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_test_tp_hit: {y_test_tp_hit.value_counts().to_dict()}")
        else:
            y_train_tp_hit, y_test_tp_hit, y_tp_hit_cv_pool = None, None, None
            train_tp_hit_model_flag = False

    except ValueError as e:
        logging.error(
            f"–û—à–∏–±–∫–∞ ValueError –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {tf_train}: {e}. –í–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏.")
        return
    except Exception as e:
        logging.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {tf_train}: {e}", exc_info=True)
        return

    # --- –í–ù–ï–°–ï–ù–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–ô: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ target_class –≤ 0/1 ---
    logging.info("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ target_class –≤ 0/1 (DOWN=0, UP=1)...")
    label_mapping = {'DOWN': 0, 'UP': 1}

    # –ü—Ä–∏–º–µ–Ω—è–µ–º .map() –∫ –∫–∞–∂–¥–æ–π —Å–µ—Ä–∏–∏. .map() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Ä–∏—é.
    if not y_class_all.empty:
        y_class_all = y_class_all.map(label_mapping)
    if not y_class_cv_pool.empty:  # y_class_cv_pool is an alias for y_class_train_cv_pool initially
        y_class_cv_pool = y_class_cv_pool.map(label_mapping)
    if not y_train_class.empty:  # y_train_class is an alias for y_class_train_cv_pool initially
        y_train_class = y_train_class.map(label_mapping)
    if not y_test_class.empty:
        y_test_class = y_test_class.map(label_mapping)

    # –ï—Å–ª–∏ y_class_train_cv_pool –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è y_class_cv_pool –∏ y_train_class,
    # –∏ –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–µ–º–∏ –∂–µ –æ–±—ä–µ–∫—Ç–∞–º–∏, —Ç–æ –Ω—É–∂–Ω–æ –±—ã–ª–æ –º–∞–ø–∏—Ç—å y_class_train_cv_pool
    # –∏ –ø–µ—Ä–µ–ø—Ä–∏—Å–≤–∞–∏–≤–∞—Ç—å y_class_cv_pool = y_class_train_cv_pool, y_train_class = y_class_train_cv_pool
    # –û–¥–Ω–∞–∫–æ, —Ç–∞–∫ –∫–∞–∫ –∫–∞–∂–¥–∞—è –∏–∑ –Ω–∏—Ö –º–∞–ø–∏—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ, —ç—Ç–æ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ —Å–ª—É—á–∞–∏.
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ y_class_train_cv_pool —Ç–∞–∫–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞, –µ—Å–ª–∏ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–¥–µ-—Ç–æ –µ—â–µ
    # –∏–ª–∏ –µ—Å–ª–∏ y_class_cv_pool/y_train_class –Ω–µ —è–≤–ª—è—é—Ç—Å—è –µ–µ –ø—Ä—è–º—ã–º–∏ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è–º–∏ –ø–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞.
    # –í –¥–∞–Ω–Ω–æ–º –∫–æ–¥–µ y_class_cv_pool –∏ y_train_class - —ç—Ç–æ —Ç–µ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–ª–µ–µ.

    logging.info(
        f"–ü—Ä–∏–º–µ—Ä y_train_class –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {y_train_class.head().tolist() if not y_train_class.empty else '–ø—É—Å—Ç–æ'}")
    logging.info(
        f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_train_class –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {y_train_class.value_counts().to_dict() if not y_train_class.empty else '–ø—É—Å—Ç–æ'}")
    logging.info(
        f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_test_class –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {y_test_class.value_counts().to_dict() if not y_test_class.empty else '–ø—É—Å—Ç–æ'}")
    logging.info(
        f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_class_cv_pool –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {y_class_cv_pool.value_counts().to_dict() if not y_class_cv_pool.empty else '–ø—É—Å—Ç–æ'}")
    # --- –ö–û–ù–ï–¶ –í–ù–ï–°–ï–ù–ò–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô ---

    logging.info(f"–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫: Train/CV Pool: {len(X_cv_pool)}, Test: {len(X_test_full)}")
    if X_cv_pool.empty or X_test_full.empty:
        logging.error(f"X_cv_pool (Train/CV) –∏–ª–∏ X_test_full –ø—É—Å—Ç—ã –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–ª—è {tf_train}.")
        return

    logging.info(
        f"üöÄ –ù–∞—á–∞–ª–æ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Top 20) –¥–ª—è clf_class ({tf_train}) –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(X_cv_pool)} –æ–±—Ä–∞–∑—Ü–æ–≤...")
    top20_features = []
    if X_cv_pool.empty or y_class_cv_pool.empty:
        logging.warning(
            f"X_cv_pool –∏–ª–∏ y_class_cv_pool –ø—É—Å—Ç—ã –ø–µ—Ä–µ–¥ –æ—Ç–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –û—Ç–±–æ—Ä –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.")
        top20_features = X_cv_pool.columns.tolist()
    elif len(X_cv_pool.columns) <= 20:
        logging.info(
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({len(X_cv_pool.columns)}) –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ 20. –û—Ç–±–æ—Ä –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        top20_features = X_cv_pool.columns.tolist()
    else:
        # y_class_cv_pool —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç 0/1, —á—Ç–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è StratifiedKFold
        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        min_samples_in_class = y_class_cv_pool.value_counts().min() if not y_class_cv_pool.empty else 0

        if min_samples_in_class < kf.get_n_splits():
            logging.warning(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ y_class_cv_pool ({y_class_cv_pool.value_counts().to_dict() if not y_class_cv_pool.empty else '–ø—É—Å—Ç–æ'}) "
                f"–¥–ª—è StratifiedKFold. –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è."
            )
            top20_features = X_cv_pool.columns.tolist()
        else:
            feature_importances_fi = np.zeros(X_cv_pool.shape[1])
            num_successful_folds = 0
            for fold, (train_idx, val_idx) in enumerate(kf.split(X_cv_pool, y_class_cv_pool)):
                logging.info(f"–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: —Ñ–æ–ª–¥ {fold + 1}/{kf.get_n_splits()}")
                X_fold_train, X_fold_val = X_cv_pool.iloc[train_idx], X_cv_pool.iloc[val_idx]
                y_fold_train, y_fold_val = y_class_cv_pool.iloc[train_idx], y_class_cv_pool.iloc[val_idx]

                if X_fold_val.empty or y_fold_val.empty:
                    logging.warning(
                        f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø—É—Å—Ç –≤ —Ñ–æ–ª–¥–µ {fold + 1}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    continue
                clf_fi = CatBoostClassifier(
                    iterations=200, learning_rate=0.05,
                    early_stopping_rounds=25, verbose=False,
                    loss_function='Logloss',  # Logloss is suitable for 0/1 binary targets
                    task_type="GPU", devices='0', random_seed=42
                )
                clf_fi.fit(X_fold_train, y_fold_train, eval_set=(X_fold_val, y_fold_val))
                feature_importances_fi += clf_fi.get_feature_importance()
                num_successful_folds += 1

            if num_successful_folds > 0:
                fi_mean = pd.Series(feature_importances_fi / num_successful_folds,
                                    index=X_cv_pool.columns).sort_values(ascending=False)
                top20_features = fi_mean.head(20).index.tolist()
                logging.info(f"–¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {tf_train}: {top20_features}")
            else:
                logging.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–æ–ª–¥–∞ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ.")
                top20_features = X_cv_pool.columns.tolist()

    X_train_sel = X_train_full[top20_features]
    X_test_sel = X_test_full[top20_features]
    X_cv_pool_sel = X_cv_pool[top20_features]

    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞: {len(top20_features)}")
    feature_cols_final = top20_features
    try:
        features_list_path = f"models/{tf_train}_features_selected.txt"
        with open(features_list_path, "w", encoding="utf-8") as f:
            f.write("\n".join(feature_cols_final))
        logging.info(f"–°–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {features_list_path}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    # --- –í–ù–ï–°–ï–ù–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–ô: –†–∞—Å—á–µ—Ç class_weights –¥–ª—è 0/1 –∫–ª–∞—Å—Å–æ–≤ ---
    logging.info(f"–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è clf_class ({tf_train}) –Ω–∞ –æ—Å–Ω–æ–≤–µ y_train_class (0/1)...")
    class_weights_final_for_catboost = {}
    if not y_train_class.empty:
        counts_class_in_train = y_train_class.value_counts().to_dict()  # e.g., {0: N_down, 1: N_up}
        total_class_in_train = sum(counts_class_in_train.values())

        # Calculate for class 0 (formerly 'DOWN')
        count_0 = counts_class_in_train.get(0, 0)
        if count_0 > 0:
            class_weights_final_for_catboost[0] = total_class_in_train / count_0
        else:
            class_weights_final_for_catboost[0] = 1.0
            logging.warning("–ö–ª–∞—Å—Å 0 ('DOWN') –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ y_train_class. –í–µ—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ 1.0.")

        # Calculate for class 1 (formerly 'UP')
        count_1 = counts_class_in_train.get(1, 0)
        if count_1 > 0:
            class_weights_final_for_catboost[1] = total_class_in_train / count_1
        else:
            class_weights_final_for_catboost[1] = 1.0
            logging.warning("–ö–ª–∞—Å—Å 1 ('UP') –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ y_train_class. –í–µ—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ 1.0.")

        logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_train_class (0/1): {counts_class_in_train}")
    else:
        logging.warning(
            "y_train_class –ø—É—Å—Ç, –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–µ—Å–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (1.0).")
        class_weights_final_for_catboost = {0: 1.0, 1: 1.0}  # Default if no data

    logging.info(f"–í–µ—Å–∞ –¥–ª—è clf_class (CatBoost, –∫–ª—é—á–∏ 0/1): {class_weights_final_for_catboost}")
    # --- –ö–û–ù–ï–¶ –í–ù–ï–°–ï–ù–ò–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô ---

    clf_class_model = None
    num_unique_classes_clf = y_class_cv_pool.nunique() if not y_class_cv_pool.empty else 0
    if num_unique_classes_clf == 2:
        loss_func_clf = 'Logloss'  # Suitable for 0/1 binary classification
        eval_metric_clf = 'Accuracy'  # Or 'AUC', 'F1'
    elif num_unique_classes_clf == 1:
        logging.warning(
            f"–í y_class_cv_pool —Ç–æ–ª—å–∫–æ {num_unique_classes_clf} —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å. "
            f"–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏–ª–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Logloss/Accuracy."
        )
        loss_func_clf = 'Logloss'
        eval_metric_clf = 'Accuracy'
    else:  # 0 or more than 2 (should not happen for binary 'target_class' after mapping)
        logging.error(
            f"–í y_class_cv_pool {num_unique_classes_clf} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤. "
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Logloss/Accuracy –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."
        )
        loss_func_clf = 'Logloss'
        eval_metric_clf = 'Accuracy'

    logging.info(f"\nüîÑ –ó–∞–ø—É—Å–∫ RandomSearch –¥–ª—è clf_class ({tf_train})...")
    best_score_rs = -1
    best_params_rs = None
    num_rs_trials = 10

    default_cv_params_class = {
        'iterations': 700, 'learning_rate': 0.03, 'depth': 6,
        'loss_function': loss_func_clf, 'eval_metric': eval_metric_clf,
        'early_stopping_rounds': 50, 'random_seed': 42,
        'task_type': 'GPU', 'devices': '0', 'verbose': 0,
        'class_weights': class_weights_final_for_catboost,  # Now uses 0/1 keys
    }
    cv_params_class = default_cv_params_class.copy()
    best_iter_class_cv = cv_params_class['iterations']

    min_samples_for_cv = 5  # CatBoost default for n_splits=5 in cv
    if not X_cv_pool_sel.empty and not y_class_cv_pool.empty and \
            y_class_cv_pool.nunique() >= 2 and \
            (y_class_cv_pool.value_counts().min() if not y_class_cv_pool.empty else 0) >= min_samples_for_cv:
        for i in range(num_rs_trials):
            logging.info(f"RandomSearch –¥–ª—è clf_class: –ø–æ–ø—ã—Ç–∫–∞ {i + 1}/{num_rs_trials}")
            trial_params = {
                'iterations': random.choice([300, 500, 700, 1000]),
                'learning_rate': random.choice([0.01, 0.03, 0.05, 0.07]),
                'depth': random.choice([4, 5, 6, 7, 8]),
                'l2_leaf_reg': random.choice([1, 3, 5, 7, 9]),
                'loss_function': loss_func_clf,
                'eval_metric': eval_metric_clf,
                'early_stopping_rounds': 50,
                'random_seed': 42,
                'task_type': 'GPU', 'devices': '0',
                'verbose': 0,
                'class_weights': class_weights_final_for_catboost,  # Uses 0/1 keys
            }
            try:
                # y_class_cv_pool is already 0/1
                current_pool = Pool(
                    data=X_cv_pool_sel,
                    label=y_class_cv_pool,
                    feature_names=list(X_cv_pool_sel.columns),
                    cat_features=[]  # —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –Ω–µ—Ç
                )
                cv_data = cv(current_pool, params=trial_params, fold_count=5, shuffle=True, stratified=True, plot=False)
                metric_key_rs = f'test-{eval_metric_clf}-mean'
                if eval_metric_clf in ['Logloss', 'RMSE', 'MultiClass']:  # Metrics to minimize
                    current_score_rs = -cv_data[metric_key_rs].min()  # Maximize the negative of a minimization metric
                    current_best_iter_rs = cv_data[metric_key_rs].idxmin() + 1
                else:  # Metrics to maximize (Accuracy, AUC, F1)
                    current_score_rs = cv_data[metric_key_rs].max()
                    current_best_iter_rs = cv_data[metric_key_rs].idxmax() + 1

                trial_params['iterations'] = current_best_iter_rs
                if trial_params['iterations'] <= trial_params['early_stopping_rounds']:
                    trial_params['iterations'] = trial_params[
                                                     'early_stopping_rounds'] * 2 + 1  # Ensure more iterations than early stopping

                logging.info(
                    f"  RandomSearch Trial {i + 1}: params={ {k: v for k, v in trial_params.items() if k in ['iterations', 'learning_rate', 'depth', 'l2_leaf_reg']} }, score ({eval_metric_clf})={current_score_rs:.4f} (raw_cv_score: {cv_data[metric_key_rs].iloc[-1]:.4f})")
                if current_score_rs > best_score_rs:
                    best_score_rs = current_score_rs
                    best_params_rs = trial_params.copy()
                    logging.info(
                        f"  üéâ New best RandomSearch score: {best_score_rs:.4f} with params: { {k: v for k, v in best_params_rs.items() if k in ['iterations', 'learning_rate', 'depth', 'l2_leaf_reg']} }")
            except Exception as e:
                logging.warning(f"  –û—à–∏–±–∫–∞ –≤ RandomSearch trial {i + 1} ({trial_params}): {e}")
                continue
        if best_params_rs:
            cv_params_class = best_params_rs
            best_iter_class_cv = best_params_rs['iterations']  # This is already set in best_params_rs
            logging.info(
                f"üéØ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è clf_class –æ—Ç RandomSearch: { {k: v for k, v in cv_params_class.items() if k in ['iterations', 'learning_rate', 'depth', 'l2_leaf_reg']} } —Å CV-score ({eval_metric_clf}): {best_score_rs:.4f}")
        else:
            logging.warning("RandomSearch –Ω–µ –Ω–∞—à–µ–ª –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ.")
    else:
        logging.warning(
            f"RandomSearch –¥–ª—è clf_class –ø—Ä–æ–ø—É—â–µ–Ω. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–ª–∞—Å—Å–æ–≤. X_cv_pool_sel empty: {X_cv_pool_sel.empty}, y_class_cv_pool empty: {y_class_cv_pool.empty}, unique classes: {y_class_cv_pool.nunique() if not y_class_cv_pool.empty else 'N/A'}, min class count: {(y_class_cv_pool.value_counts().min() if not y_class_cv_pool.empty and y_class_cv_pool.nunique() > 0 else 'N/A')}. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")

    logging.info(
        f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ clf_class –¥–ª—è {tf_train} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: { {k: v for k, v in cv_params_class.items() if k in ['iterations', 'learning_rate', 'depth', 'l2_leaf_reg']} }")
    clf_class_model = CatBoostClassifier(**cv_params_class)  # class_weights with 0/1 keys passed here
    clf_class_model.set_params(verbose=100)

    if not X_train_sel.empty and not y_train_class.empty:
        # y_train_class and y_test_class are already 0/1
        eval_set_class = (X_test_sel, y_test_class) if not X_test_sel.empty and not y_test_class.empty else None
        try:
            clf_class_model.fit(X_train_sel, y_train_class, eval_set=eval_set_class, plot=False)
            logging.info(f"clf_class best_iteration: {clf_class_model.get_best_iteration()}")
            if clf_class_model.get_best_score() and eval_set_class:
                validation_scores = clf_class_model.get_best_score().get('validation',
                                                                         clf_class_model.get_best_score().get(
                                                                             'validation_0'))
                if validation_scores:
                    logging.info(
                        f"clf_class validation_{eval_metric_clf}: {validation_scores[eval_metric_clf]:.4f}")
            if clf_class_model is not None and hasattr(clf_class_model, 'get_feature_importance'):
                importances = clf_class_model.get_feature_importance()
                feature_importance_df = pd.DataFrame({'feature': X_train_sel.columns, 'importance': importances})
                feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)
                logging.info(
                    f"\nüî• –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è clf_class ({tf_train}):\n{feature_importance_df.head(10).to_string(index=False)}")
                try:
                    fi_path = f"logs/feature_importance_clf_class_{tf_train}.csv"
                    feature_importance_df.to_csv(fi_path, index=False)
                    logging.info(f"üìÅ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è clf_class —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {fi_path}")
                except Exception as e_fi_save:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e_fi_save}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ clf_class_model: {e}")
            clf_class_model = None
    else:
        logging.error("–û–±—É—á–µ–Ω–∏–µ clf_class –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ: X_train_sel –∏–ª–∏ y_train_class –ø—É—Å—Ç—ã.")
        clf_class_model = None

    reg_delta_model = None
    logging.info(f"--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è reg_delta ({tf_train}) ---")
    if not y_train_delta.empty: logging.info(
        f"y_train_delta ({len(y_train_delta)}): min={y_train_delta.min():.6f}, max={y_train_delta.max():.6f}, mean={y_train_delta.mean():.6f}, std={y_train_delta.std():.6f}")
    if not y_test_delta.empty: logging.info(
        f"y_test_delta ({len(y_test_delta)}): min={y_test_delta.min():.6f}, max={y_test_delta.max():.6f}, mean={y_test_delta.mean():.6f}, std={y_test_delta.std():.6f}")
    logging.info(f"–û–±—É—á–µ–Ω–∏–µ reg_delta –¥–ª—è {tf_train}...")
    reg_delta_model = CatBoostRegressor(
        iterations=500, learning_rate=0.03, depth=6,
        loss_function='RMSE', eval_metric='RMSE', custom_metric=['MAE'],
        min_data_in_leaf=10, task_type="GPU", devices='0',
        random_seed=42, verbose=100, early_stopping_rounds=50
    )
    if not X_train_sel.empty and not y_train_delta.empty:
        eval_set_delta = (X_test_sel, y_test_delta) if not X_test_sel.empty and not y_test_delta.empty else None
        try:
            reg_delta_model.fit(X_train_sel, y_train_delta, eval_set=eval_set_delta)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ reg_delta_model: {e}")
            reg_delta_model = None
    else:
        logging.error("–û–±—É—á–µ–Ω–∏–µ reg_delta –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        reg_delta_model = None

    reg_vol_model = None
    logging.info(f"--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è reg_vol ({tf_train}) ---")
    if not y_train_vol.empty: logging.info(
        f"y_train_vol ({len(y_train_vol)}): min={y_train_vol.min():.6f}, max={y_train_vol.max():.6f}, mean={y_train_vol.mean():.6f}, std={y_train_vol.std():.6f}")
    if not y_test_vol.empty: logging.info(
        f"y_test_vol ({len(y_test_vol)}): min={y_test_vol.min():.6f}, max={y_test_vol.max():.6f}, mean={y_test_vol.mean():.6f}, std={y_test_vol.std():.6f}")
    logging.info(f"–û–±—É—á–µ–Ω–∏–µ reg_vol –¥–ª—è {tf_train}...")
    reg_vol_model = CatBoostRegressor(
        iterations=500, learning_rate=0.03, depth=6,
        loss_function='RMSE', eval_metric='RMSE', custom_metric=['MAE'],
        min_data_in_leaf=10, task_type="GPU", devices='0',
        random_seed=42, verbose=100, early_stopping_rounds=50
    )
    if not X_train_sel.empty and not y_train_vol.empty:
        eval_set_vol = (X_test_sel, y_test_vol) if not X_test_sel.empty and not y_test_vol.empty else None
        try:
            reg_vol_model.fit(X_train_sel, y_train_vol, eval_set=eval_set_vol)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ reg_vol_model: {e}")
            reg_vol_model = None
    else:
        logging.error("–û–±—É—á–µ–Ω–∏–µ reg_vol –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
        reg_vol_model = None

    clf_tp_hit_model = None
    if train_tp_hit_model_flag and y_tp_hit_cv_pool is not None and y_test_tp_hit is not None:
        logging.info(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é clf_tp_hit –¥–ª—è {tf_train}...")
        class_weights_tp_hit_final_model = {0: 1, 1: 10}  # Assuming TP-hit is 0/1
        counts_tp_hit_train = y_train_tp_hit.value_counts().to_dict() if not y_train_tp_hit.empty else {}
        logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ y_train_tp_hit (clf_tp_hit): {counts_tp_hit_train}")
        logging.info(f"–í–µ—Å–∞ –¥–ª—è clf_tp_hit: {class_weights_tp_hit_final_model}")
        best_iter_tp_hit = 300
        if use_stratified_cv_for_tp_hit and not X_cv_pool_sel.empty and not y_tp_hit_cv_pool.empty and \
                y_tp_hit_cv_pool.nunique() >= 2 and \
                (y_tp_hit_cv_pool.value_counts().min() if not y_tp_hit_cv_pool.empty else 0) >= min_samples_for_cv:
            logging.info("–ó–∞–ø—É—Å–∫ CV –¥–ª—è clf_tp_hit...")
            cv_params_tp_hit = {
                'iterations': 500, 'learning_rate': 0.03, 'depth': 4,
                'loss_function': 'Logloss', 'eval_metric': 'Accuracy',
                'early_stopping_rounds': 50, 'random_seed': 42,
                'task_type': 'GPU', 'devices': '0', 'verbose': 0,
                'class_weights': class_weights_tp_hit_final_model
            }
            strat_cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            try:
                pool_tp_hit_cv = Pool(X_cv_pool_sel, y_tp_hit_cv_pool.astype(int),
                                      feature_names=list(X_cv_pool_sel.columns))
                cv_data_tp_hit = cv(pool_tp_hit_cv, params=cv_params_tp_hit, folds=strat_cv_folds, plot=False)
                metric_key_cv_tp_hit = f'test-{cv_params_tp_hit["eval_metric"]}-mean'
                if cv_params_tp_hit["eval_metric"] in ['Logloss', 'RMSE']:
                    best_iter_tp_hit = cv_data_tp_hit[metric_key_cv_tp_hit].idxmin() + 1
                else:
                    best_iter_tp_hit = cv_data_tp_hit[metric_key_cv_tp_hit].idxmax() + 1
                if best_iter_tp_hit <= cv_params_tp_hit['early_stopping_rounds']:
                    best_iter_tp_hit = cv_params_tp_hit['early_stopping_rounds'] * 2 + 1
                logging.info(
                    f"üîç CatBoostCV (TP-hit): best_iterations = {best_iter_tp_hit}, val_score = {cv_data_tp_hit[metric_key_cv_tp_hit].iloc[-1]:.4f}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ CV –¥–ª—è clf_tp_hit: {e}. –ò—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
        else:
            logging.info(f"CV –¥–ª—è clf_tp_hit –ø—Ä–æ–ø—É—â–µ–Ω. –ò—Ç–µ—Ä–∞—Ü–∏–∏: {best_iter_tp_hit}")
        logging.info(f"–û–±—É—á–µ–Ω–∏–µ clf_tp_hit —Å {best_iter_tp_hit} –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏...")
        clf_tp_hit_model = CatBoostClassifier(
            iterations=best_iter_tp_hit, learning_rate=0.03, depth=4,
            class_weights=class_weights_tp_hit_final_model,
            loss_function='Logloss', eval_metric='Accuracy', random_state=42, verbose=100,
            task_type="GPU", devices='0', early_stopping_rounds=50
        )
        if not X_train_sel.empty and not y_train_tp_hit.empty:
            eval_set_tp_hit = (X_test_sel,
                               y_test_tp_hit.astype(int)) if not X_test_sel.empty and not y_test_tp_hit.empty else None
            try:
                clf_tp_hit_model.fit(X_train_sel, y_train_tp_hit.astype(int), eval_set=eval_set_tp_hit)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ clf_tp_hit_model: {e}")
                clf_tp_hit_model = None
        else:
            logging.error("–û–±—É—á–µ–Ω–∏–µ clf_tp_hit –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
            clf_tp_hit_model = None
    elif train_tp_hit_model_flag:
        logging.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è clf_tp_hit.")

    if clf_class_model: joblib.dump(clf_class_model, f"models/{tf_train}_clf_class.pkl")
    if reg_delta_model: joblib.dump(reg_delta_model, f"models/{tf_train}_reg_delta.pkl")
    if reg_vol_model: joblib.dump(reg_vol_model, f"models/{tf_train}_reg_vol.pkl")
    if clf_tp_hit_model: joblib.dump(clf_tp_hit_model, f"models/{tf_train}_clf_tp_hit.pkl")

    if X_test_sel.empty:
        logging.warning("X_test_sel –ø—É—Å—Ç, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏.")
    else:
        metrics_output = f"\nüìä  –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ ({tf_train}) —Å {len(top20_features)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏:\n"
        report_class_str, acc_class_val = "N/A", "N/A"
        f1_class_val, precision_class_val = "N/A", "N/A"
        cm_df_str = "N/A"
        auc_class_val = "N/A"

        if clf_class_model and not y_test_class.empty:  # y_test_class is now 0/1
            try:
                proba_class_test = clf_class_model.predict_proba(X_test_sel)
                # Assuming classes_ are [0, 1] and 1 corresponds to 'UP'
                # For binary classification, predict_proba typically returns [P(class_0), P(class_1)]
                # If model.classes_ is [0,1], then proba_class_test[:, 1] is P(class_1)
                # Positive class is 1 (formerly 'UP')
                positive_class_label_int = 1

                # Get the index of the positive class (1) from the model's learned classes
                model_classes_ = getattr(clf_class_model, 'classes_', [0, 1])  # Default to [0,1] if not found
                try:
                    positive_class_idx = list(model_classes_).index(positive_class_label_int)
                except ValueError:
                    logging.warning(
                        f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å {positive_class_label_int} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ model.classes_ ({model_classes_}). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω–¥–µ–∫—Å 1 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.")
                    positive_class_idx = 1  # Fallback, assumes second column is positive class

                y_pred_class_flat = np.where(proba_class_test[:, positive_class_idx] > PREDICT_PROBA_THRESHOLD_CLASS,
                                             positive_class_label_int,  # Predict 1 for UP
                                             1 - positive_class_label_int)  # Predict 0 for DOWN

                y_test_class_flat = y_test_class.squeeze()  # Already 0/1

                report_labels_int = [0, 1]  # Actual labels used in y_true, y_pred
                # TARGET_CLASS_NAMES = ['DOWN', 'UP'] for display in report

                unique_labels_test_set = set(y_test_class_flat.unique())
                unique_labels_pred_set = set(pd.Series(y_pred_class_flat).unique())
                all_present_labels_set = unique_labels_test_set | unique_labels_pred_set
                # Check against integer labels {0, 1}
                if not all_present_labels_set.issubset(set(report_labels_int)):
                    logging.warning(
                        f"–ú–µ—Ç–∫–∏ –≤ y_test/y_pred ({all_present_labels_set}) –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –æ–∂–∏–¥–∞–µ–º—ã—Ö –º–µ—Ç–∫–∞—Ö {set(report_labels_int)}")

                report_class_str = classification_report(y_test_class_flat, y_pred_class_flat,
                                                         labels=report_labels_int,
                                                         target_names=TARGET_CLASS_NAMES,  # Display 'DOWN', 'UP'
                                                         digits=4, zero_division=0)
                acc_class_val = accuracy_score(y_test_class_flat, y_pred_class_flat)

                # Calculate F1 and Precision for the positive class (1, formerly 'UP')
                f1_class_val = f1_score(y_test_class_flat, y_pred_class_flat, pos_label=positive_class_label_int,
                                        zero_division=0)
                precision_class_val = precision_score(y_test_class_flat, y_pred_class_flat,
                                                      pos_label=positive_class_label_int, zero_division=0)

                # –†–∞—Å—á—ë—Ç ROC AUC
                try:
                    # y_test_class_flat is already 0/1. This is y_true_binary.
                    if len(np.unique(y_test_class_flat)) > 1:
                        y_score_probs = proba_class_test[:, positive_class_idx]  # Probabilities for the positive class
                        auc_class_val = roc_auc_score(y_test_class_flat, y_score_probs)
                    else:
                        logging.warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å ROC AUC: –≤ y_test_class_flat —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å ({np.unique(y_test_class_flat)}).")
                        auc_class_val = "N/A (–æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ y_true)"
                except Exception as e_auc:
                    logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å ROC AUC: {e_auc}")
                    auc_class_val = "N/A (–æ—à–∏–±–∫–∞)"

                cm = confusion_matrix(y_test_class_flat, y_pred_class_flat, labels=report_labels_int)
                cm_df = pd.DataFrame(cm, index=[f"True_{l}" for l in TARGET_CLASS_NAMES],  # Display 'DOWN', 'UP'
                                     columns=[f"Pred_{l}" for l in TARGET_CLASS_NAMES])  # Display 'DOWN', 'UP'
                logging.info(f"üß† Confusion Matrix (clf_class) –¥–ª—è {tf_train}:\n{cm_df}")
                cm_df_str = cm_df.to_string()
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫ clf_class: {e}", exc_info=True)
        else:
            logging.warning("–ú–µ—Ç—Ä–∏–∫–∏ clf_class –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è (–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ y_test_class –ø—É—Å—Ç).")

        metrics_output += f"--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (clf_class) [{eval_metric_clf}] (proba_threshold={PREDICT_PROBA_THRESHOLD_CLASS}) ---\n"
        metrics_output += report_class_str + "\n"
        metrics_output += f"   üéØ Accuracy (class): {acc_class_val:.4f}\n" if isinstance(acc_class_val,
                                                                                        float) else f"   üéØ Accuracy (class): {acc_class_val}\n"
        metrics_output += f"   üß† F1-score (UP/1): {f1_class_val:.4f}\n" if isinstance(f1_class_val,
                                                                                      float) else f"   üß† F1-score (UP/1): {f1_class_val}\n"
        metrics_output += f"   üéØ Precision (UP/1): {precision_class_val:.4f}\n" if isinstance(precision_class_val,
                                                                                              float) else f"   üéØ Precision (UP/1): {precision_class_val}\n"
        metrics_output += f"   üü¶ ROC AUC (UP/1): {auc_class_val:.4f}\n" if isinstance(auc_class_val,
                                                                                      float) else f"   üü¶ ROC AUC (UP/1): {auc_class_val}\n"
        metrics_output += f"   üß© Confusion Matrix (clf_class):\n{cm_df_str}\n"

        mae_delta_val = "N/A"
        if reg_delta_model and not y_test_delta.empty:
            try:
                y_pred_delta_test = reg_delta_model.predict(X_test_sel)
                mae_delta_val = mean_absolute_error(y_test_delta, y_pred_delta_test)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ MAE Delta: {e}")
        else:
            logging.warning("MAE Delta –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.")
        metrics_output += f"   üìà MAE Delta: {mae_delta_val:.6f}\n" if isinstance(mae_delta_val,
                                                                                 float) else f"   üìà MAE Delta: {mae_delta_val}\n"

        mae_vol_val = "N/A"
        if reg_vol_model and not y_test_vol.empty:
            try:
                y_pred_vol_test = reg_vol_model.predict(X_test_sel)
                mae_vol_val = mean_absolute_error(y_test_vol, y_pred_vol_test)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ MAE Volatility: {e}")
        else:
            logging.warning("MAE Volatility –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.")
        metrics_output += f"   ‚ö° MAE Volatility: {mae_vol_val:.6f}\n" if isinstance(mae_vol_val,
                                                                                    float) else f"   ‚ö° MAE Volatility: {mae_vol_val}\n"

        if train_tp_hit_model_flag:
            metrics_output += "\n--- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä TP-Hit (clf_tp_hit) ---\n"
            report_tp_hit_str, acc_tp_hit_val = "N/A", "N/A"
            if clf_tp_hit_model and y_test_tp_hit is not None and not y_test_tp_hit.empty:
                try:
                    y_pred_tp_hit_test = clf_tp_hit_model.predict(X_test_sel)
                    y_pred_tp_hit_flat = [item[0] if isinstance(item, (np.ndarray, list)) and len(item) == 1 else item
                                          for item in y_pred_tp_hit_test]
                    y_test_tp_hit_flat = y_test_tp_hit.squeeze().astype(int)
                    y_pred_tp_hit_flat = pd.Series(y_pred_tp_hit_flat).astype(int)
                    valid_indices = ~y_test_tp_hit_flat.isna() & ~y_pred_tp_hit_flat.isna()
                    y_test_tp_hit_flat_valid, y_pred_tp_hit_flat_valid = y_test_tp_hit_flat[valid_indices], \
                    y_pred_tp_hit_flat[
                        valid_indices]
                    if len(y_test_tp_hit_flat_valid) > 0:
                        report_tp_hit_str = classification_report(y_test_tp_hit_flat_valid, y_pred_tp_hit_flat_valid,
                                                                  digits=4,
                                                                  zero_division=0,
                                                                  target_names=['No TP Hit (0)', 'TP Hit (1)'],
                                                                  labels=[0, 1])
                        acc_tp_hit_val = accuracy_score(y_test_tp_hit_flat_valid, y_pred_tp_hit_flat_valid)
                    else:
                        report_tp_hit_str, acc_tp_hit_val = "N/A (–Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)", "N/A (–Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)"
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫ clf_tp_hit: {e}", exc_info=True)
            else:
                logging.warning("–ú–µ—Ç—Ä–∏–∫–∏ clf_tp_hit –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.")
            metrics_output += report_tp_hit_str + "\n"
            metrics_output += f"   üéØ Accuracy (TP-hit): {acc_tp_hit_val:.4f}\n" if isinstance(acc_tp_hit_val,
                                                                                              float) else f"   üéØ Accuracy (TP-hit): {acc_tp_hit_val}\n"

        print(metrics_output)
        log_file_path = f"logs/train_metrics_{tf_train}.txt"
        mode = "a" if os.path.exists(log_file_path) else "w"
        with open(log_file_path, mode, encoding="utf-8") as f_log:
            if mode == "w": f_log.write(f"=== –ù–∞—á–∞–ª–æ –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {tf_train} ({pd.Timestamp.now()}) ===\n")
            f_log.write(metrics_output)
            f_log.write(f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(feature_cols_final)} —à—Ç—É–∫):\n" + ", ".join(
                feature_cols_final) + "\n\n")
        logging.info(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ª–æ–≥: {log_file_path}")

    logging.info(f"‚úÖ  –ú–æ–¥–µ–ª–∏ –¥–ª—è {tf_train} –æ–±—É—á–µ–Ω—ã.")
    if train_tp_hit_model_flag and clf_tp_hit_model:
        logging.info("–ù–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ: –¥–ª—è predict_all.py —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ predict_proba –¥–ª—è clf_tp_hit.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π CatBoost.")
    parser.add_argument('--tf', type=str, required=True, help="–¢–∞–π–º—Ñ—Ä–µ–π–º (1m, 5m, ...)")
    args = parser.parse_args()
    log_file_path_main = f"logs/train_metrics_{args.tf}.txt"
    # Ensure the main log file is cleared at the start of a specific tf run, if train_all_models appends.
    # The current logic in train_all_models handles its own log file appending/writing.
    # This initial write to log_file_path_main here will be overwritten if train_all_models also writes to it in 'w' mode.
    # It seems train_all_models opens its own log file path with "a" or "w", so this is fine.

    # Overwrite specific log file for this run at the beginning of __main__
    # This helps to have a clean log for each invocation of the script for a given --tf
    with open(log_file_path_main, "w", encoding="utf-8") as f_clear:
        f_clear.write(f"=== –ù–∞—á–∞–ª–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ª–æ–≥–∞ –¥–ª—è {args.tf} ({pd.Timestamp.now()}) ===\n")
        f_clear.write(f"–ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ train_model.py –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {args.tf}\n")

    try:
        train_all_models(args.tf)
    except KeyboardInterrupt:
        print(f"\n[TrainModel] üõë –û–±—É—á–µ–Ω–∏–µ –¥–ª—è {args.tf} –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
        logging.info(f"–û–±—É—á–µ–Ω–∏–µ –¥–ª—è {args.tf} –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(130)
    except Exception as e:
        logging.error(f"[TrainModel] üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ {args.tf}: {e}", exc_info=True)
        sys.exit(1)